@inproceedings{akkaynak2019a,
  title = {Sea-Thru: {{A}} Method for Removing Water from Underwater Images},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Akkaynak, D. and Treibitz, T.},
  year = 2019,
  pages = {1682--1691},
  citation-number = {9},
  langid = {english}
}

@inproceedings{akkaynak2019a,
  title = {Sea-Thru: {{A}} Method for Removing Water from Underwater Images},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Akkaynak, D. and Treibitz, T.},
  year = 2019,
  pages = {1682--1691},
  citation-number = {9},
  langid = {english}
}

@inproceedings{akkaynak2019a,
  title = {Sea-Thru: {{A}} Method for Removing Water from Underwater Images},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Akkaynak, D. and Treibitz, T.},
  year = 2019,
  pages = {1682--1691},
  citation-number = {9},
  langid = {english}
}

@misc{alhashmiEfficientDorsalFinbased2024,
  title = {Efficient Dorsal Fin-Based Classification of {{Risso}}'s and Common {{Bottlenose}} Dolphins Using {{YOLOv7}} and {{YOLOv8}} Models for Real-Time Applications},
  author = {Alhashmi, Fawaghy and Alhefeiti, Maryam and Mirza, Shaher Bano and Ridouane, Fouad Lamghari},
  year = 2024,
  month = jun
}

@article{ancuti2018a,
  title = {Color Balance and Fusion for Underwater Image Enhancement},
  author = {Ancuti, C.O.},
  year = 2018,
  journal = {IEEE Transactions on Image Processing},
  volume = {27},
  number = {1},
  pages = {379--393},
  citation-number = {15},
  langid = {english}
}

@article{ancuti2018a,
  title = {Color Balance and Fusion for Underwater Image Enhancement},
  author = {Ancuti, C.O.},
  year = 2018,
  journal = {IEEE Transactions on Image Processing},
  volume = {27},
  number = {1},
  pages = {379--393},
  citation-number = {15},
  langid = {english}
}

@article{ancuti2018a,
  title = {Color Balance and Fusion for Underwater Image Enhancement},
  author = {Ancuti, C.O.},
  year = 2018,
  journal = {IEEE Transactions on Image Processing},
  volume = {27},
  number = {1},
  pages = {379--393},
  citation-number = {15},
  langid = {english}
}

@article{ancuti2018b,
  title = {Color Balance and Fusion for Underwater Image Enhancement},
  author = {Ancuti, C.O.},
  year = 2018,
  journal = {IEEE Transactions on Image Processing},
  volume = {27},
  number = {1},
  pages = {379--393},
  citation-number = {19},
  langid = {english}
}

@article{ancuti2018b,
  title = {Color Balance and Fusion for Underwater Image Enhancement},
  author = {Ancuti, C.O.},
  year = 2018,
  journal = {IEEE Transactions on Image Processing},
  volume = {27},
  number = {1},
  pages = {379--393},
  citation-number = {19},
  langid = {english}
}

@article{ancuti2018b,
  title = {Color Balance and Fusion for Underwater Image Enhancement},
  author = {Ancuti, C.O.},
  year = 2018,
  journal = {IEEE Transactions on Image Processing},
  volume = {27},
  number = {1},
  pages = {379--393},
  citation-number = {19},
  langid = {english}
}

@inproceedings{ancutiEnhancingUnderwaterImages2012,
  title = {Enhancing Underwater Images and Videos by Fusion},
  booktitle = {2012 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Ancuti, C.O.},
  year = 2012,
  pages = {81--88},
  publisher = {IEEE},
  citation-number = {18},
  langid = {english},
  file = {/Users/dan/Zotero/storage/FPPFTFVL/Ancuti et al. - 2012 - Enhancing underwater images and videos by fusion.pdf}
}

@inproceedings{arunaUnderwaterFishIdentification2023,
  title = {Underwater {{Fish Identification}} in {{Real-Time}} Using {{Convolutional Neural Network}}},
  booktitle = {2023 7th {{International Conference}} on {{Intelligent Computing}} and {{Control Systems}} ({{ICICCS}})},
  author = {Aruna, S.K. and Deepa, N. and Devi, T},
  year = 2023,
  month = may,
  pages = {586--591},
  issn = {2768-5330},
  doi = {10.1109/ICICCS56967.2023.10142531},
  urldate = {2024-04-22},
  abstract = {Artificial Intelligence (AD is the wide application that learns the problem and features by given data and processes the data like the human brain. When a computer program imitates a characteristic of the human brain that is considered ``innovator.'' Among the methods are statistical methods, methods for artificial intelligence, and traditional order to verify the validity. The expansion of AI is also related to virtually infinite storage and an abundance of data, including exchanges, geospatial information, video files, photos, text messages, and audio files. Machine learning is divided into deep learning and deep learning is primarily divided into numerous layers of neural networks. This pattern gives it the ability to learn a lot of information and attempt to replicate the brain function. Increasing the efficiency by attaching more covert layers can be beneficial. It is used to gather the data and transfer the data. Aquaculture production has grown into a barrier to the growth of fish culture and the counting operation represents one of the problems experienced during the spawning process. Previous studies have primarily relied on the application of manual and automated counting techniques, which has prevented it from producing accurate results. The proposed method offers a promising method for enhancing image detection by combining the IoT techniques. The image data were divided into three categories: low frequency, intermediate density, as well as high frequency. The proposed method has used 8200 images to train and 2500 images for verification. Only the data relevant data sources were used during the train and verification phase in order to find the proper parameters and create a better VGG19 parameter calibration strategy. Consequently, the improved VGG19 model can achieve an accuracy of 98\%.},
  keywords = {Artificial Intelligence,Deep learning,Fish,Machine Learning,Manuals,Production,Real-time systems,Soft sensors,Statistical analysis,Tracking Deep Learning,Underwater object detection},
  file = {/Users/dan/Zotero/storage/TZHP8TUZ/Aruna et al. - 2023 - Underwater Fish Identification in Real-Time using .pdf;/Users/dan/Zotero/storage/BSFRJ5NQ/10142531.html}
}

@misc{arunaUnderwaterFishIdentification2023a,
  title = {Underwater {{Fish Identification}} in {{Real-Time}} Using {{Convolutional Neural Network}}},
  author = {Aruna, S. and Deepa, N. and Devi, T.},
  year = 2023,
  month = may,
  pages = {586--591}
}

@misc{arunaUnderwaterFishIdentification2023b,
  title = {Underwater {{Fish Identification}} in {{Real-Time}} Using {{Convolutional Neural Network}}},
  author = {Aruna, S. and Deepa, N. and Devi, T.},
  year = 2023,
  month = may,
  doi = {10.1109/iciccs56967.2023.10142531}
}

@article{ashtagiCombinedDeepLearning2023,
  title = {Combined Deep Learning and Machine Learning Models for the Prediction of Stages of Melanoma},
  author = {Ashtagi, Rashmi and Mane, Deepak and Deore, Mahendra and Maranur, Jyoti and Hosmani, Sridevi},
  year = 2023,
  month = oct,
  journal = {Journal of Autonomous Intelligence},
  volume = {7},
  doi = {10.32629/jai.v7i1.749},
  abstract = {p{$>$}Melanoma is deadly kind of skin cancer as it gets metastasized soon. It is really essential to recognize melanoma and start treatment at early stage. It is also necessary to determine the stage of melanoma in order to treat melanoma patients. Non-invasive technique is required to detect the stage of melanoma. Proposed system presents novel technique to classify the stages of melanoma based on thickness of tumor. This system uses dimensionality reduction technique to reduce the number of features and it also uses combine approach of deep learning (DL) and machine learning (ML) algorithm which include multilayer perception (MLP) and random forest (RF). Deep learning method is always better for training as they can reduce the need for data preprocessing and feature engineering and can provide simple trainable models built using only five or six different operations. Secondly, they are scalable, as they can be easily parallelized on GPUs or TPUs and can be trained by iterating over small batches of data. Thirdly they are reusable, so they can be trained on additional data without starting from scratch, making them viable for continuous online learning. For classification task machine learning algorithm that is, random forest is used as it decreases over fitting in decision trees and aids to increase the accuracy. Total of three algorithms were used, MLP, RF and proposed algorithm combined multilayer perception and random forest that is, MLP-RF. Among these models, the MLP-RF showed the best results in predicting melanoma stages with the accuracy of 97.42\%.}
}

@misc{azmiNaturalbasedUnderwaterImage2019,
  title = {Natural-Based Underwater Image Color Enhancement through Fusion of Swarm-Intelligence Algorithm},
  author = {Azmi, Kamil Zakwan Mohd and Ghani, Ahmad Shahrizan Abdul and Yusof, Zulkifli Md and Ibrahim, Zuwairie},
  year = 2019,
  month = sep,
  journal = {Elsevier BV},
  volume = {85},
  pages = {105810--105810},
  doi = {10.1016/j.asoc.2019.105810}
}

@article{bach2015a,
  title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
  author = {Bach, S. and Binder, A. and Montavon, G. and Klauschen, F. and M{\"u}ller, K.R. and Samek, W.},
  year = 2015,
  journal = {PLOS ONE},
  volume = {10},
  number = {7},
  pages = {0130140},
  citation-number = {40},
  langid = {english}
}

@article{bach2015a,
  title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
  author = {Bach, S.},
  year = 2015,
  journal = {PLOS ONE},
  volume = {10},
  number = {7},
  pages = {0130140},
  citation-number = {61},
  langid = {english}
}

@article{bach2015a,
  title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
  author = {Bach, S.},
  year = 2015,
  journal = {PLOS ONE},
  volume = {10},
  number = {7},
  pages = {0130140},
  citation-number = {61},
  langid = {english}
}

@article{bach2015a,
  title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
  author = {Bach, S.},
  year = 2015,
  journal = {PLOS ONE},
  volume = {10},
  number = {7},
  pages = {0130140},
  citation-number = {61},
  langid = {english}
}

@article{bianco2019a,
  title = {Underwater Image Enhancement by Light Absorption and Scattering Compensation},
  author = {Bianco, S.},
  year = 2019,
  journal = {IEEE Transactions on Image Processing},
  volume = {29},
  pages = {2410--2421},
  citation-number = {12},
  langid = {english}
}

@article{bianco2019a,
  title = {Underwater Image Enhancement by Light Absorption and Scattering Compensation},
  author = {Bianco, S.},
  year = 2019,
  journal = {IEEE Transactions on Image Processing},
  volume = {29},
  pages = {2410--2421},
  citation-number = {12},
  langid = {english}
}

@article{bianco2019a,
  title = {Underwater Image Enhancement by Light Absorption and Scattering Compensation},
  author = {Bianco, S.},
  year = 2019,
  journal = {IEEE Transactions on Image Processing},
  volume = {29},
  pages = {2410--2421},
  citation-number = {12},
  langid = {english}
}

@misc{binderLayerwiseRelevancePropagation2016,
  title = {Layer-Wise {{Relevance Propagation}} for {{Neural Networks}} with {{Local Renormalization Layers}}},
  author = {Binder, Alexander and Montavon, Gr{\'e}goire and Bach, Sebastian and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = 2016,
  month = apr,
  number = {arXiv:1604.00825},
  eprint = {1604.00825},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-06},
  abstract = {Layer-wise relevance propagation is a framework which allows to decompose the prediction of a deep neural network computed over a sample, e.g. an image, down to relevance scores for the single input dimensions of the sample such as subpixels of an image. While this approach can be applied directly to generalized linear mappings, product type non-linearities are not covered. This paper proposes an approach to extend layer-wise relevance propagation to neural networks with local renormalization layers, which is a very common product-type non-linearity in convolutional neural networks. We evaluate the proposed method for local renormalization layers on the CIFAR-10, Imagenet and MIT Places datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dan/Zotero/storage/S2XW5LAS/Binder et al. - 2016 - Layer-wise Relevance Propagation for Neural Networ.pdf}
}

@misc{binderLayerWiseRelevancePropagation2016a,
  title = {Layer-{{Wise Relevance Propagation}} for {{Neural Networks}} with {{Local Renormalization Layers}}},
  author = {Binder, Alexander and Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = 2016,
  month = jan,
  journal = {Springer Science+Business Media},
  pages = {63--71},
  doi = {10.1007/978-3-319-44781-0_8}
}

@misc{bochkovskiyYOLOv4OptimalSpeed2020,
  title = {{{YOLOv4}}: {{Optimal Speed}} and {{Accuracy}} of {{Object Detection}}},
  shorttitle = {{{YOLOv4}}},
  author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
  year = 2020,
  month = apr,
  number = {arXiv:2004.10934},
  eprint = {2004.10934},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-19},
  abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5\% AP (65.7\% AP50) for the MS COCO dataset at a realtime speed of {$\sim$}65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/dan/Zotero/storage/GMM2T4IF/Bochkovskiy et al. - 2020 - YOLOv4 Optimal Speed and Accuracy of Object Detec.pdf}
}

@inproceedings{bodla2017a,
  title = {Soft-{{NMS}}---{{Improving}} Object Detection with One Line of Code},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Bodla, N.},
  year = 2017,
  pages = {5561--5569},
  citation-number = {39},
  langid = {english}
}

@inproceedings{bodla2017a,
  title = {Soft-{{NMS}}---{{Improving}} Object Detection with One Line of Code},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Bodla, N.},
  year = 2017,
  pages = {5561--5569},
  citation-number = {39},
  langid = {english}
}

@inproceedings{bodla2017a,
  title = {Soft-{{NMS}}---{{Improving}} Object Detection with One Line of Code},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Bodla, N.},
  year = 2017,
  pages = {5561--5569},
  citation-number = {39},
  langid = {english}
}

@article{boudhaneUnderwaterImageProcessing2016,
  title = {Underwater Image Processing Method for Fish Localization and Detection in Submarine Environment},
  author = {Boudhane, Mohcine and Nsiri, Benayad},
  year = 2016,
  month = aug,
  journal = {Journal of Visual Communication and Image Representation},
  volume = {39},
  pages = {226--238},
  issn = {10473203},
  doi = {10.1016/j.jvcir.2016.05.017},
  urldate = {2024-04-23},
  abstract = {Object detection is an important process in image processing, it aims to detect instances of semantic objects of a certain class in digital images and videos. Object detection has applications in many areas of computer vision such as underwater fish detection. In this paper we present a method for preprocessing and fish localization in underwater images. We are based on a Poisson--Gauss theory, because it can accurately describe the noise present in a large variety of imaging systems. In the preprocessing step we denoise and restore the raw images. These images are split into regions utilizing the mean shift algorithm. For each region, statistical estimation is done independently in order to combine regions into objects. The method is tested under different underwater conditions. Experimental results show that the proposed approach outperforms state of the art methods.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/QMTJMQMZ/Boudhane and Nsiri - 2016 - Underwater image processing method for fish locali.pdf}
}

@misc{boudhaneUnderwaterImageProcessing2016a,
  title = {Underwater Image Processing Method for Fish Localization and Detection in Submarine Environment},
  author = {Boudhane, Mohcine and Nsiri, Benayad},
  year = 2016,
  month = jun,
  volume = {39},
  pages = {226--238}
}

@misc{boudhaneUnderwaterImageProcessing2016b,
  title = {Underwater Image Processing Method for Fish Localization and Detection in Submarine Environment},
  author = {Boudhane, Mohcine and Nsiri, Benayad},
  year = 2016,
  month = jun,
  journal = {Elsevier BV},
  volume = {39},
  pages = {226--238},
  doi = {10.1016/j.jvcir.2016.05.017}
}

@misc{bradskiOpenCVLibrary2000,
  title = {The {{OpenCV}} Library},
  author = {Bradski, Gary},
  year = 2000,
  month = jan,
  volume = {25},
  pages = {120--125}
}

@article{bradskiOpenCVLibrary2000a,
  title = {The {{OpenCV Library}}},
  author = {Bradski, G.},
  year = 2000,
  journal = {Dr. Dobb's Journal of Software Tools},
  keywords = {bibtex-import}
}

@article{brinkerDeepLearningApproach2021,
  title = {Deep Learning Approach to Predict Sentinel Lymph Node Status Directly from Routine Histology of Primary Melanoma Tumours},
  author = {Brinker, Titus J. and Kiehl, Lennard and Schmitt, Max and Jutzi, Tanja B. and {Krieghoff-Henning}, Eva I. and Krahl, Dieter and Kutzner, Heinz and Gholam, Patrick and Haferkamp, Sebastian and Klode, Joachim and Schadendorf, Dirk and Hekler, Achim and Fr{\"o}hling, Stefan and Kather, Jakob N. and Haggenm{\"u}ller, Sarah and Von Kalle, Christof and Heppt, Markus and Hilke, Franz and Ghoreschi, Kamran and Tiemann, Markus and Wehkamp, Ulrike and Hauschild, Axel and Weichenthal, Michael and Utikal, Jochen S.},
  year = 2021,
  month = sep,
  journal = {European Journal of Cancer},
  volume = {154},
  pages = {227--234},
  issn = {09598049},
  doi = {10.1016/j.ejca.2021.05.026},
  urldate = {2024-04-21},
  abstract = {Methods: A total of 415 H\&E slides from primary melanoma tumours with known sentinel node (SN) status from three German university hospitals and one private pathological practice were digitised (150 SN positive/265 SN negative). Two hundred ninety-one slides were used to train artificial neural networks (ANNs). The remaining 124 slides were used to test the ability of the ANNs to predict sentinel status. ANNs were trained and/or tested on data sets that were matched or not matched between SN-positive and SN-negative cases for patient age, ulceration, and tumour thickness, factors that are known to correlate with lymph node status. Results: The best accuracy was achieved by an ANN that was trained and tested on unmatched cases (61.8\% \AE{} 0.2\%) area under the receiver operating characteristic (AUROC). In contrast, ANNs that were trained and/or tested on matched cases achieved (55.0\% \AE{} 3.5\%) AUROC or less. Conclusion: Our results indicate that the image classifier can predict lymph node status to some, albeit so far not clinically relevant, extent. It may do so by mostly detecting equivalents of factors on histological slides that are already known to correlate with lymph node status. Our results provide a basis for future research with larger data cohorts.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/H85FGUCL/Brinker et al. - 2021 - Deep learning approach to predict sentinel lymph n.pdf}
}

@article{bryson2016a,
  title = {True Color Correction of Autonomous Underwater Vehicle Imagery},
  author = {Bryson, M. and {Johnson-Roberson}, M. and Pizarro, O. and Williams, S.B.},
  year = 2016,
  journal = {Journal of Field Robotics},
  volume = {33},
  number = {6},
  pages = {853--874},
  citation-number = {18},
  langid = {english}
}

@inproceedings{buades2005a,
  title = {A Non-Local Algorithm for Image Denoising},
  booktitle = {2005 {{IEEE}} Computer Society Conference on Computer Vision and Pattern Recognition},
  author = {Buades, A.},
  year = 2005,
  volume = {2},
  pages = {60--65},
  citation-number = {23},
  langid = {english}
}

@inproceedings{buades2005a,
  title = {A Non-Local Algorithm for Image Denoising},
  booktitle = {2005 {{IEEE}} Computer Society Conference on Computer Vision and Pattern Recognition},
  author = {Buades, A.},
  year = 2005,
  volume = {2},
  pages = {60--65},
  citation-number = {23},
  langid = {english}
}

@inproceedings{buades2005a,
  title = {A Non-Local Algorithm for Image Denoising},
  booktitle = {2005 {{IEEE}} Computer Society Conference on Computer Vision and Pattern Recognition},
  author = {Buades, A.},
  year = 2005,
  volume = {2},
  pages = {60--65},
  citation-number = {23},
  langid = {english}
}

@inproceedings{buadesNonLocalAlgorithmImage2005,
  title = {A {{Non-Local Algorithm}} for {{Image Denoising}}},
  booktitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  author = {Buades, A. and Coll, B. and Morel, J.-M.},
  year = 2005,
  volume = {2},
  pages = {60--65},
  publisher = {IEEE},
  address = {San Diego, CA, USA},
  doi = {10.1109/CVPR.2005.38},
  urldate = {2024-10-12},
  abstract = {We propose a new measure, the method noise, to evaluate and compare the performance of digital image denoising methods. We first compute and analyze this method noise for a wide class of denoising algorithms, namely the local smoothing filters. Second, we propose a new algorithm, the non local means (NL-means), based on a non local averaging of all pixels in the image. Finally, we present some experiments comparing the NL-means algorithm and the local smoothing filters.},
  isbn = {978-0-7695-2372-9},
  langid = {english},
  file = {/Users/dan/Zotero/storage/A7MKZXQW/Buades et al. - 2005 - A Non-Local Algorithm for Image Denoising.pdf}
}

@misc{buadesNonLocalAlgorithmImage2005a,
  title = {A {{Non-Local Algorithm}} for {{Image Denoising}}},
  author = {Buades, Antoni and Coll, B. and Morel, Jean-Michel},
  year = 2005,
  month = jul,
  doi = {10.1109/cvpr.2005.38}
}

@misc{caiOnceAllTrainOne2019,
  title = {Once-for-{{All}}: {{Train One Network}} and {{Specialize}} It for {{Efficient Deployment}}},
  author = {Cai, Han and Gan, Chuang and Wang, Tianzhe and Zhang, Zhekai and Han, Song},
  year = 2019,
  month = jan
}

@misc{caiOnceAllTrainOne2019a,
  title = {Once-for-{{All}}: {{Train One Network}} and {{Specialize}} It for {{Efficient Deployment}}},
  author = {Cai, Han and Gan, Chuang and Wang, Tianzhe and Zhang, Zhekai and Han, Song},
  year = 2019,
  month = jan,
  journal = {Cornell University},
  doi = {10.48550/arxiv.1908.09791}
}

@misc{caiOnceAllTrainOne2020,
  title = {Once-for-{{All}}: {{Train One Network}} and {{Specialize}} It for {{Efficient Deployment}}},
  shorttitle = {Once-for-{{All}}},
  author = {Cai, Han and Gan, Chuang and Wang, Tianzhe and Zhang, Zhekai and Han, Song},
  year = 2020,
  month = apr,
  number = {arXiv:1908.09791},
  eprint = {1908.09791},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-06-27},
  abstract = {We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices. Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing \$CO\_2\$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks (\${$>$} 10\textasciicircum\textbraceleft 19\textbraceright\$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0\% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and \$CO\_2\$ emission. In particular, OFA achieves a new SOTA 80.0\% ImageNet top-1 accuracy under the mobile setting (\${$<\$$}600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and 50 pre-trained models (for many devices \& many latency constraints) are released at https://github.com/mit-han-lab/once-for-all.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: ICLR 2020},
  file = {/Users/dan/Zotero/storage/XFQMWU82/Cai et al. - 2020 - Once-for-All Train One Network and Specialize it .pdf;/Users/dan/Zotero/storage/K9G28JVW/1908.html}
}

@incollection{cappo2003a,
  title = {Potential of Video Techniques to Monitor Diversity, Abundance and Size of Fish in Studies of Marine Protected Areas},
  booktitle = {Aquatic Protected Areas-What Works Best and How Do We Know},
  author = {Cappo, M. and Harvey, E. and Malcolm, H. and Speare, P.},
  year = 2003,
  pages = {455--464},
  citation-number = {8},
  langid = {english}
}

@misc{carrascoTYOLOTinyVehicle2021,
  title = {T-{{YOLO}}: {{Tiny Vehicle Detection Based}} on {{YOLO}} and {{Multi-Scale Convolutional Neural Networks}}},
  author = {Carrasco, D. and Rashwan, Hatem A. and Garc{\'i}a, Miguel {\'A}ngel and Puig, Dom{\`e}nec},
  year = 2021,
  month = dec,
  volume = {11},
  pages = {22430--22440}
}

@misc{carrascoTYOLOTinyVehicle2021a,
  title = {T-{{YOLO}}: {{Tiny Vehicle Detection Based}} on {{YOLO}} and {{Multi-Scale Convolutional Neural Networks}}},
  author = {Carrasco, D. and Rashwan, Hatem A. and Garc{\'i}a, Miguel {\'A}ngel and Puig, Dom{\`e}nec},
  year = 2021,
  month = dec,
  journal = {Institute of Electrical and Electronics Engineers},
  volume = {11},
  pages = {22430--22440},
  doi = {10.1109/access.2021.3137638}
}

@misc{changYOLORBasedMultiTaskLearning2023,
  title = {{{YOLOR-Based Multi-Task Learning}}},
  author = {Chang, Hung-Shuo and Wang, Chien-Yao and Wang, Richard Robert and Chou, Gene and Liao, Hong-Yuan Mark},
  year = 2023,
  month = sep,
  number = {arXiv:2309.16921},
  eprint = {2309.16921},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-19},
  abstract = {Multi-task learning (MTL) aims to learn multiple tasks using a single model and jointly improve all of them assuming generalization and shared semantics. Reducing conflicts between tasks during joint learning is difficult and generally requires careful network design and extremely large models. We propose building on You Only Learn One Representation (YOLOR) (Wang et al., 2023c), a network architecture specifically designed for multitasking. YOLOR leverages both explicit and implicit knowledge, from data observations and learned latents, respectively, to improve a shared representation while minimizing the number of training parameters. However, YOLOR and its follow-up, YOLOv7 (Wang et al., 2023a), only trained two tasks at once. In this paper, we jointly train object detection, instance segmentation, semantic segmentation, and image captioning. We analyze tradeoffs and attempt to maximize sharing of semantic information. Through our architecture and training strategies, we find that our method achieves competitive performance on all tasks while maintaining a low parameter count and without any pre-training. We will release code soon.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dan/Zotero/storage/5LJY5KSL/Chang et al. - 2023 - YOLOR-Based Multi-Task Learning.pdf}
}

@inproceedings{chen2014a,
  title = {{{DianNao}}: {{A}} Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning},
  booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
  author = {Chen, T. and Du, Z. and Sun, N. and Wu, J. and Chen, X. and Temam, O. and Tang, T.},
  year = 2014,
  pages = {269--284},
  citation-number = {27},
  langid = {english}
}

@article{chen2016a,
  title = {Eyeriss: {{An}} Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks},
  author = {Chen, T.},
  year = 2016,
  journal = {IEEE Journal of Solid-State Circuits},
  volume = {52},
  number = {1},
  pages = {127--138},
  citation-number = {56},
  langid = {english}
}

@article{chen2016a,
  title = {Eyeriss: {{An}} Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks},
  author = {Chen, T.},
  year = 2016,
  journal = {IEEE Journal of Solid-State Circuits},
  volume = {52},
  number = {1},
  pages = {127--138},
  citation-number = {56},
  langid = {english}
}

@article{chen2016a,
  title = {Eyeriss: {{An}} Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks},
  author = {Chen, T.},
  year = 2016,
  journal = {IEEE Journal of Solid-State Circuits},
  volume = {52},
  number = {1},
  pages = {127--138},
  citation-number = {56},
  langid = {english}
}

@article{chen2019a,
  title = {Underwater Image Enhancement Based on Color Restoration and Image Dehazing},
  author = {Chen, J. and Zou, D. and Zeng, H. and Wu, Q.},
  year = 2019,
  journal = {Computational Intelligence and Neuroscience},
  citation-number = {21},
  langid = {english}
}

@article{chen2019a,
  title = {Underwater Object Detection Using Multi-Scale Features and Deep Residual Networks},
  author = {Chen, H.},
  year = 2019,
  journal = {Journal of Marine Science and Engineering},
  volume = {7},
  number = {11},
  pages = {373},
  citation-number = {36},
  langid = {english}
}

@article{chen2019a,
  title = {Underwater Object Detection Using Multi-Scale Features and Deep Residual Networks},
  author = {Chen, H.},
  year = 2019,
  journal = {Journal of Marine Science and Engineering},
  volume = {7},
  number = {11},
  pages = {373},
  citation-number = {36},
  langid = {english}
}

@article{chen2019a,
  title = {Underwater Object Detection Using Multi-Scale Features and Deep Residual Networks},
  author = {Chen, H.},
  year = 2019,
  journal = {Journal of Marine Science and Engineering},
  volume = {7},
  number = {11},
  pages = {373},
  citation-number = {36},
  langid = {english}
}

@article{cheng2018a,
  title = {Model Compression and Acceleration for Deep Neural Networks: {{The}} Principles, Progress, and Challenges},
  author = {Cheng, Y.},
  year = 2018,
  journal = {IEEE Signal Processing Magazine},
  volume = {35},
  number = {1},
  pages = {126--136},
  citation-number = {49},
  langid = {english}
}

@article{cheng2018a,
  title = {Model Compression and Acceleration for Deep Neural Networks: {{The}} Principles, Progress, and Challenges},
  author = {Cheng, Y.},
  year = 2018,
  journal = {IEEE Signal Processing Magazine},
  volume = {35},
  number = {1},
  pages = {126--136},
  citation-number = {49},
  langid = {english}
}

@article{cheng2018a,
  title = {Model Compression and Acceleration for Deep Neural Networks: {{The}} Principles, Progress, and Challenges},
  author = {Cheng, Y.},
  year = 2018,
  journal = {IEEE Signal Processing Magazine},
  volume = {35},
  number = {1},
  pages = {126--136},
  citation-number = {49},
  langid = {english}
}

@misc{chengYOLOWorldRealTimeOpenVocabulary2024,
  title = {{{YOLO-World}}: {{Real-Time Open-Vocabulary Object Detection}}},
  shorttitle = {{{YOLO-World}}},
  author = {Cheng, Tianheng and Song, Lin and Ge, Yixiao and Liu, Wenyu and Wang, Xinggang and Shan, Ying},
  year = 2024,
  month = feb,
  number = {arXiv:2401.17270},
  eprint = {2401.17270},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-20},
  abstract = {The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools. However, their reliance on predefined and trained object categories limits their applicability in open scenarios. Addressing this limitation, we introduce YOLO-World, an innovative approach that enhances YOLO with openvocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. Specifically, we propose a new Re-parameterizable VisionLanguage Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information. Our method excels in detecting a wide range of objects in a zero-shot manner with high efficiency. On the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on V100, which outperforms many state-of-the-art methods in terms of both accuracy and speed. Furthermore, the fine-tuned YOLO-World achieves remarkable performance on several downstream tasks, including object detection and openvocabulary instance segmentation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Work still in progress. Code \& models are available at: https://github.com/AILab-CVC/YOLO-World},
  file = {/Users/dan/Zotero/storage/Y2DEKA75/Cheng et al. - 2024 - YOLO-World Real-Time Open-Vocabulary Object Detec.pdf}
}

@article{chiang2012a,
  title = {Underwater Image Enhancement by Wavelength Compensation and Dehazing},
  author = {Chiang, J.Y. and Chen, Y.C.},
  year = 2012,
  journal = {IEEE Transactions on Image Processing},
  volume = {21},
  number = {4},
  pages = {1756--1769},
  citation-number = {16},
  langid = {english}
}

@misc{CopyNPWEETeam,
  title = {Copy of {{NPWEE Team}} 29 {{Proposal}}},
  journal = {Google Docs},
  urldate = {2024-07-17},
  abstract = {Real-time Terrain Mapping with Field Programmable Gate Arrays  Personal Investigator: Kyle Villamayor kvillamayor26@gmail.com 551-339-4090  Subject Matter Expert: George Sklivanitis, Ph.D gsklivanitis@fau.edu  561-297-1163  Pending Subject Matter:   Project Manager: Jadah Malloy  Chief Scientist:...},
  howpublished = {https://docs.google.com/document/u/1/d/1t8I7aLJTkBira5bL\_Z6X0\_tHd5DprZotAy5MOL\_mfTc/edit?fromCopy=true\&ct=2\&usp=embed\_facebook},
  langid = {english},
  file = {/Users/dan/Zotero/storage/JEMW4FZ5/edit.html}
}

@book{corporation2021a,
  author = {Corporation, N.V.I.D.I.A.},
  year = 2021,
  publisher = {NVIDIA TensorRT},
  citation-number = {42},
  langid = {albanian}
}

@book{corporation2021a,
  author = {Corporation, N.V.I.D.I.A.},
  year = 2021,
  publisher = {NVIDIA TensorRT},
  citation-number = {59},
  langid = {albanian}
}

@book{corporation2021a,
  author = {Corporation, N.V.I.D.I.A.},
  year = 2021,
  publisher = {NVIDIA TensorRT},
  citation-number = {59},
  langid = {albanian}
}

@book{corporation2021a,
  title = {{NVIDIA TensorRT}},
  author = {Corporation, N.V.I.D.I.A.},
  year = 2021,
  publisher = {NVIDIA TensorRT},
  citation-number = {59},
  langid = {albanian}
}

@misc{corporation2022a,
  title = {{NVIDIA jetson orin nano developer kit}},
  author = {Corporation, N.V.I.D.I.A.},
  year = 2022,
  citation-number = {41},
  langid = {bulgarian}
}

@misc{corporation2022a,
  title = {{NVIDIA jetson orin nano developer kit}},
  author = {Corporation, N.V.I.D.I.A.},
  year = 2022,
  citation-number = {48},
  langid = {bulgarian}
}

@misc{corporation2022a,
  title = {{NVIDIA jetson orin nano developer kit}},
  author = {Corporation, N.V.I.D.I.A.},
  year = 2022,
  citation-number = {48},
  langid = {bulgarian}
}

@misc{corporation2022a,
  title = {{NVIDIA jetson orin nano developer kit}},
  author = {Corporation, N.V.I.D.I.A.},
  year = 2022,
  citation-number = {48},
  langid = {bulgarian}
}

@article{costello2013a,
  title = {Can We Name {{Earth}}'s Species before They Go Extinct?},
  author = {Costello, M.J. and May, R.M. and Stork, N.E.},
  year = 2013,
  journal = {Science},
  volume = {339},
  number = {6118},
  pages = {413--416},
  citation-number = {7},
  langid = {english}
}

@article{costelloCanWeName2013,
  title = {Can {{We Name Earth}}'s {{Species Before They Go Extinct}}?},
  author = {Costello, Mark J. and May, Robert M. and Stork, Nigel E.},
  year = 2013,
  month = jan,
  journal = {Science},
  volume = {339},
  number = {6118},
  pages = {413--416},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1230318},
  urldate = {2024-10-20},
  abstract = {Completing the Catalog                            Despite the widely held belief that the number of taxonomists is decreasing, there is evidence that increasing numbers of authors are describing species new to science. In parallel, several statistically sophisticated attempts have been made to better quantify the number of species that may exist on Earth, including the oceans. Estimates of recent extinction rates have also been re-examined to question whether we are in, or heading toward, an anthropogenic mass extinction event.                                Costello                 et al.                              (p.               413               ) review these findings, provide hope that science will describe most species within this century, and suggest how this complete description can be facilitated.                        ,              Some people despair that most species will go extinct before they are discovered. However, such worries result from overestimates of how many species may exist, beliefs that the expertise to describe species is decreasing, and alarmist estimates of extinction rates. We argue that the number of species on Earth today is 5 \textpm{} 3 million, of which 1.5 million are named. New databases show that there are more taxonomists describing species than ever before, and their number is increasing faster than the rate of species description. Conservation efforts and species survival in secondary habitats are at least delaying extinctions. Extinction rates are, however, poorly quantified, ranging from 0.01 to 1\% (at most 5\%) per decade. We propose practical actions to improve taxonomic productivity and associated understanding and conservation of biodiversity.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/6AC2KMR5/Costello et al. - 2013 - Can We Name Earth's Species Before They Go Extinct.pdf}
}

@article{costelloCanWeName2013a,
  title = {Can We Name {{Earth}}'s Species before They Go Extinct?},
  author = {Costello, M.J. and May, R.M. and Stork, N.E.},
  year = 2013,
  journal = {Science},
  volume = {339},
  number = {6118},
  pages = {413--416},
  citation-number = {7},
  langid = {english}
}

@article{cozzolinoMachineLearningPredict2023,
  title = {Machine Learning to Predict Overall Short-Term Mortality in Cutaneous Melanoma},
  author = {Cozzolino, C. and Buja, A. and Rugge, M. and Miatton, A. and Zorzi, M. and Vecchiato, A. and Del Fiore, P. and Tropea, S. and Brazzale, A. and Damiani, G. and {dall'Olmo}, L. and Rossi, C. R. and Mocellin, S.},
  year = 2023,
  month = jan,
  journal = {Discov Onc},
  volume = {14},
  number = {1},
  pages = {13},
  issn = {2730-6011},
  doi = {10.1007/s12672-023-00622-5},
  urldate = {2024-04-21},
  abstract = {Background\enspace{} Cutaneous malignant melanoma (CMM) ranks among the ten most frequent malignancies, clinicopathological staging being of key importance to predict prognosis. Artificial intelligence (AI) has been recently applied to develop prognostically reliable staging systems for CMM. This study aims to provide a useful machine learning based tool to predict the overall CMM short-term survival. Methods\enspace{} CMM records as collected at the Veneto Cancer Registry (RTV) and at the Veneto regional health service were considered. A univariate Cox regression validated the strength and direction of each independent variable with overall mortality. A range of machine learning models (Logistic Regression classifier, Support-Vector Machine, Random Forest, Gradient Boosting, and k-Nearest Neighbors) and a Deep Neural Network were then trained to predict the 3-years mortality probability. Five-fold cross-validation and Grid Search were performed to test the best data preprocessing procedures, features selection, and to optimize models hyperparameters. A final evaluation was carried out on a separate test set in terms of balanced accuracy, precision, recall and F1 score. The best model was deployed as online tool. Results\enspace{} The univariate analysis confirmed the significant prognostic value of TNM staging. Adjunctive clinicopathological variables not included in the AJCC 8th melanoma staging system, i.e., sex, tumor site, histotype, growth phase, and age, were significantly linked to overall survival. Among the models, the Neural Network and the Random Forest models featured the best prognostic performance, achieving a balanced accuracy of 91\% and 88\%, respectively. According to the Gini importance score, age, T and M stages, mitotic count, and ulceration appeared to be the variables with the greatest impact on survival prediction. Conclusions\enspace{} Using data from patients with CMM, we developed an AI algorithm with high staging reliability, on top of which a web tool was implemented (unipd.link/melanomaprediction). Being essentially based on routinely recorded clinicopathological variables, it can already be implemented with minimal effort and further tested in the current clinical practice, an essential phase for validating the model's accuracy beyond the original research context.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/NE87IEB4/Cozzolino et al. - 2023 - Machine learning to predict overall short-term mor.pdf}
}

@article{cuiFishDetectionUsing2020,
  title = {Fish {{Detection Using Deep Learning}}},
  author = {Cui, Suxia and Zhou, Yu and Wang, Yonghui and Zhai, Lujun},
  year = 2020,
  month = jan,
  journal = {Applied Computational Intelligence and Soft Computing},
  volume = {2020},
  pages = {1--13},
  issn = {1687-9724, 1687-9732},
  doi = {10.1155/2020/3738108},
  urldate = {2024-04-23},
  abstract = {Recently, human being's curiosity has been expanded from the land to the sky and the sea. Besides sending people to explore the ocean and outer space, robots are designed for some tasks dangerous for living creatures. Take the ocean exploration for an example. There are many projects or competitions on the design of Autonomous Underwater Vehicle (AUV) which attracted many interests. Authors of this article have learned the necessity of platform upgrade from a previous AUV design project, and would like to share the experience of one task extension in the area of fish detection. Because most of the embedded systems have been improved by fast growing computing and sensing technologies, which makes them possible to incorporate more and more complicated algorithms. In an AUV, after acquiring surrounding information from sensors, how to perceive and analyse corresponding information for better judgement is one of the challenges. The processing procedure can mimic human being's learning routines. An advanced system with more computing power can facilitate deep learning feature, which exploit many neural network algorithms to simulate human brains. In this paper, a convolutional neural network (CNN) based fish detection method was proposed. The training data set was collected from the Gulf of Mexico by a digital camera. To fit into this unique need, three optimization approaches were applied to the CNN: data augmentation, network simplification, and training process speed up. Data augmentation transformation provided more learning samples; the network was simplified to accommodate the artificial neural network; the training process speed up is introduced to make the training process more time efficient. Experimental results showed that the proposed model is promising, and has the potential to be extended to other underwear objects.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/dan/Zotero/storage/TG4W6WYP/Cui et al. - 2020 - Fish Detection Using Deep Learning.pdf}
}

@misc{cuiFishDetectionUsing2020a,
  title = {Fish {{Detection Using Deep Learning}}},
  author = {Cui, Suxia and Zhou, Yu and Wang, Yonghui and Zhai, Lujun},
  year = 2020,
  month = jan,
  volume = {2020},
  pages = {1--13}
}

@misc{cuiFishDetectionUsing2020b,
  title = {Fish {{Detection Using Deep Learning}}},
  author = {Cui, Suxia and Zhou, Yu and Wang, Yonghui and Zhai, Lujun},
  year = 2020,
  month = jan,
  journal = {Hindawi Publishing Corporation},
  volume = {2020},
  pages = {1--13},
  doi = {10.1155/2020/3738108}
}

@misc{deci-aiYOLONAS,
  title = {{{YOLO-NAS}}},
  author = {{Deci-AI}}
}

@article{doney2012a,
  title = {Climate Change Impacts on Marine Ecosystems},
  author = {Doney, S.C. and Ruckelshaus, M. and Duffy, J.E. and Barry, J.P. and Chan, F. and English, C.A. and Talley, L.D.},
  year = 2012,
  journal = {Annual Review of Marine Science},
  volume = {4},
  pages = {11--37},
  citation-number = {6},
  langid = {english}
}

@article{doneyClimateChangeImpacts2012,
  title = {Climate {{Change Impacts}} on {{Marine Ecosystems}}},
  author = {Doney, Scott C. and Ruckelshaus, Mary and Emmett Duffy, J. and Barry, James P. and Chan, Francis and English, Chad A. and Galindo, Heather M. and Grebmeier, Jacqueline M. and Hollowed, Anne B. and Knowlton, Nancy and Polovina, Jeffrey and Rabalais, Nancy N. and Sydeman, William J. and Talley, Lynne D.},
  year = 2012,
  month = jan,
  journal = {Annu. Rev. Mar. Sci.},
  volume = {4},
  number = {1},
  pages = {11--37},
  issn = {1941-1405, 1941-0611},
  doi = {10.1146/annurev-marine-041911-111611},
  urldate = {2024-10-20},
  abstract = {In marine ecosystems, rising atmospheric CO2 and climate change are associated with concurrent shifts in temperature, circulation, stratification, nutrient input, oxygen content, and ocean acidification, with potentially wideranging biological effects. Population-level shifts are occurring because of physiological intolerance to new environments, altered dispersal patterns, and changes in species interactions. Together with local climate-driven invasion and extinction, these processes result in altered community structure and diversity, including possible emergence of novel ecosystems. Impacts are particularly striking for the poles and the tropics, because of the sensitivity of polar ecosystems to sea-ice retreat and poleward species migrations as well as the sensitivity of coral-algal symbiosis to minor increases in temperature. Midlatitude upwelling systems, like the California Current, exhibit strong linkages between climate and species distributions, phenology, and demography. Aggregated effects may modify energy and material flows as well as biogeochemical cycles, eventually impacting the overall ecosystem functioning and services upon which people and societies depend.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/JKXBQQUV/Doney et al. - 2012 - Climate Change Impacts on Marine Ecosystems.pdf}
}

@inproceedings{drews2013a,
  title = {Transmission Estimation in Underwater Single Images},
  booktitle = {2013 {{IEEE}} International Conference on Computer Vision Workshops},
  author = {Drews, Jr., P.L.},
  year = 2013,
  pages = {825--830},
  publisher = {IEEE},
  citation-number = {17},
  langid = {english}
}

@inproceedings{drews2013a,
  title = {Transmission Estimation in Underwater Single Images},
  booktitle = {2013 {{IEEE}} International Conference on Computer Vision Workshops},
  author = {Drews, Jr., P.L.},
  year = 2013,
  pages = {825--830},
  publisher = {IEEE},
  citation-number = {17},
  langid = {english}
}

@inproceedings{drews2013a,
  title = {Transmission Estimation in Underwater Single Images},
  booktitle = {2013 {{IEEE}} International Conference on Computer Vision Workshops},
  author = {Drews, Jr., P.L.},
  year = 2013,
  pages = {825--830},
  publisher = {IEEE},
  citation-number = {17},
  langid = {english}
}

@article{EfficientDorsalFinbased2024,
  title = {Efficient Dorsal Fin-Based Classification of {{Risso}}'s and Common {{Bottlenose}} Dolphins Using {{YOLOv7}} and {{YOLOv8}} Models for Real-Time Applications},
  year = 2024,
  month = jun,
  journal = {IJATEE},
  volume = {11},
  number = {115},
  issn = {23945443, 23947454},
  doi = {10.19101/IJATEE.2023.10102512},
  urldate = {2024-08-28},
  abstract = {The existence of whales and dolphins serves as a key sign of the well-being of the marine environment of that area. It is imperative to undertake research and conservation initiatives to safeguard these marine mammals and their ecosystem. This guarantees their persistence for the well-being of future generations. In recent years, marine surveys conducted in Fujairah offshore waters have generated valuable data concerning the distribution of cetacean species. Notably, common Bottlenose dolphins (Tursiops truncatus) and Risso's dolphins (Grampus griseus) have emerged as prevalent species in the region. These data hold significant information that is useful in species identification and its habitat loss mitigation efforts. Computer vision offers an efficient solution for analysing and interpreting vast visual data compared to of the manual detection methods. Therefore, the primary objective of this study is to assess and contrast the efficacy of you only look once version 7 (YOLOv7) and you only look once version 8 (YOLOv8) models in the identification of cetacean species. The findings indicate that both models exhibit strong performance in identifying and categorizing the desired species. Specifically, YOLOv8 demonstrates a slightly superior precision rate of 91.6\% compared to YOLOv7. Additionally, YOLOv8 exhibits improved recall (92.5\%) and mean average precision (mAP) of 95.9\%. The improved performance of YOLOv8 can be attributed to its comprehensive feature map and optimised convolutional network, combined with a novel backbone network.},
  langid = {english},
  file = {/Users/dan/Documents/FAU/Thesis/New Lit/dorsal.pdf}
}

@misc{EfficientDorsalFinbased2024a,
  title = {Efficient Dorsal Fin-Based Classification of {{Risso}}'s and Common {{Bottlenose}} Dolphins Using {{YOLOv7}} and {{YOLOv8}} Models for Real-Time Applications},
  year = 2024,
  month = jun,
  volume = {11},
  number = {115}
}

@inproceedings{emberton2015a,
  title = {Hierarchical Rank-Based Veiling Light Estimation for Underwater Dehazing},
  booktitle = {2015 {{IEEE}} International Conference on Image Processing},
  author = {Emberton, S.},
  year = 2015,
  pages = {636--640},
  publisher = {IEEE},
  citation-number = {13},
  langid = {english}
}

@inproceedings{emberton2015a,
  title = {Hierarchical Rank-Based Veiling Light Estimation for Underwater Dehazing},
  booktitle = {2015 {{IEEE}} International Conference on Image Processing},
  author = {Emberton, S.},
  year = 2015,
  pages = {636--640},
  publisher = {IEEE},
  citation-number = {13},
  langid = {english}
}

@inproceedings{emberton2015a,
  title = {Hierarchical Rank-Based Veiling Light Estimation for Underwater Dehazing},
  booktitle = {2015 {{IEEE}} International Conference on Image Processing},
  author = {Emberton, S.},
  year = 2015,
  pages = {636--640},
  publisher = {IEEE},
  citation-number = {13},
  langid = {english},
  file = {/Users/dan/Zotero/storage/RGV25QQ6/Emberton - 2015 - Hierarchical rank-based veiling light estimation f.pdf}
}

@article{erResearchChallengesRecent2023,
  title = {Research {{Challenges}}, {{Recent Advances}}, and {{Popular Datasets}} in {{Deep Learning-Based Underwater Marine Object Detection}}: {{A Review}}},
  shorttitle = {Research {{Challenges}}, {{Recent Advances}}, and {{Popular Datasets}} in {{Deep Learning-Based Underwater Marine Object Detection}}},
  author = {Er, Meng Joo and Chen, Jie and Zhang, Yani and Gao, Wenxiao},
  year = 2023,
  month = feb,
  journal = {Sensors},
  volume = {23},
  number = {4},
  pages = {1990},
  issn = {1424-8220},
  doi = {10.3390/s23041990},
  urldate = {2024-10-19},
  abstract = {Underwater marine object detection, as one of the most fundamental techniques in the community of marine science and engineering, has been shown to exhibit tremendous potential for exploring the oceans in recent years. It has been widely applied in practical applications, such as monitoring of underwater ecosystems, exploration of natural resources, management of commercial fisheries, etc. However, due to complexity of the underwater environment, characteristics of marine objects, and limitations imposed by exploration equipment, detection performance in terms of speed, accuracy, and robustness can be dramatically degraded when conventional approaches are used. Deep learning has been found to have significant impact on a variety of applications, including marine engineering. In this context, we offer a review of deep learning-based underwater marine object detection techniques. Underwater object detection can be performed by different sensors, such as acoustic sonar or optical cameras. In this paper, we focus on vision-based object detection due to several significant advantages. To facilitate a thorough understanding of this subject, we organize research challenges of vision-based underwater object detection into four categories: image quality degradation, small object detection, poor generalization, and real-time detection. We review recent advances in underwater marine object detection and highlight advantages and disadvantages of existing solutions for each challenge. In addition, we provide a detailed critical examination of the most extensively used datasets. In addition, we present comparative studies with previous reviews, notably those approaches that leverage artificial intelligence, as well as future trends related to this hot topic.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/dan/Zotero/storage/7TWQJWXC/Er et al. - 2023 - Research Challenges, Recent Advances, and Popular .pdf}
}

@inproceedings{ester1996a,
  title = {A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise},
  booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
  author = {Ester, M.},
  year = 1996,
  pages = {226--231},
  citation-number = {45},
  langid = {english}
}

@inproceedings{ester1996a,
  title = {A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise},
  booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
  author = {Ester, M.},
  year = 1996,
  pages = {226--231},
  citation-number = {45},
  langid = {english}
}

@inproceedings{ester1996a,
  title = {A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise},
  booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
  author = {Ester, M.},
  year = 1996,
  pages = {226--231},
  citation-number = {45},
  langid = {english}
}

@article{eunjijeongDeepLearningInference2022,
  title = {Deep {{Learning Inference Parallelization}} on {{Heterogeneous Processors With TensorRT}}},
  author = {{Eunji Jeong} and {Jangryul Kim} and {Samnieng Tan} and {Jaeseong Lee} and {Soonhoi Ha}},
  year = 2022,
  month = mar,
  journal = {IEEE Embedded Systems Letters},
  volume = {14},
  pages = {15--18},
  doi = {10.1109/LES.2021.3087707},
  file = {/Users/dan/Zotero/storage/NRMBJUFV/Eunji Jeong et al. - 2022 - Deep Learning Inference Parallelization on Heterog.pdf}
}

@article{eunjijeongTensorRTBasedFrameworkOptimization2022,
  title = {{{TensorRT-Based Framework}} and {{Optimization Methodology}} for {{Deep Learning Inference}} on {{Jetson Boards}}},
  author = {{Eunji Jeong} and {Jangryul Kim} and {Soonhoi Ha}},
  year = 2022,
  month = jan,
  journal = {ACM Transactions on Embedded Computing Systems (TECS)},
  volume = {21},
  pages = {1--26},
  doi = {10.1145/3508391},
  file = {/Users/dan/Zotero/storage/NE7GJ3AL/Eunji Jeong et al. - 2022 - TensorRT-Based Framework and Optimization Methodol.pdf}
}

@misc{fangYouOnlyLook2021,
  title = {You {{Only Look}} at {{One Sequence}}: {{Rethinking Transformer}} in {{Vision}} through {{Object Detection}}},
  shorttitle = {You {{Only Look}} at {{One Sequence}}},
  author = {Fang, Yuxin and Liao, Bencheng and Wang, Xinggang and Fang, Jiemin and Qi, Jiyang and Wu, Rui and Niu, Jianwei and Liu, Wenyu},
  year = 2021,
  month = oct,
  number = {arXiv:2106.00666},
  eprint = {2106.00666},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-20},
  abstract = {Can Transformer perform 2D object- and region-level recognition from a pure sequence-to-sequence perspective with minimal knowledge about the 2D spatial structure? To answer this question, we present You Only Look at One Sequence (YOLOS), a series of object detection models based on the vanilla Vision Transformer with the fewest possible modifications, region priors, as well as inductive biases of the target task. We find that YOLOS pre-trained on the mid-sized ImageNet-1k dataset only can already achieve quite competitive performance on the challenging COCO object detection benchmark, e.g., YOLOS-Base directly adopted from BERT-Base architecture can obtain 42.0 box AP on COCO val. We also discuss the impacts as well as limitations of current pre-train schemes and model scaling strategies for Transformer in vision through YOLOS. Code and pre-trained models are available at https://github.com/hustvl/YOLOS.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: NeurIPS 2021 Camera Ready},
  file = {/Users/dan/Zotero/storage/797ZLWH9/Fang et al. - 2021 - You Only Look at One Sequence Rethinking Transfor.pdf}
}

@article{fanLightweightModelUnderwater2024,
  title = {A {{Lightweight Model}} of {{Underwater Object Detection Based}} on {{YOLOv8n}} for an {{Edge Computing Platform}}},
  author = {Fan, Yibing and Zhang, Lanyong and Li, Peng},
  year = 2024,
  month = apr,
  journal = {JMSE},
  volume = {12},
  number = {5},
  pages = {697},
  issn = {2077-1312},
  doi = {10.3390/jmse12050697},
  urldate = {2024-06-25},
  abstract = {The visual signal object detection technology of deep learning, as a high-precision perception technology, can be adopted in various image analysis applications, and it has important application prospects in the utilization and protection of marine biological resources. While the marine environment is generally far from cities where the rich computing power in cities cannot be utilized, deploying models on mobile edge devices is an efficient solution. However, because of computing resource limitations on edge devices, the workload of performing deep learning-based computationally intensive object detection on mobile edge devices is often insufficient in meeting high-precision and low-latency requirements. To address the problem of insufficient computing resources, this paper proposes a lightweight process based on a neural structure search and knowledge distillation using deep learning YOLOv8 as the baseline model. Firstly, the neural structure search algorithm was used to compress the YOLOv8 model and reduce its computational complexity. Secondly, a new knowledge distillation architecture was designed, which distills the detection head output layer and NECK feature layer to compensate for the accuracy loss caused by model reduction. When compared to YOLOv8n, the computational complexity of the lightweight model optimized in this study (in terms of floating point operations (FLOPs)) was 7.4 Gflops, which indicated a reduction of 1.3 Gflops. The multiply--accumulate operations (MACs) stood at 2.72 G, thereby illustrating a decrease of 32\%; this saw an increase in the AP50, AP75, and mAP by 2.0\%, 3.0\%, and 1.9\%, respectively. Finally, this paper designed an edge computing service architecture, and it deployed the model on the Jetson Xavier NX platform through TensorRT.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/dan/Zotero/storage/UAWEVDJW/Fan et al. - 2024 - A Lightweight Model of Underwater Object Detection.pdf}
}

@article{fanta-jendeSemanticRealTimeMapping2023,
  title = {Semantic {{Real-Time Mapping}} with {{UAVs}}},
  author = {{Fanta-Jende}, Phillipp and Steininger, Daniel and Kern, Alexander and Widhalm, Verena and Apud Baca, Javier G. and Hofst{\"a}tter, Markus and Simon, Julia and Bruckm{\"u}ller, Felix and Sulzbachner, Christoph},
  year = 2023,
  month = jun,
  journal = {PFG},
  volume = {91},
  number = {3},
  pages = {157--170},
  issn = {2512-2789, 2512-2819},
  doi = {10.1007/s41064-023-00242-2},
  urldate = {2024-06-27},
  abstract = {Whilst mapping with UAVs has become an established tool for geodata acquisition in many domains, certain time-critical applications, such as crisis and disaster response, demand fast geodata processing pipelines rather than photogrammetric post-processing approaches. Based on our 3D-capable real-time mapping pipeline, this contribution presents not only an array of optimisations of the original implementation but also an extension towards understanding the image content with respect to land cover and object detection using machine learning. This paper (1) describes the pipeline in its entirety, (2) compares the performance of the semantic labelling and object detection models quantitatively and (3) showcases real-world experiments with qualitative evaluations.},
  langid = {english},
  file = {/Users/dan/Downloads/s41064-023-00242-2.pdf}
}

@incollection{ferozObjectDetectionClassification2022,
  title = {Object {{Detection}} and {{Classification}} from a {{Real-Time Video Using SSD}} and {{YOLO Models}}},
  booktitle = {Computational {{Intelligence}} in {{Pattern Recognition}}},
  author = {Feroz, Md. Alamin and Sultana, Marjia and Hasan, Md. Rakib and Sarker, Aditi and Chakraborty, Partha and Choudhury, Tanupriya},
  editor = {Das, Asit Kumar and Nayak, Janmenjoy and Naik, Bighnaraj and Dutta, Soumi and Pelusi, Danilo},
  year = 2022,
  volume = {1349},
  pages = {37--47},
  publisher = {Springer Singapore},
  address = {Singapore},
  doi = {10.1007/978-981-16-2543-5_4},
  urldate = {2024-10-20},
  isbn = {978-981-16-2542-8 978-981-16-2543-5},
  langid = {english}
}

@article{FieldProgrammableGate2017,
  title = {Field {{Programmable Gate Array}}--Based {{Implementation}} of an {{Improved Algorithm}} for {{Objects Distance Measurement}}},
  year = 2017,
  month = jan,
  journal = {IJE},
  volume = {30},
  number = {1},
  issn = {17281431, 17359244},
  doi = {10.5829/idosi.ije.2017.30.01a.08},
  urldate = {2024-07-17}
}

@inproceedings{fisher2016a,
  title = {{{Fish4Knowledge}}: {{Collecting}} and Analyzing Massive Coral Reef Fish Video Data},
  booktitle = {2016 {{IEEE}} International Conference on Big Data},
  author = {Fisher, R.B.},
  year = 2016,
  pages = {2182--2189},
  citation-number = {1},
  langid = {english}
}

@inproceedings{fisher2016a,
  title = {{{Fish4Knowledge}}: {{Collecting}} and Analyzing Massive Coral Reef Fish Video Data},
  booktitle = {2016 {{IEEE}} International Conference on Big Data},
  author = {Fisher, R.B.},
  year = 2016,
  pages = {2182--2189},
  citation-number = {1},
  langid = {english}
}

@inproceedings{fisher2016a,
  title = {{{Fish4Knowledge}}: {{Collecting}} and Analyzing Massive Coral Reef Fish Video Data},
  booktitle = {2016 {{IEEE}} International Conference on Big Data},
  author = {Fisher, R.B.},
  year = 2016,
  pages = {2182--2189},
  citation-number = {1},
  langid = {english}
}

@book{fisherFish4KnowledgeCollectingAnalyzing2016,
  title = {{{Fish4Knowledge}}: {{Collecting}} and {{Analyzing Massive Coral Reef Fish Video Data}}},
  shorttitle = {{{Fish4Knowledge}}},
  editor = {Fisher, Robert B. and {Chen-Burger}, Yun-Heh and Giordano, Daniela and Hardman, Lynda and Lin, Fang-Pang},
  year = 2016,
  series = {Intelligent {{Systems Reference Library}}},
  volume = {104},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-30208-9},
  urldate = {2024-04-23},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-319-30206-5 978-3-319-30208-9},
  langid = {english},
  file = {/Users/dan/Zotero/storage/TIJKPTTH/Fisher et al. - 2016 - Fish4Knowledge Collecting and Analyzing Massive C.pdf}
}

@misc{fisherFish4KnowledgeCollectingAnalyzing2016a,
  title = {{{Fish4Knowledge}}: {{Collecting}} and {{Analyzing Massive Coral Reef Fish Video Data}}},
  author = {Fisher, Robert B. and {Chen-Burger}, Yun-Heh and Giordano, Daniela and Hardman, Lynda and Lin, Fang-Pang},
  year = 2016,
  month = jan,
  volume = {104}
}

@misc{fisherFish4KnowledgeCollectingAnalyzing2016b,
  title = {{{Fish4Knowledge}}: {{Collecting}} and {{Analyzing Massive Coral Reef Fish Video Data}}},
  author = {Fisher, Robert B. and {Chen-Burger}, Yun-Heh and Giordano, Daniela and Hardman, Lynda and Lin, Fang-Pang},
  year = 2016,
  month = jan,
  journal = {Springer Nature},
  doi = {10.1007/978-3-319-30208-9}
}

@misc{frankleLotteryTicketHypothesis2018,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  year = 2018,
  month = jan
}

@misc{frankleLotteryTicketHypothesis2018a,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  year = 2018,
  month = jan,
  journal = {Cornell University},
  doi = {10.48550/arxiv.1803.03635}
}

@misc{frankleLotteryTicketHypothesis2019,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  year = 2019,
  month = mar,
  number = {arXiv:1803.03635},
  eprint = {1803.03635},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-26},
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  note = {Comment: ICLR camera ready},
  file = {/Users/dan/Zotero/storage/Q5PFWEDG/Frankle and Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf;/Users/dan/Zotero/storage/FDBGNI2T/1803.html}
}

@misc{fu2017a,
  title = {{DSSD: Deconvolutional single shot detector}},
  author = {Fu, C.Y.},
  year = 2017,
  eprint = {1701.06659},
  archiveprefix = {arXiv},
  citation-number = {29},
  langid = {catalan},
  note = {arXiv preprint arXiv:1701.06659.}
}

@misc{fu2017a,
  title = {{DSSD: Deconvolutional single shot detector}},
  author = {Fu, C.Y.},
  year = 2017,
  eprint = {1701.06659},
  archiveprefix = {arXiv},
  citation-number = {29},
  langid = {catalan},
  note = {arXiv preprint arXiv:1701.06659.}
}

@misc{fu2017a,
  title = {{DSSD: Deconvolutional single shot detector}},
  author = {Fu, C.Y.},
  year = 2017,
  eprint = {1701.06659},
  archiveprefix = {arXiv},
  citation-number = {29},
  langid = {catalan},
  note = {arXiv preprint arXiv:1701.06659.}
}

@misc{fuDSSDDeconvolutionalSingle2017,
  title = {{{DSSD}} : {{Deconvolutional Single Shot Detector}}},
  shorttitle = {{{DSSD}}},
  author = {Fu, Cheng-Yang and Liu, Wei and Ranga, Ananth and Tyagi, Ambrish and Berg, Alexander C.},
  year = 2017,
  month = jan,
  number = {arXiv:1701.06659},
  eprint = {1701.06659},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-21},
  abstract = {The main contribution of this paper is an approach for introducing additional context into state-of-the-art general object detection. To achieve this we first combine a state-of-the-art classifier (Residual-101[14]) with a fast detection framework (SSD[18]). We then augment SSD+Residual-101 with deconvolution layers to introduce additional large-scale context in object detection and improve accuracy, especially for small objects, calling our resulting system DSSD for deconvolutional single shot detector. While these two contributions are easily described at a high-level, a naive implementation does not succeed. Instead we show that carefully adding additional stages of learned transformations, specifically a module for feed-forward connections in deconvolution and a new output module, enables this new approach and forms a potential way forward for further detection research. Results are shown on both PASCAL VOC and COCO detection. Our DSSD with \$513 \textbackslash times 513\$ input achieves 81.5\% mAP on VOC2007 test, 80.0\% mAP on VOC2012 test, and 33.2\% mAP on COCO, outperforming a state-of-the-art method R-FCN[3] on each dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dan/Zotero/storage/W7NWWXRD/Fu et al. - 2017 - DSSD  Deconvolutional Single Shot Detector.pdf;/Users/dan/Zotero/storage/GT5DTWC2/1701.html}
}

@article{galdranAutomaticRedchannelUnderwater2015,
  title = {Automatic Red-Channel Underwater Image Restoration},
  author = {Galdran, A.},
  year = 2015,
  journal = {Journal of Visual Communication and Image Representation},
  volume = {26},
  pages = {132--145},
  citation-number = {14},
  langid = {english},
  file = {/Users/dan/Zotero/storage/HPYIP33R/Galdran - 2015 - Automatic red-channel underwater image restoration.pdf}
}

@article{gargLowEffortApproach2020,
  title = {A {{Low Effort Approach}} to {{Structured CNN Design Using PCA}}},
  author = {Garg, Isha and Panda, Priyadarshini and Roy, Kaushik},
  year = 2020,
  journal = {IEEE Access},
  volume = {8},
  pages = {1347--1360},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2961960},
  urldate = {2024-06-28},
  abstract = {Deep learning models hold state of the art performance in many fields, yet their design is still based on heuristics or grid search methods that often result in overparametrized networks. This work proposes a method to analyze a trained network and deduce an optimized, compressed architecture that preserves accuracy while keeping computational costs tractable. Model compression is an active field of research that targets the problem of realizing deep learning models in hardware. However, most pruning methodologies tend to be experimental, requiring large compute and time intensive iterations of retraining the entire network. We introduce structure into model design by proposing a single shot analysis of a trained network that serves as a first order, low effort approach to dimensionality reduction, by using PCA (Principal Component Analysis). The proposed method simultaneously analyzes the activations of each layer and considers the dimensionality of the space described by the filters generating these activations. It optimizes the architecture in terms of number of layers, and number of filters per layer without any iterative retraining procedures, making it a viable, low effort technique to design efficient networks. We demonstrate the proposed methodology on AlexNet and VGG style networks on the CIFAR-10, CIFAR-100 and ImageNet datasets, and successfully achieve an optimized architecture with a reduction of up to 3.8X and 9X in the number of operations and parameters respectively, while trading off less than 1\% accuracy. We also apply the method to MobileNet, and achieve 1.7X and 3.9X reduction in the number of operations and parameters respectively, while improving accuracy by almost one percentage point.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {/Users/dan/Downloads/1812.06224v4.pdf}
}

@inproceedings{gasparovicEvaluatingYOLOV5YOLOV62023,
  title = {Evaluating {{YOLOV5}}, {{YOLOV6}}, {{YOLOV7}}, and {{YOLOV8}} in {{Underwater Environment}}: {{Is There Real Improvement}}?},
  shorttitle = {Evaluating {{YOLOV5}}, {{YOLOV6}}, {{YOLOV7}}, and {{YOLOV8}} in {{Underwater Environment}}},
  booktitle = {2023 8th {{International Conference}} on {{Smart}} and {{Sustainable Technologies}} ({{SpliTech}})},
  author = {Ga{\v s}parovi{\'c}, Boris and Mau{\v s}a, Goran and Rukavina, Josip and Lerga, Jonatan},
  year = 2023,
  month = jun,
  pages = {1--4},
  publisher = {IEEE},
  address = {Split/Bol, Croatia},
  doi = {10.23919/SpliTech58164.2023.10193505},
  urldate = {2024-10-22},
  abstract = {This paper compares several new implementations of the YOLO (You Only Look Once) object detection algorithms in harsh underwater environments. Using a dataset collected by a remotely operated vehicle (ROV), we evaluated the performance of YOLOv5, YOLOv6, YOLOv7, and YOLOv8 in detecting objects in challenging underwater conditions. We aimed to determine whether newer YOLO versions are superior to older ones and how much, in terms of object detection performance, for our underwater pipeline dataset. According to our findings, YOLOv5 achieved the highest mean Average Precision (mAP) score, followed by YOLOv7 and YOLOv6. When examining the precision-recall curves, YOLOv5 and YOLOv7 displayed the highest precision and recall values, respectively. Our comparison of the obtained results to those of our previous work using YOLOv4 demonstrates that each version of YOLO detectors provides significant improvement.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-953-290-128-3},
  langid = {english},
  file = {/Users/dan/Zotero/storage/L3V7L3RV/Gaparovi et al. - 2023 - Evaluating YOLOV5, YOLOV6, YOLOV7, and YOLOV8 in U.pdf}
}

@inproceedings{gDetectionElectronicDevices2021,
  title = {Detection of {{Electronic Devices}} in Real Images Using {{Deep Learning Techniques}}},
  booktitle = {2021 5th {{International Conference}} on {{Computer}}, {{Communication}} and {{Signal Processing}} ({{ICCCSP}})},
  author = {G, Krijeshan and P, Raghul and N N, Nachiappan and Beulah, A. and Priyadharshini, R.},
  year = 2021,
  month = may,
  pages = {295--300},
  publisher = {IEEE},
  address = {Chennai, India},
  doi = {10.1109/ICCCSP52374.2021.9465345},
  urldate = {2024-10-20},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-6654-3277-1}
}

@inproceedings{gDetectionElectronicDevices2021a,
  title = {Detection of {{Electronic Devices}} in Real Images Using {{Deep Learning Techniques}}},
  booktitle = {2021 5th {{International Conference}} on {{Computer}}, {{Communication}} and {{Signal Processing}} ({{ICCCSP}})},
  author = {G, Krijeshan and P, Raghul and N N, Nachiappan and Beulah, A. and Priyadharshini, R.},
  year = 2021,
  month = may,
  pages = {295--300},
  publisher = {IEEE},
  address = {Chennai, India},
  doi = {10.1109/ICCCSP52374.2021.9465345},
  urldate = {2024-10-20},
  abstract = {Object Detection from real world scenario is a subset of Computer Vision, that uses state-of-the-art algorithms and techniques in deep learning to identify and locate the objects in an image or video. Latest advancements in Deep Learning, especially Convolutional Neural Networks (CNN) and in the field of image processing has further improved the process of object detection. Deep learning algorithms that are developed over the years aim to solve several challenges associated with object detection which includes localizing the object in an image, classifying the object correctly with a high confidence score and realtime detection of objects. The performance of the existing algorithms involves a tradeoff between accuracy and detection speed. Algorithms like Faster Region based Convolutional Neural Networks (R-CNN) and Single Shot Detector (SSD) that achieved high accuracy in classifying objects were slow in detecting the objects. Such algorithms were not able to keep up with the pace of detection with video input in realtime and thus were not suitable for implementation in critical applications. The drawbacks associated with these algorithms can be eliminated by following a unified one-state approach. The approach is to fully identify and classify the required objects of interest by passing the image only once through the network. This approach thus decreases detection time considerably. You Only Look Once (YOLO) family of algorithms is one such single shot detector that uses CNNs to detect objects. In our work, we have used the YOLOv3 algorithm to develop a model that detects electronic devices. The model was also tested against realtime input from webcam and mean Average Precision (mAP) of YOLOv3 has been computed and compared with another model developed using Faster R-CNN.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-6654-3277-1},
  langid = {english},
  file = {/Users/dan/Zotero/storage/Z3Y68R4Z/G et al. - 2021 - Detection of Electronic Devices in real images usi.pdf}
}

@misc{geYOLOXExceedingYOLO2021,
  title = {{{YOLOX}}: {{Exceeding YOLO Series}} in 2021},
  shorttitle = {{{YOLOX}}},
  author = {Ge, Zheng and Liu, Songtao and Wang, Feng and Li, Zeming and Sun, Jian},
  year = 2021,
  month = aug,
  number = {arXiv:2107.08430},
  eprint = {2107.08430},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-20},
  abstract = {In this report, we present some experienced improvements to YOLO series, forming a new high-performance detector -- YOLOX. We switch the YOLO detector to an anchor-free manner and conduct other advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art results across a large scale range of models: For YOLO-Nano with only 0.91M parameters and 1.08G FLOPs, we get 25.3\% AP on COCO, surpassing NanoDet by 1.8\% AP; for YOLOv3, one of the most widely used detectors in industry, we boost it to 47.3\% AP on COCO, outperforming the current best practice by 3.0\% AP; for YOLOX-L with roughly the same amount of parameters as YOLOv4-CSP, YOLOv5-L, we achieve 50.0\% AP on COCO at a speed of 68.9 FPS on Tesla V100, exceeding YOLOv5-L by 1.8\% AP. Further, we won the 1st Place on Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model. We hope this report can provide useful experience for developers and researchers in practical scenes, and we also provide deploy versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at https://github.com/Megvii-BaseDetection/YOLOX.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dan/Zotero/storage/CV2BTQ5Y/Ge et al. - 2021 - YOLOX Exceeding YOLO Series in 2021.pdf}
}

@inproceedings{gidaris2016a,
  title = {{{LocNet}}: {{Improving}} Localization Accuracy for Object Detection},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Gidaris, S. and Komodakis, N.},
  year = 2016,
  pages = {789--798},
  citation-number = {41},
  langid = {english}
}

@inproceedings{gidaris2016a,
  title = {{{LocNet}}: {{Improving}} Localization Accuracy for Object Detection},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Gidaris, S. and Komodakis, N.},
  year = 2016,
  pages = {789--798},
  citation-number = {41},
  langid = {english}
}

@inproceedings{gidaris2016a,
  title = {{{LocNet}}: {{Improving}} Localization Accuracy for Object Detection},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Gidaris, S. and Komodakis, N.},
  year = 2016,
  pages = {789--798},
  citation-number = {41},
  langid = {english},
  file = {/Users/dan/Zotero/storage/R689L3S2/Gidaris and Komodakis - 2016 - LocNet Improving localization accuracy for object.pdf}
}

@inproceedings{gintert2012a,
  title = {Second-Generation Underwater Landscape Mosaics for Coral Reef Mapping and Monitoring},
  booktitle = {Proceedings of the 12th International Coral Reef Symposium},
  author = {Gintert, B. and Gleason, A.C.R. and Gracias, N. and Reid, R.P. and Kramer, P.},
  year = 2012,
  citation-number = {12},
  langid = {english}
}

@inproceedings{girshick2015a,
  title = {Fast R-{{CNN}}},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Girshick, R.},
  year = 2015,
  pages = {1440--1448},
  citation-number = {40},
  langid = {english}
}

@inproceedings{girshick2015a,
  title = {Fast R-{{CNN}}},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Girshick, R.},
  year = 2015,
  pages = {1440--1448},
  citation-number = {40},
  langid = {english}
}

@inproceedings{girshick2015a,
  title = {Fast R-{{CNN}}},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Girshick, R.},
  year = 2015,
  pages = {1440--1448},
  citation-number = {40},
  langid = {english}
}

@misc{girshickFastRCNN2015,
  title = {Fast {{R-CNN}}},
  author = {Girshick, Ross},
  year = 2015,
  month = sep,
  number = {arXiv:1504.08083},
  eprint = {1504.08083},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-19},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9\texttimes{} faster than R-CNN, is 213\texttimes{} faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3\texttimes{} faster, tests 10\texttimes{} faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https: //github.com/rbgirshick/fast-rcnn.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: To appear in ICCV 2015},
  file = {/Users/dan/Zotero/storage/KCF9NPFP/Girshick - 2015 - Fast R-CNN.pdf}
}

@inproceedings{gomesRobustUnderwaterObject2020,
  title = {Robust {{Underwater Object Detection}} with {{Autonomous Underwater Vehicle}}: {{A Comprehensive Study}}},
  shorttitle = {Robust {{Underwater Object Detection}} with {{Autonomous Underwater Vehicle}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Computing Advancements}}},
  author = {Gomes, Dipta and Saif, A. F. M. Saifuddin and Nandi, Dip},
  year = 2020,
  month = jan,
  pages = {1--10},
  publisher = {ACM},
  address = {Dhaka Bangladesh},
  doi = {10.1145/3377049.3377052},
  urldate = {2024-10-19},
  abstract = {Underwater Object Detection had been one of the most challenging research fields of Computer Vision and Image Processing. Before Computer Vision techniques were used for underwater imaging, all the tasks associated with object detection had to be done manually by marine scientists making the task one of the most tedious and error prone. For this case, Underwater Autonomous Vehicles (UAV) has been developed to capture real time videos for specific object detection. Using different hardware improvements and using many varied forms of algorithms, classification of objects, mainly living objects had been carried with different AUVs and highresolution cameras. Conventional object detection methods of Computer Vision fail to provide accurate detection results due to some challenges faced underwater. For such reasons, object detection underwater needs to be robust, real time and fast also being accurate, for which deep learning approaches are introduced. In this paper, all the works here all the trending underwater object detection techniques are discussed in details and a comprehensive comparative study is carried out.},
  isbn = {978-1-4503-7778-2},
  langid = {english},
  file = {/Users/dan/Zotero/storage/HXWHEJLE/Gomes et al. - 2020 - Robust Underwater Object Detection with Autonomous.pdf}
}

@article{gonzalez-sabbaghSurveyUnderwaterComputer2023,
  title = {A {{Survey}} on {{Underwater Computer Vision}}},
  author = {{Gonz{\'a}lez-Sabbagh}, Salma P. and {Robles-Kelly}, Antonio},
  year = 2023,
  month = dec,
  journal = {ACM Comput. Surv.},
  volume = {55},
  number = {13s},
  pages = {1--39},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3578516},
  urldate = {2024-10-20},
  abstract = {Underwater computer vision has attracted increasing attention in the research community due to the recent advances in underwater platforms such as of rovers, gliders,               autonomous underwater vehicles (AUVs)               , and the like, that now make possible the acquisition of vast amounts of imagery and video for applications such as biodiversity assessment, environmental monitoring, and search and rescue. Despite growing interest, underwater computer vision is still a relatively under-researched area, where the attention in the literature has been paid to the use of computer vision techniques for image restoration and reconstruction, where image formation models and image processing methods are used to recover colour corrected or enhanced images. This is due to the notion that these methods can be used to achieve photometric invariants to perform higher-level vision tasks such as shape recovery and recognition under the challenging and widely varying imaging conditions that apply to underwater scenes. In this paper, we review underwater computer vision techniques for image reconstruction, restoration, recognition, depth, and shape recovery. Further, we review current applications such as biodiversity assessment, management and protection, infrastructure inspection and AUVs navigation, amongst others. We also delve upon the current trends in the field and examine the challenges and opportunities in the area.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/PW8N99DE/Gonzlez-Sabbagh and Robles-Kelly - 2023 - A Survey on Underwater Computer Vision.pdf}
}

@book{gonzalez2002a,
  title = {{Digital image processing}},
  author = {Gonzalez, R.C. and Woods, R.E.},
  year = 2002,
  publisher = {Prentice Hall},
  citation-number = {10},
  langid = {japanese}
}

@book{gonzalez2002a,
  title = {{Digital image processing}},
  author = {Gonzalez, R.C. and Woods, R.E.},
  year = 2002,
  publisher = {Prentice Hall},
  citation-number = {10},
  langid = {japanese}
}

@book{gonzalez2002a,
  title = {{Digital image processing}},
  author = {Gonzalez, R.C. and Woods, R.E.},
  year = 2002,
  publisher = {Prentice Hall},
  citation-number = {10},
  langid = {japanese}
}

@incollection{goodfellow2014a,
  title = {Generative Adversarial Nets},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Goodfellow, I.},
  year = 2014,
  pages = {2672--2680},
  citation-number = {7},
  langid = {english}
}

@incollection{goodfellow2014a,
  title = {Generative Adversarial Nets},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Goodfellow, I.},
  year = 2014,
  pages = {2672--2680},
  citation-number = {7},
  langid = {english}
}

@incollection{goodfellow2014a,
  title = {Generative Adversarial Nets},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Goodfellow, I.},
  year = 2014,
  pages = {2672--2680},
  citation-number = {7},
  langid = {english},
  file = {/Users/dan/Zotero/storage/BMCGGHE3/Goodfellow - 2014 - Generative adversarial nets.pdf}
}

@inproceedings{guo2017a,
  title = {On Calibration of Modern Neural Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  author = {Guo, C.},
  year = 2017,
  pages = {1321--1330},
  citation-number = {43},
  langid = {english}
}

@inproceedings{guo2017a,
  title = {On Calibration of Modern Neural Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  author = {Guo, C.},
  year = 2017,
  pages = {1321--1330},
  citation-number = {43},
  langid = {english}
}

@inproceedings{guo2017a,
  title = {On Calibration of Modern Neural Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  author = {Guo, C.},
  year = 2017,
  pages = {1321--1330},
  citation-number = {43},
  langid = {english}
}

@article{h.zivarianm.soleimanim.h.doostmohammadiFieldProgrammableGate2017,
  title = {Field {{Programmable Gate Array}}--Based {{Implementation}} of an {{Improved Algorithm}} for {{Objects Distance Measurement}}},
  author = {H. Zivarian, M. Soleimani, M. H. DoostMohammadi},
  year = 2017,
  month = jan,
  journal = {IJE},
  volume = {30},
  number = {1},
  issn = {17281431, 17359244},
  doi = {10.5829/idosi.ije.2017.30.01a.08},
  urldate = {2024-06-27},
  file = {/Users/dan/Zotero/storage/LJ37VQBD/2017 - Field Programmable Gate Arraybased Implementation.pdf}
}

@inproceedings{han2016a,
  title = {Deep Compression: {{Compressing}} Deep Neural Networks with Pruning, Trained Quantization and {{Huffman}} Coding},
  booktitle = {International Conference on Learning Representations},
  author = {Han, S.},
  year = 2016,
  citation-number = {50},
  langid = {english}
}

@inproceedings{han2016a,
  title = {Deep Compression: {{Compressing}} Deep Neural Networks with Pruning, Trained Quantization and {{Huffman}} Coding},
  booktitle = {International Conference on Learning Representations},
  author = {Han, S.},
  year = 2016,
  citation-number = {50},
  langid = {english}
}

@inproceedings{han2016a,
  title = {Deep Compression: {{Compressing}} Deep Neural Networks with Pruning, Trained Quantization and {{Huffman}} Coding},
  booktitle = {International Conference on Learning Representations},
  author = {Han, S.},
  year = 2016,
  citation-number = {50},
  langid = {english}
}

@misc{hanhirovaLatencyThroughputCharacterization2018,
  title = {Latency and Throughput Characterization of Convolutional Neural Networks for Mobile Computer Vision},
  author = {Hanhirova, Jussi and K{\"a}m{\"a}r{\"a}inen, Teemu and Sepp{\"a}l{\"a}, Sipi and Siekkinen, Matti and Hirvisalo, Vesa and {Yl{\"a}-J{\"a}{\"a}ski}, Antti},
  year = 2018,
  month = jun,
  doi = {10.1145/3204949.3204975}
}

@misc{hanhirovaLatencyThroughputCharacterization2018a,
  title = {Latency and Throughput Characterization of Convolutional Neural Networks for Mobile Computer Vision},
  author = {Hanhirova, Jussi and K{\"a}m{\"a}r{\"a}inen, Teemu and Sepp{\"a}l{\"a}, Sipi and Siekkinen, Matti and Hirvisalo, Vesa and {Yl{\"a}-J{\"a}{\"a}ski}, Antti},
  year = 2018,
  month = jun
}

@article{he2011a,
  title = {Single Image Haze Removal Using Dark Channel Prior},
  author = {He, K.},
  year = 2011,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {33},
  number = {12},
  pages = {2341--2353},
  citation-number = {16},
  langid = {english}
}

@article{he2011a,
  title = {Single Image Haze Removal Using Dark Channel Prior},
  author = {He, K.},
  year = 2011,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {33},
  number = {12},
  pages = {2341--2353},
  citation-number = {16},
  langid = {english}
}

@article{he2011a,
  title = {Single Image Haze Removal Using Dark Channel Prior},
  author = {He, K.},
  year = 2011,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {33},
  number = {12},
  pages = {2341--2353},
  citation-number = {16},
  langid = {english}
}

@misc{heChannelPruningAccelerating2017,
  title = {Channel {{Pruning}} for {{Accelerating Very Deep Neural Networks}}},
  author = {He, Yihui and Zhang, Xiangyu and Sun, Jian},
  year = 2017,
  month = aug,
  number = {arXiv:1707.06168},
  eprint = {1707.06168},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-21},
  abstract = {In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks.Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction. We further generalize this algorithm to multi-layer and multi-branch cases. Our method reduces the accumulated error and enhance the compatibility with various architectures. Our pruned VGG-16 achieves the state-of-the-art results by 5x speed-up along with only 0.3\% increase of error. More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only 1.4\%, 1.0\% accuracy loss under 2x speed-up respectively, which is significant. Code has been made publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: To be appear at ICCV 2017},
  file = {/Users/dan/Zotero/storage/I9WNGMRX/He et al. - 2017 - Channel Pruning for Accelerating Very Deep Neural .pdf;/Users/dan/Zotero/storage/EIWUM2UC/1707.html}
}

@misc{heChannelPruningAccelerating2017a,
  title = {Channel {{Pruning}} for {{Accelerating Very Deep Neural Networks}}},
  author = {He, Yihui and Zhang, Xiangyu and Sun, Jian},
  year = 2017,
  month = oct
}

@inproceedings{heidemann2012a,
  title = {Research Challenges and Applications for Underwater Sensor Networking},
  booktitle = {Proceedings of the {{IEEE}} Wireless Communications and Networking Conference},
  author = {Heidemann, J.},
  year = 2012,
  pages = {228--235},
  citation-number = {58},
  langid = {english}
}

@inproceedings{heidemann2012a,
  title = {Research Challenges and Applications for Underwater Sensor Networking},
  booktitle = {Proceedings of the {{IEEE}} Wireless Communications and Networking Conference},
  author = {Heidemann, J.},
  year = 2012,
  pages = {228--235},
  citation-number = {58},
  langid = {english}
}

@inproceedings{heidemann2012a,
  title = {Research Challenges and Applications for Underwater Sensor Networking},
  booktitle = {Proceedings of the {{IEEE}} Wireless Communications and Networking Conference},
  author = {Heidemann, J.},
  year = 2012,
  pages = {228--235},
  citation-number = {58},
  langid = {english}
}

@inproceedings{hitamMixtureContrastLimited2013,
  title = {Mixture Contrast Limited Adaptive Histogram Equalization for Underwater Image Enhancement},
  booktitle = {2013 {{International Conference}} on {{Computer Applications Technology}} ({{ICCAT}})},
  author = {Hitam, M. S. and Yussof, Wan Nural Jawahir Hj Wan and Awalludin, Ezmahamrul Afreen and Bachok, Z.},
  year = 2013,
  month = jan,
  pages = {1--5},
  publisher = {IEEE},
  address = {Sousse},
  doi = {10.1109/ICCAT.2013.6522017},
  urldate = {2024-10-09},
  abstract = {Within the last decades, improving the quality of an underwater image has received considerable attention due to poor visibility of the image which is caused by physical properties of the water medium. This paper presents a new method called mixture Contrast Limited Adaptive Histogram Equalization (CLAHE) color models that specifically developed for underwater image enhancement. The method operates CLAHE on RGB and HSV color models and both results are combined together using Euclidean norm. The underwater images used in this study were taken from Redang Island and Bidong Island in Terengganu, Malaysia. Experimental results show that the proposed approach significantly improves the visual quality of underwater images by enhancing contrast, as well as reducing noise and artifacts.},
  isbn = {978-1-4673-5285-7 978-1-4673-5284-0 978-1-4673-5283-3},
  langid = {english},
  file = {/Users/dan/Zotero/storage/EQUSTZBA/Hitam et al. - 2013 - Mixture contrast limited adaptive histogram equali.pdf}
}

@article{hongkhaiUnderwaterFishDetection2022,
  title = {Underwater {{Fish Detection}} and {{Counting Using Mask Regional Convolutional Neural Network}}},
  author = {Hong Khai, Teh and Abdullah, Siti Norul Huda Sheikh and Hasan, Mohammad Kamrul and Tarmizi, Ahmad},
  year = 2022,
  month = jan,
  journal = {Water},
  volume = {14},
  number = {2},
  pages = {222},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2073-4441},
  doi = {10.3390/w14020222},
  urldate = {2024-04-22},
  abstract = {Fish production has become a roadblock to the development of fish farming, and one of the issues encountered throughout the hatching process is the counting procedure. Previous research has mainly depended on the use of non-machine learning-based and machine learning-based counting methods and so was unable to provide precise results. In this work, we used a robotic eye camera to capture shrimp photos on a shrimp farm to train the model. The image data were classified into three categories based on the density of shrimps: low density, medium density, and high density. We used the parameter calibration strategy to discover the appropriate parameters and provided an improved Mask Regional Convolutional Neural Network (Mask R-CNN) model. As a result, the enhanced Mask R-CNN model can reach an accuracy rate of up to 97.48\%.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {counting,deep learning,machine learning,shrimp detection,underwater fish},
  file = {/Users/dan/Zotero/storage/W4INUQJ9/Hong Khai et al. - 2022 - Underwater Fish Detection and Counting Using Mask .pdf}
}

@misc{howard2017a,
  title = {{{MobileNets}}: {{Efficient}} Convolutional Neural Networks for Mobile Vision Applications},
  author = {Howard, A.G.},
  year = 2017,
  eprint = {1704.04861},
  archiveprefix = {arXiv},
  citation-number = {35},
  langid = {english},
  note = {arXiv preprint arXiv:1704.04861.}
}

@misc{howard2017a,
  title = {{{MobileNets}}: {{Efficient}} Convolutional Neural Networks for Mobile Vision Applications},
  author = {Howard, A.G.},
  year = 2017,
  eprint = {1704.04861},
  archiveprefix = {arXiv},
  citation-number = {35},
  langid = {english},
  note = {arXiv preprint arXiv:1704.04861.}
}

@misc{howard2017a,
  title = {{{MobileNets}}: {{Efficient}} Convolutional Neural Networks for Mobile Vision Applications},
  author = {Howard, A.G.},
  year = 2017,
  eprint = {1704.04861},
  archiveprefix = {arXiv},
  citation-number = {35},
  langid = {english},
  note = {arXiv preprint arXiv:1704.04861.}
}

@inproceedings{huang2017a,
  title = {Underwater Image Enhancement Using a Joint Domain Transform},
  booktitle = {2017 {{IEEE}} International Conference on Computer Vision},
  author = {Huang, D.A.},
  year = 2017,
  pages = {3781--3790},
  publisher = {IEEE},
  citation-number = {11},
  langid = {english}
}

@inproceedings{huang2017a,
  title = {Underwater Image Enhancement Using a Joint Domain Transform},
  booktitle = {2017 {{IEEE}} International Conference on Computer Vision},
  author = {Huang, D.A.},
  year = 2017,
  pages = {3781--3790},
  publisher = {IEEE},
  citation-number = {11},
  langid = {english}
}

@inproceedings{huang2017a,
  title = {Underwater Image Enhancement Using a Joint Domain Transform},
  booktitle = {2017 {{IEEE}} International Conference on Computer Vision},
  author = {Huang, D.A.},
  year = 2017,
  pages = {3781--3790},
  publisher = {IEEE},
  citation-number = {11},
  langid = {english}
}

@inproceedings{huangDenselyConnectedConvolutional2017,
  title = {Densely {{Connected Convolutional Networks}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
  year = 2017,
  month = jul,
  pages = {2261--2269},
  publisher = {IEEE},
  address = {Honolulu, HI},
  doi = {10.1109/CVPR.2017.243},
  urldate = {2024-04-25},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/Users/dan/Documents/FAU/Thesis/Literature Review/Papers/Densely_Connected_Convolutional_Networks.pdf}
}

@misc{huangDenselyConnectedConvolutional2017a,
  title = {Densely {{Connected Convolutional Networks}}},
  author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
  year = 2017,
  month = jul,
  pages = {2261--2269}
}

@misc{huangDenselyConnectedConvolutional2017b,
  title = {Densely {{Connected Convolutional Networks}}},
  author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
  year = 2017,
  month = jul,
  doi = {10.1109/cvpr.2017.243}
}

@misc{huangSpeedAccuracyTradeoffs2017,
  title = {Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors},
  author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin},
  year = 2017,
  month = apr,
  number = {arXiv:1611.10012},
  eprint = {1611.10012},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-22},
  abstract = {The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-toapples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [31], R-FCN [6] and SSD [26] systems, which we view as ``meta-architectures'' and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Accepted to CVPR 2017},
  file = {/Users/dan/Zotero/storage/53PM3QIG/Huang et al. - 2017 - Speedaccuracy trade-offs for modern convolutional.pdf}
}

@misc{hussainAutomaticFishSpecies2019,
  title = {Automatic {{Fish Species Classification Using Deep Convolutional Neural Networks}}},
  author = {Hussain, Muhammad Ather Iqbal and Wang, Zhijie and Ali, Zain Anwar and Riaz, Shazia},
  year = 2019,
  month = aug,
  volume = {116},
  number = {2},
  pages = {1043--1053}
}

@misc{hussainAutomaticFishSpecies2019a,
  title = {Automatic {{Fish Species Classification Using Deep Convolutional Neural Networks}}},
  author = {Hussain, Muhammad Ather Iqbal and Wang, Zhijie and Ali, Zain Anwar and Riaz, Shazia},
  year = 2019,
  month = aug,
  journal = {Springer Science+Business Media},
  volume = {116},
  number = {2},
  pages = {1043--1053},
  doi = {10.1007/s11277-019-06634-1}
}

@techreport{ipbesGlobalAssessmentReport2019,
  title = {Global Assessment Report on Biodiversity and Ecosystem Services of the {{Intergovernmental Science-Policy Platform}} on {{Biodiversity}} and {{Ecosystem Services}}},
  author = {IPBES},
  year = 2019,
  month = may,
  institution = {Zenodo},
  doi = {10.5281/ZENODO.3831673},
  urldate = {2024-10-20},
  abstract = {IPBES is to perform regular and timely assessments of knowledge on biodiversity and ecosystem services and their interlinkages at the global level. Also addressing an invitation by the Conference of the Parties of the Convention on Biological Diversity (CBD) to prepare a global assessment of biodiversity and ecosystem services building, inter alia, on its own and other relevant regional, subregional and thematic assessments, as well as on national reports. The overall scope of the assessment is to assess the status and trends with regard to biodiversity and ecosystem services, the impact of biodiversity and ecosystem services on human well-being and the effectiveness of responses, including the Strategic Plan and its Aichi Biodiversity Targets. It is anticipated that this deliverable will contribute to the process for the evaluation and renewal of the Strategic Plan for Biodiversity and its Aichi Biodiversity Targets. The IPBES Global Assessment on Biodiversity and Ecosystem Services is composed of 1) a Summary for Policymakers (SPM), approved by the IPBES Plenary at its 7th session in May 2019 in Paris, France (IPBES-7); and 2) a set of six Chapters, accepted by the IPBES Plenary.},
  collaborator = {Brondizio, Eduardo and Diaz, Sandra and Settele, Josef and Ngo, Hien T.},
  copyright = {Creative Commons Attribution 4.0 International},
  langid = {english},
  keywords = {Full assessment report,Global Assessment,IPBES},
  file = {/Users/dan/Zotero/storage/ARPWWJ6U/IPBES - 2019 - Global assessment report on biodiversity and ecosy.pdf}
}

@article{iqbalAutomaticFishSpecies2021,
  title = {Automatic {{Fish Species Classification Using Deep Convolutional Neural Networks}}},
  author = {Iqbal, Muhammad Ather and Wang, Zhijie and Ali, Zain Anwar and Riaz, Shazia},
  year = 2021,
  month = jan,
  journal = {Wireless Pers Commun},
  volume = {116},
  number = {2},
  pages = {1043--1053},
  issn = {0929-6212, 1572-834X},
  doi = {10.1007/s11277-019-06634-1},
  urldate = {2024-04-23},
  abstract = {In this paper, we presented an automated system for identification and classification of fish species. It helps the marine biologists to have greater understanding of the fish species and their habitats. The proposed model is based on deep convolutional neural networks. It uses a reduced version of AlexNet model comprises of four convolutional layers and two fully connected layers. A comparison is presented against the other deep learning models such as AlexNet and VGGNet. The four parameters are considered that is number of convolutional layers and number of fully-connected layers, number of iterations to achieve 100\% accuracy on training data, batch size and dropout layer. The results show that the proposed and modified AlexNet model with less number of layers has achieved the testing accuracy of 90.48\% while the original AlexNet model achieved 86.65\% over the untrained benchmark fish dataset. The inclusion of dropout layer has enhanced the overall performance of our proposed model. It contain less training images, less memory and it is also less computational complex.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/6HI49PHM/Iqbal et al. - 2021 - Automatic Fish Species Classification Using Deep C.pdf}
}

@article{isaOptimizingHyperparameterTuning2022,
  title = {Optimizing the {{Hyperparameter Tuning}} of {{YOLOv5}} for {{Underwater Detection}}},
  author = {Isa, Iza Sazanita and Rosli, Mohamed Syazwan Asyraf and Yusof, Umi Kalsom and Maruzuki, Mohd Ikmal Fitri and Sulaiman, Siti Noraini},
  year = 2022,
  journal = {IEEE Access},
  volume = {10},
  pages = {52818--52831},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3174583},
  urldate = {2024-10-22},
  abstract = {This study optimized the latest YOLOv5 framework, including its subset models, with training on different datasets that differed in image contrast and cloudiness to assess model performances based on quantitative metrics and image processing speed. The hyperparameter in the feature-extraction phase was configured based on the learning rate and momentum and further improved based on the adaptive moment estimation (ADAM) optimizer and the function reducing-learning-rate-on-plateau to optimize the model's training scheme. The optimized YOLOv5s achieved a better performance, with a mean average precision of 98.6\% and a high inference speed of 106 frames per second. The ADAM optimizer with a detailed learning rate (0.0001) and momentum (0.99) fine-tuning yielded a sufficient convergence rate (0.69\% at 55th epoch) to assist YOLOv5s in attaining a more precise detection for underwater objects.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {/Users/dan/Zotero/storage/CQNLEYU9/Isa et al. - 2022 - Optimizing the Hyperparameter Tuning of YOLOv5 for.pdf}
}

@inproceedings{islam2020a,
  title = {Faster {{R-CNN}} Based Deep Learning for Object Detection and Classification in Underwater Environment},
  booktitle = {2020 {{IEEE}}/{{OES}} Autonomous Underwater Vehicles Symposium},
  author = {Islam, M.J.},
  year = 2020,
  pages = {1--5},
  publisher = {IEEE},
  citation-number = {46},
  langid = {english}
}

@inproceedings{islam2020a,
  title = {Faster {{R-CNN}} Based Deep Learning for Object Detection and Classification in Underwater Environment},
  booktitle = {2020 {{IEEE}}/{{OES}} Autonomous Underwater Vehicles Symposium},
  author = {Islam, M.J.},
  year = 2020,
  pages = {1--5},
  publisher = {IEEE},
  citation-number = {46},
  langid = {english}
}

@inproceedings{islam2020a,
  title = {Faster {{R-CNN}} Based Deep Learning for Object Detection and Classification in Underwater Environment},
  booktitle = {2020 {{IEEE}}/{{OES}} Autonomous Underwater Vehicles Symposium},
  author = {Islam, M.J.},
  year = 2020,
  pages = {1--5},
  publisher = {IEEE},
  citation-number = {46},
  langid = {english}
}

@article{jackson2001a,
  title = {Historical Overfishing and the Recent Collapse of Coastal Ecosystems},
  author = {Jackson, J.B.C. and Kirby, M.X. and Berger, W.H. and Bjorndal, K.A. and Botsford, L.W. and Bourque, B.J. and Warner, R.R.},
  year = 2001,
  journal = {Science},
  volume = {293},
  number = {5530},
  pages = {629--637},
  citation-number = {5},
  langid = {english}
}

@article{jacksonHistoricalOverfishingRecent2001,
  title = {Historical {{Overfishing}} and the {{Recent Collapse}} of {{Coastal Ecosystems}}},
  author = {Jackson, Jeremy B. C. and Kirby, Michael X. and Berger, Wolfgang H. and Bjorndal, Karen A. and Botsford, Louis W. and Bourque, Bruce J. and Bradbury, Roger H. and Cooke, Richard and Erlandson, Jon and Estes, James A. and Hughes, Terence P. and Kidwell, Susan and Lange, Carina B. and Lenihan, Hunter S. and Pandolfi, John M. and Peterson, Charles H. and Steneck, Robert S. and Tegner, Mia J. and Warner, Robert R.},
  year = 2001,
  month = jul,
  journal = {Science},
  volume = {293},
  number = {5530},
  pages = {629--637},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1059199},
  urldate = {2024-10-20},
  abstract = {Ecological extinction caused by overfishing precedes all other pervasive human disturbance to coastal ecosystems, including pollution, degradation of water quality, and anthropogenic climate change. Historical abundances of large consumer species were fantastically large in comparison with recent observations. Paleoecological, archaeological, and historical data show that time lags of decades to centuries occurred between the onset of overfishing and consequent changes in ecological communities, because unfished species of similar trophic level assumed the ecological roles of overfished species until they too were overfished or died of epidemic diseases related to overcrowding. Retrospective data not only help to clarify underlying causes and rates of ecological change, but they also demonstrate achievable goals for restoration and management of coastal ecosystems that could not even be contemplated based on the limited perspective of recent observations alone.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/MS3HGREB/Jackson et al. - 2001 - Historical Overfishing and the Recent Collapse of .pdf}
}

@article{jaffe1990a,
  title = {Computer Modeling and the Design of Optimal Underwater Imaging Systems},
  author = {Jaffe, J.S.},
  year = 1990,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {15},
  number = {2},
  pages = {101--111},
  citation-number = {17},
  langid = {english}
}

@misc{janiModelCompressionMethods2023,
  title = {Model {{Compression Methods}} for {{YOLOv5}}: {{A Review}}},
  shorttitle = {Model {{Compression Methods}} for {{YOLOv5}}},
  author = {Jani, Mohammad and Fayyad, Jamil and {Al-Younes}, Younes and Najjaran, Homayoun},
  year = 2023,
  month = jul,
  number = {arXiv:2307.11904},
  eprint = {2307.11904},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-21},
  abstract = {Over the past few years, extensive research has been devoted to enhancing YOLO object detectors. Since its introduction, eight major versions of YOLO have been introduced with the purpose of improving its accuracy and efficiency. While the evident merits of YOLO have yielded to its extensive use in many areas, deploying it on resource-limited devices poses challenges. To address this issue, various neural network compression methods have been developed, which fall under three main categories, namely network pruning, quantization, and knowledge distillation. The fruitful outcomes of utilizing model compression methods, such as lowering memory usage and inference time, make them favorable, if not necessary, for deploying large neural networks on hardware-constrained edge devices. In this review paper, our focus is on pruning and quantization due to their comparative modularity. We categorize them and analyze the practical results of applying those methods to YOLOv5. By doing so, we identify gaps in adapting pruning and quantization for compressing YOLOv5, and provide future directions in this area for further exploration. Among several versions of YOLO, we specifically choose YOLOv5 for its excellent trade-off between recency and popularity in literature. This is the first specific review paper that surveys pruning and quantization methods from an implementation point of view on YOLOv5. Our study is also extendable to newer versions of YOLO as implementing them on resource-limited devices poses the same challenges that persist even today. This paper targets those interested in the practical deployment of model compression methods on YOLOv5, and in exploring different compression techniques that can be used for subsequent versions of YOLO.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  note = {Comment: 18 pages, 7 Figures},
  file = {/Users/dan/Zotero/storage/ZZ6JA58Q/Jani et al. - 2023 - Model Compression Methods for YOLOv5 A Review.pdf;/Users/dan/Zotero/storage/69GARFXZ/2307.html}
}

@misc{janiModelCompressionMethods2023a,
  title = {Model {{Compression Methods}} for {{YOLOv5}}: {{A Review}}},
  shorttitle = {Model {{Compression Methods}} for {{YOLOv5}}},
  author = {Jani, Mohammad and Fayyad, Jamil and {Al-Younes}, Younes and Najjaran, Homayoun},
  year = 2023,
  month = jul,
  number = {arXiv:2307.11904},
  eprint = {2307.11904},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-02},
  abstract = {Over the past few years, extensive research has been devoted to enhancing YOLO object detectors. Since its introduction, eight major versions of YOLO have been introduced with the purpose of improving its accuracy and efficiency. While the evident merits of YOLO have yielded to its extensive use in many areas, deploying it on resourcelimited devices poses challenges. To address this issue, various neural network compression methods have been developed, which fall under three main categories, namely network pruning, quantization, and knowledge distillation. The fruitful outcomes of utilizing model compression methods, such as lowering memory usage and inference time, make them favorable, if not necessary, for deploying large neural networks on hardware-constrained edge devices. In this review paper, our focus is on pruning and quantization due to their comparative modularity. We categorize them and analyze the practical results of applying those methods to YOLOv5. By doing so, we identify gaps in adapting pruning and quantization for compressing YOLOv5, and provide future directions in this area for further exploration. Among several versions of YOLO, we specifically choose YOLOv5 for its excellent trade-off between recency and popularity in literature. This is the first specific review paper that surveys pruning and quantization methods from an implementation point of view on YOLOv5. Our study is also extendable to newer versions of YOLO as implementing them on resource-limited devices poses the same challenges that persist even today. This paper targets those interested in the practical deployment of model compression methods on YOLOv5, and in exploring different compression techniques that can be used for subsequent versions of YOLO.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  note = {Comment: 18 pages, 7 Figures},
  file = {/Users/dan/Zotero/storage/EWAHXVGV/Jani et al. - 2023 - Model Compression Methods for YOLOv5 A Review.pdf}
}

@misc{janiModelCompressionMethods2023b,
  title = {Model {{Compression Methods}} for {{YOLOv5}}: {{A Review}}},
  author = {J{\'a}ni, Martin and Fayyad, Jamil and Younes, Younes Al and Najjaran, Homayoun},
  year = 2023,
  month = jan
}

@misc{janiModelCompressionMethods2023c,
  title = {Model {{Compression Methods}} for {{YOLOv5}}: {{A Review}}},
  author = {J{\'a}ni, Martin and Fayyad, Jamil and Younes, Younes Al and Najjaran, Homayoun},
  year = 2023,
  month = jan,
  journal = {Cornell University},
  doi = {10.48550/arxiv.2307.11904}
}

@article{jeankossaifiTensorRegressionNetworks2017,
  title = {Tensor {{Regression Networks}}},
  author = {{Jean Kossaifi} and {Zachary Chase Lipton} and {A. Khanna} and {Tommaso Furlanello} and {Anima Anandkumar}},
  year = 2017,
  month = jul,
  journal = {ArXiv},
  volume = {abs/1707.08308},
  file = {/Users/dan/Zotero/storage/JBWLQHXG/Jean Kossaifi et al. - 2017 - Tensor Regression Networks.pdf}
}

@misc{jeongDeepLearningInference2021,
  title = {Deep {{Learning Inference Parallelization}} on {{Heterogeneous Processors With TensorRT}}},
  author = {Jeong, Eunjin and Kim, Jangryul and Tan, Samnieng and Lee, Jaeseong and Ha, Soonhoi},
  year = 2021,
  month = jun,
  journal = {Institute of Electrical and Electronics Engineers},
  volume = {14},
  number = {1},
  pages = {15--18},
  doi = {10.1109/les.2021.3087707}
}

@misc{jeongDeepLearningInference2021a,
  title = {Deep {{Learning Inference Parallelization}} on {{Heterogeneous Processors With TensorRT}}},
  author = {Jeong, Eunjin and Kim, Jangryul and Tan, Samnieng and Lee, Jaeseong and Ha, Soonhoi},
  year = 2021,
  month = jun,
  volume = {14},
  number = {1},
  pages = {15--18}
}

@misc{jeongTensorRTBasedFrameworkOptimization2022,
  title = {{{TensorRT-Based Framework}} and {{Optimization Methodology}} for {{Deep Learning Inference}} on {{Jetson Boards}}},
  author = {Jeong, Eunjin and Kim, Jangryul and Ha, Soonhoi},
  year = 2022,
  month = jan,
  journal = {Association for Computing Machinery},
  volume = {21},
  number = {5},
  pages = {1--26},
  doi = {10.1145/3508391}
}

@misc{jeongTensorRTBasedFrameworkOptimization2022a,
  title = {{{TensorRT-Based Framework}} and {{Optimization Methodology}} for {{Deep Learning Inference}} on {{Jetson Boards}}},
  author = {Jeong, Eunjin and Kim, Jangryul and Ha, Soonhoi},
  year = 2022,
  month = jan,
  volume = {21},
  number = {5},
  pages = {1--26}
}

@inproceedings{jesusUnderwaterObjectClassification2022,
  title = {Underwater {{Object Classification}} and {{Detection}}: First Results and Open Challenges},
  shorttitle = {Underwater {{Object Classification}} and {{Detection}}},
  booktitle = {{{OCEANS}} 2022 - {{Chennai}}},
  author = {Jesus, Andre and Zito, Claudio and Tortorici, Claudio and Roura, Eloy and De Masi, Giulia},
  year = 2022,
  month = feb,
  eprint = {2201.00977},
  primaryclass = {cs},
  pages = {1--6},
  doi = {10.1109/OCEANSChennai45887.2022.9775417},
  urldate = {2024-10-07},
  abstract = {This work reviews the problem of object detection in underwater environments. We analyse and quantify the shortcomings of conventional state-of-the-art (SOTA) algorithms in the computer vision community when applied to this challenging environment, as well as providing insights and general guidelines for future research efforts. First, we assessed if pretraining with the conventional ImageNet is beneficial when the object detector needs to be applied to environments that may be characterised by a different feature distribution. We then investigate whether two-stage detectors yields to better performance with respect to single-stage detectors, in terms of accuracy, intersection of union (IoU), floating operation per second (FLOPS), and inference time. Finally, we assessed the generalisation capability of each model to a lower quality dataset to simulate performance on a real scenario, in which harsher conditions ought to be expected. Our experimental results provide evidence that underwater object detection requires searching for ``ad-hoc'' architectures than merely training SOTA architectures on new data, and that pretraining is not beneficial.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/dan/Zotero/storage/XNKL9WXI/Jesus et al. - 2022 - Underwater Object Classification and Detection fi.pdf}
}

@article{jiaSolublePDL1Predictor2020,
  title = {Soluble {{PD-L1}} as a {{Predictor}} of the {{Response}} to {{EGFR-TKIs}} in {{Non-small Cell Lung Cancer Patients With EGFR Mutations}}},
  author = {Jia, Yijun and Li, Xuefei and Zhao, Chao and Ren, Shengxiang and Su, Chunxia and Gao, Guanghui and Li, Wei and Zhou, Fei and Li, Jiayu and Zhou, Caicun},
  year = 2020,
  month = aug,
  journal = {Front. Oncol.},
  volume = {10},
  publisher = {Frontiers},
  issn = {2234-943X},
  doi = {10.3389/fonc.2020.01455},
  urldate = {2024-04-20},
  abstract = {{$<$}p{$>$}Programmed cell death ligand 1 (PD-L1) expressed on tumor tissues is a vital molecule for immune suppression and its impact on the response to epidermal growth factor receptor tyrosine kinase inhibitors (EGFR-TKIs) has been reported. The significance of soluble PD-L1 (sPD-L1) for lung cancer patients remains unknown. This study investigated whether sPD-L1 could predict the response of {$<$}italic{$>$}EGFR{$<$}/italic{$>$}-mutated non-small cell lung cancer (NSCLC) to EGFR-targeted therapy. We retrospectively evaluated patients who received first-line treatment with EGFR-TKIs for advanced NSCLC with {$<$}italic{$>$}EGFR{$<$}/italic{$>$} mutations. Pre-treatment plasma concentrations of PD-L1 and on-treatment (1 month after treatment initiation) plasma concentrations of PD-L1 were measured using the R-plex Human PD-L1 assay. The association between the sPD-L1 level and the clinical outcome was analyzed. Among 66 patients who were eligible for the study, patients with high pre-treatment or on-treatment sPD-L1 levels had decreased objective response rate (ORR) compared with that of patients with low sPD-L1 levels (39.4 vs. 66.7\%, {$<$}italic{$>$}p{$<$}/italic{$>$} = 0.026 for pre-treatment sPD-L1 level, and 43.5 vs. 73.9\%, {$<$}italic{$>$}p{$<$}/italic{$>$} = 0.025 for on-treatment sPD-L1 level). A high baseline sPD-L1 level was associated with a shortened progression-free survival (PFS) rate (9.9 vs. 16.1 months, {$<$}italic{$>$}p{$<$}/italic{$>$} = 0.005). Both univariate and multivariate analyses showed that a high baseline sPD-L1 level was an independent factor associated with the PFS (hazard ratio [HR] 2.56, {$<$}italic{$>$}p{$<$}/italic{$>$} = 0.011). Our study revealed that the sPD-L1 level was strongly related to the outcome of EGFR-TKIs in NSCLC patients harboring {$<$}italic{$>$}EGFR{$<$}/italic{$>$} mutations.{$<$}/p{$>$}},
  langid = {english},
  keywords = {efficacy,EGFR-TKIs,Non-small cell lung cancer,prediction,soluble PD-L1},
  file = {/Users/dan/Zotero/storage/FW5BAGSJ/Jia et al. - 2020 - Soluble PD-L1 as a Predictor of the Response to EG.pdf}
}

@book{jocher2020a,
  title = {{YOLOv5}},
  author = {Jocher, G.},
  year = 2020,
  publisher = {GitHub Repository},
  citation-number = {38},
  langid = {hmn}
}

@book{jocher2020a,
  title = {{YOLOv5}},
  author = {Jocher, G.},
  year = 2020,
  publisher = {GitHub Repository},
  citation-number = {30},
  langid = {hmn}
}

@book{jocher2020a,
  title = {{YOLOv5}},
  author = {Jocher, G.},
  year = 2020,
  publisher = {GitHub Repository},
  citation-number = {30},
  langid = {hmn}
}

@book{jocher2020a,
  title = {{YOLOv5}},
  author = {Jocher, G.},
  year = 2020,
  publisher = {GitHub Repository},
  citation-number = {30},
  langid = {hmn}
}

@misc{jocherUltralytics,
  type = {{{GitHub Repository}}},
  title = {Ultralytics},
  author = {Jocher, Glen}
}

@misc{jocherYOLOv5,
  type = {{{GitHub Repository}}},
  title = {{{YOLOv5}}},
  author = {Jocher, Glen}
}

@misc{jocherYOLOv5a,
  title = {{{YOLOv5}}},
  author = {Jocher, Glen}
}

@inproceedings{joly2015a,
  title = {{{LifeCLEF}} 2015: {{Multimedia}} Life Species Identification Challenges},
  booktitle = {International Conference of the Cross-Language Evaluation Forum for European Languages},
  author = {Joly, A.},
  year = 2015,
  pages = {462--488},
  publisher = {Springer},
  citation-number = {2},
  langid = {english}
}

@inproceedings{joly2015a,
  title = {{{LifeCLEF}} 2015: {{Multimedia}} Life Species Identification Challenges},
  booktitle = {International Conference of the Cross-Language Evaluation Forum for European Languages},
  author = {Joly, A.},
  year = 2015,
  pages = {462--488},
  publisher = {Springer},
  citation-number = {2},
  langid = {english}
}

@inproceedings{joly2015a,
  title = {{{LifeCLEF}} 2015: {{Multimedia}} Life Species Identification Challenges},
  booktitle = {International Conference of the Cross-Language Evaluation Forum for European Languages},
  author = {Joly, A.},
  year = 2015,
  pages = {462--488},
  publisher = {Springer},
  citation-number = {2},
  langid = {english}
}

@inproceedings{jouppi2017a,
  title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
  author = {Jouppi, N.P.},
  year = 2017,
  pages = {1--12},
  citation-number = {52},
  langid = {english}
}

@inproceedings{jouppi2017a,
  title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
  author = {Jouppi, N.P.},
  year = 2017,
  pages = {1--12},
  citation-number = {52},
  langid = {english}
}

@inproceedings{jouppi2017a,
  title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
  author = {Jouppi, N.P.},
  year = 2017,
  pages = {1--12},
  citation-number = {52},
  langid = {english}
}

@article{jussihanhirovaLatencyThroughputCharacterization2018,
  title = {Latency and Throughput Characterization of Convolutional Neural Networks for Mobile Computer Vision},
  author = {{Jussi Hanhirova} and {Teemu K\"am\"ar\"ainen} and {S. Sepp\"al\"a} and {M. Siekkinen} and {V. Hirvisalo} and {Antti Yl\"a-J\"a\"aski}},
  year = 2018,
  month = mar,
  journal = {Proceedings of the 9th ACM Multimedia Systems Conference},
  doi = {10.1145/3204949.3204975},
  file = {/Users/dan/Zotero/storage/7QGCDELH/Jussi Hanhirova et al. - 2018 - Latency and throughput characterization of convolu.pdf}
}

@article{kang2017a,
  title = {T-{{CNN}}: {{Tubelets}} with Convolutional Neural Networks for Object Detection from Videos},
  author = {Kang, K.},
  year = 2017,
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {28},
  number = {10},
  pages = {2896--2907},
  citation-number = {42},
  langid = {english}
}

@article{kang2017a,
  title = {T-{{CNN}}: {{Tubelets}} with Convolutional Neural Networks for Object Detection from Videos},
  author = {Kang, K.},
  year = 2017,
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {28},
  number = {10},
  pages = {2896--2907},
  citation-number = {42},
  langid = {english}
}

@article{kang2017a,
  title = {T-{{CNN}}: {{Tubelets}} with Convolutional Neural Networks for Object Detection from Videos},
  author = {Kang, K.},
  year = 2017,
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {28},
  number = {10},
  pages = {2896--2907},
  citation-number = {42},
  langid = {english}
}

@inproceedings{karasmanoglouHeatmapbasedExplanationYOLOv52022,
  title = {Heatmap-Based {{Explanation}} of {{YOLOv5 Object Detection}} with {{Layer-wise Relevance Propagation}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Imaging Systems}} and {{Techniques}} ({{IST}})},
  author = {Karasmanoglou, Apostolos and Antonakakis, Marios and Zervakis, Michalis},
  year = 2022,
  month = jun,
  pages = {1--6},
  publisher = {IEEE},
  address = {Kaohsiung, Taiwan},
  doi = {10.1109/IST55454.2022.9827744},
  urldate = {2024-10-15},
  abstract = {Deep Neural Networks (DNNs) have been effective in providing solutions to most prevailing problems in the field Computer Vision. Modern Object Detector architecture families such as Single Shot Detector, You Only Look Once (YOLO), Region Based Convolutional Neural Networks (RCNN) and others are powerful models with strong capabilities of distinguishing many common object classes in images with unprecedented speed and accuracy. However, in most of their applications the use of deep networks conceals the inference of object class and location from the input data by introducing many layers of complex black-box functionality. This leads to sometimes bizarre and inexplicable detection outputs. A potential use of these networks in riskprone applications motivates the development of techniques for visually interpreting their outputs, which could lead to more controlled training and deployment. Layer-wise Relevance Propagation (LRP) is a popular ''eXplainable AI'' (XAI) technique for constructing such explanations, frequently used in investigating the output of DNNs. For object detection, LRP could provide us with a useful heatmap of the input's relevance to the output class, localized within an estimate of a bounding box. In this work, we propose a new and efficient method for conducting relevance propagation in object detector DNNs that leads to visually informative results. In our experiments, we make use of the YOLOv5 model showing meaningful visual explanations of its output. Our implementation philosophy exemplifies how relevance propagation can be regulated for complex modular architectures.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-8102-1},
  langid = {english},
  file = {/Users/dan/Zotero/storage/TDMMV5RQ/Karasmanoglou et al. - 2022 - Heatmap-based Explanation of YOLOv5 Object Detecti.pdf}
}

@article{katijaFathomNetGlobalImage2022,
  title = {{{FathomNet}}: {{A}} Global Image Database for Enabling Artificial Intelligence in the Ocean},
  shorttitle = {{{FathomNet}}},
  author = {Katija, Kakani and Orenstein, Eric and Schlining, Brian and Lundsten, Lonny and Barnard, Kevin and Sainz, Giovanna and Boulais, Oceane and Cromwell, Megan and Butler, Erin and Woodward, Benjamin and Bell, Katherine L. C.},
  year = 2022,
  month = sep,
  journal = {Sci Rep},
  volume = {12},
  number = {1},
  pages = {15914},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-19939-2},
  urldate = {2024-04-22},
  abstract = {The ocean is experiencing unprecedented rapid change, and visually monitoring marine biota at the spatiotemporal scales needed for responsible stewardship is a formidable task. As baselines are sought by the research community, the volume and rate of this required data collection rapidly outpaces our abilities to process and analyze them. Recent advances in machine learning enables fast, sophisticated analysis of visual data, but have had limited success in the ocean due to lack of data standardization, insufficient formatting, and demand for large, labeled datasets. To address this need, we built FathomNet, an open-source image database that standardizes and aggregates expertly curated labeled data. FathomNet has been seeded with existing iconic and non-iconic imagery of marine animals, underwater equipment, debris, and other concepts, and allows for future contributions from distributed data sources. We demonstrate how FathomNet data can be used to train and deploy models on other institutional video to reduce annotation effort, and enable automated tracking of underwater concepts when integrated with robotic vehicles. As FathomNet continues to grow and incorporate more labeled data from the community, we can accelerate the processing of visual data to achieve a healthy and sustainable global ocean.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Biological techniques,Ecology,Imaging,Ocean sciences},
  file = {/Users/dan/Zotero/storage/NK2M7BB3/Katija et al. - 2022 - FathomNet A global image database for enabling ar.pdf}
}

@misc{katijaFathomNetGlobalImage2022a,
  title = {{{FathomNet}}: {{A}} Global Image Database for Enabling Artificial Intelligence in the Ocean},
  author = {Katija, Kakani and Orenstein, Eric C. and Schlining, Brian and Lundsten, Lonny and Barnard, Kevin and Sainz, Giovanna and Boulais, Oc{\'e}ane and Cromwell, Megan and Butler, Erin and Woodward, Benjamin and Bell, Katherine L. C.},
  year = 2022,
  month = sep,
  volume = {12},
  number = {1},
  pages = {15914}
}

@misc{katijaFathomNetGlobalImage2022b,
  title = {{{FathomNet}}: {{A}} Global Image Database for Enabling Artificial Intelligence in the Ocean},
  author = {Katija, Kakani and Orenstein, Eric C. and Schlining, Brian and Lundsten, Lonny and Barnard, Kevin and Sainz, Giovanna and Boulais, Oc{\'e}ane and Cromwell, Megan and Butler, Erin and Woodward, Benjamin and Bell, Katherine L. C.},
  year = 2022,
  month = sep,
  journal = {Nature Portfolio},
  volume = {12},
  number = {1},
  doi = {10.1038/s41598-022-19939-2}
}

@misc{khaiUnderwaterFishDetection2022,
  title = {Underwater {{Fish Detection}} and {{Counting Using Mask Regional Convolutional Neural Network}}},
  author = {Khai, Teh Hong and Abdullah, Siti Norul Huda Sheikh and Hasan, Mohammad Kamrul and Tarmizi, Ahmad},
  year = 2022,
  month = jan,
  volume = {14},
  number = {2},
  pages = {222}
}

@misc{khaiUnderwaterFishDetection2022a,
  title = {Underwater {{Fish Detection}} and {{Counting Using Mask Regional Convolutional Neural Network}}},
  author = {Khai, Teh Hong and Abdullah, Siti Norul Huda Sheikh and Hasan, Mohammad Kamrul and Tarmizi, Ahmad},
  year = 2022,
  month = jan,
  journal = {Multidisciplinary Digital Publishing Institute},
  volume = {14},
  number = {2},
  pages = {222--222},
  doi = {10.3390/w14020222}
}

@article{koldaTensorDecompositionsApplications2009,
  title = {Tensor {{Decompositions}} and {{Applications}}},
  author = {Kolda, Tamara G. and Bader, Brett W.},
  year = 2009,
  month = aug,
  journal = {SIAM Rev.},
  volume = {51},
  number = {3},
  pages = {455--500},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/07070111X},
  urldate = {2024-06-26},
  langid = {english},
  file = {/Users/dan/Zotero/storage/E6BAM4Y8/Kolda and Bader - 2009 - Tensor Decompositions and Applications.pdf}
}

@misc{koldaTensorDecompositionsApplications2009a,
  title = {Tensor {{Decompositions}} and {{Applications}}},
  author = {Kolda, Tamara G. and Bader, Brett W.},
  year = 2009,
  month = aug,
  volume = {51},
  number = {3},
  pages = {455--500}
}

@misc{koldaTensorDecompositionsApplications2009b,
  title = {Tensor {{Decompositions}} and {{Applications}}},
  author = {Kolda, Tamara G. and Bader, Brett W.},
  year = 2009,
  month = aug,
  journal = {Society for Industrial and Applied Mathematics},
  volume = {51},
  number = {3},
  pages = {455--500},
  doi = {10.1137/07070111x}
}

@misc{kossaifiTensorRegressionNetworks2017,
  title = {Tensor {{Regression Networks}}},
  author = {Kossaifi, Jean and Lipton, Zachary C. and Kolbeinsson, Arinbj{\"o}rn and Khanna, Aran and Furlanello, Tommaso and Anandkumar, Anima},
  year = 2017,
  month = jan,
  journal = {Cornell University},
  doi = {10.48550/arxiv.1707.08308}
}

@misc{kossaifiTensorRegressionNetworks2017a,
  title = {Tensor {{Regression Networks}}},
  author = {Kossaifi, Jean and Lipton, Zachary C. and Kolbeinsson, Arinbj{\"o}rn and Khanna, Aran and Furlanello, Tommaso and Anandkumar, Anima},
  year = 2017,
  month = jan,
  volume = {abs/1707.08308}
}

@inproceedings{lane2017a,
  title = {{{DeepX}}: {{A}} Software Accelerator for Low-Power Deep Learning Inference on Mobile Devices},
  booktitle = {15th International Conference on Information Processing in Sensor Networks},
  author = {Lane, N.D.},
  year = 2017,
  pages = {1--12},
  publisher = {IEEE},
  citation-number = {47},
  langid = {english}
}

@inproceedings{lane2017a,
  title = {{{DeepX}}: {{A}} Software Accelerator for Low-Power Deep Learning Inference on Mobile Devices},
  booktitle = {15th International Conference on Information Processing in Sensor Networks},
  author = {Lane, N.D.},
  year = 2017,
  pages = {1--12},
  publisher = {IEEE},
  citation-number = {47},
  langid = {english}
}

@inproceedings{lane2017a,
  title = {{{DeepX}}: {{A}} Software Accelerator for Low-Power Deep Learning Inference on Mobile Devices},
  booktitle = {15th International Conference on Information Processing in Sensor Networks},
  author = {Lane, N.D.},
  year = 2017,
  pages = {1--12},
  publisher = {IEEE},
  citation-number = {47},
  langid = {english}
}

@article{langenkaemper2017a,
  title = {{{BIIGLE}} 2.0---{{Browsing}} and Annotating Large Marine Image Collections},
  author = {Langenk{\"a}mper, D. and Zurowietz, M. and Schoening, T. and Nattkemper, T.W.},
  year = 2017,
  journal = {Frontiers in Marine Science},
  volume = {4},
  pages = {83},
  citation-number = {13},
  langid = {english},
  file = {/Users/dan/Zotero/storage/69V7P73H/Langenkmper et al. - 2017 - BIIGLE 2.0Browsing and annotating large marine im.pdf}
}

@misc{lazarevichYOLOBenchBenchmarkingEfficient2023,
  title = {{{YOLOBench}}: {{Benchmarking Efficient Object Detectors}} on {{Embedded Systems}}},
  shorttitle = {{{YOLOBench}}},
  author = {Lazarevich, Ivan and Grimaldi, Matteo and Kumar, Ravish and Mitra, Saptarshi and Khan, Shahrukh and Sah, Sudhakar},
  year = 2023,
  month = aug,
  number = {arXiv:2307.13901},
  eprint = {2307.13901},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-26},
  abstract = {We present YOLOBench, a benchmark comprised of 550+ YOLO-based object detection models on 4 different datasets and 4 different embedded hardware platforms (x86 CPU, ARM CPU, Nvidia GPU, NPU). We collect accuracy and latency numbers for a variety of YOLObased one-stage detectors at different model scales by performing a fair, controlled comparison of these detectors with a fixed training environment (code and training hyperparameters). Pareto-optimality analysis of the collected data reveals that, if modern detection heads and training techniques are incorporated into the learning process, multiple architectures of the YOLO series achieve a good accuracy-latency trade-off, including older models like YOLOv3 and YOLOv4. We also evaluate training-free accuracy estimators used in neural architecture search on YOLOBench and demonstrate that, while most state-of-the-art zero-cost accuracy estimators are outperformed by a simple baseline like MAC count, some of them can be effectively used to predict Pareto-optimal detection models. We showcase that by using a zero-cost proxy to identify a YOLO architecture competitive against a state-of-the-art YOLOv8 model on a Raspberry Pi 4 CPU. The code and data are available at https: // github. com/ Deeplite/ deeplitetorch-zoo .},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dan/Zotero/storage/JQE9CS4K/Lazarevich et al. - 2023 - YOLOBench Benchmarking Efficient Object Detectors.pdf}
}

@misc{lazarevichYOLOBenchBenchmarkingEfficient2023a,
  title = {{{YOLOBench}}: {{Benchmarking Efficient Object Detectors}} on {{Embedded Systems}}},
  author = {Lazarevich, Ivan and Grimaldi, Matteo and Kumar, Ravish and Mitra, Saptarshi and Khan, Shahrukh and Sah, Sudhakar},
  year = 2023,
  month = oct,
  doi = {10.1109/iccvw60793.2023.00126}
}

@article{li2016a,
  title = {Underwater Image Enhancement by Dehazing with Minimum Information Loss and Histogram Distribution Prior},
  author = {Li, C. and Guo, J. and Cong, R. and Pang, Y. and Wang, B.},
  year = 2016,
  journal = {IEEE Transactions on Image Processing},
  volume = {25},
  number = {12},
  pages = {5664--5677},
  citation-number = {23},
  langid = {english}
}

@article{li2017a,
  title = {{{WaterGAN}}: {{Unsupervised}} Generative Network to Enable Real-Time Color Correction of Monocular Underwater Images},
  author = {Li, C.},
  year = 2017,
  journal = {IEEE Robotics and Automation Letters},
  volume = {3},
  number = {1},
  pages = {387--394},
  citation-number = {20},
  langid = {english}
}

@article{li2017a,
  title = {{{WaterGAN}}: {{Unsupervised}} Generative Network to Enable Real-Time Color Correction of Monocular Underwater Images},
  author = {Li, C.},
  year = 2017,
  journal = {IEEE Robotics and Automation Letters},
  volume = {3},
  number = {1},
  pages = {387--394},
  citation-number = {20},
  langid = {english}
}

@article{li2017a,
  title = {{{WaterGAN}}: {{Unsupervised}} Generative Network to Enable Real-Time Color Correction of Monocular Underwater Images},
  author = {Li, C.},
  year = 2017,
  journal = {IEEE Robotics and Automation Letters},
  volume = {3},
  number = {1},
  pages = {387--394},
  citation-number = {20},
  langid = {english}
}

@article{li2019a,
  title = {Emerging from Water: {{Underwater}} Image Color Correction Based on Weakly Supervised Color Transfer},
  author = {Li, C. and Guo, J.},
  year = 2019,
  journal = {IEEE Signal Processing Letters},
  volume = {26},
  number = {11},
  pages = {1683--1687},
  citation-number = {21},
  langid = {english}
}

@article{li2019a,
  title = {Emerging from Water: {{Underwater}} Image Color Correction Based on Weakly Supervised Color Transfer},
  author = {Li, C. and Guo, J.},
  year = 2019,
  journal = {IEEE Signal Processing Letters},
  volume = {26},
  number = {11},
  pages = {1683--1687},
  citation-number = {21},
  langid = {english}
}

@article{li2019a,
  title = {Emerging from Water: {{Underwater}} Image Color Correction Based on Weakly Supervised Color Transfer},
  author = {Li, C. and Guo, J.},
  year = 2019,
  journal = {IEEE Signal Processing Letters},
  volume = {26},
  number = {11},
  pages = {1683--1687},
  citation-number = {21},
  langid = {english}
}

@article{li2019b,
  title = {Underwater Fish Detection with Weak Multi-Domain Data Augmentation},
  author = {Li, Y.},
  year = 2019,
  journal = {Applied Sciences},
  volume = {9},
  number = {10},
  citation-number = {32},
  langid = {english}
}

@article{li2019b,
  title = {Underwater Fish Detection with Weak Multi-Domain Data Augmentation},
  author = {Li, Y.},
  year = 2019,
  journal = {Applied Sciences},
  volume = {9},
  number = {10},
  citation-number = {32},
  langid = {english}
}

@article{li2019b,
  title = {Underwater Fish Detection with Weak Multi-Domain Data Augmentation},
  author = {Li, Y.},
  year = 2019,
  journal = {Applied Sciences},
  volume = {9},
  number = {10},
  citation-number = {32},
  langid = {english}
}

@misc{liaoPerfNetRTPlatformAwarePerformance2020,
  title = {{{PerfNetRT}}: {{Platform-Aware Performance Modeling}} for {{Optimized Deep Neural Networks}}},
  author = {Liao, Ying-Chiao and Wang, Chuan-Chi and Tu, Chia-Heng and Kao, Ming-Chang and Liang, Wen-Yew and Hung, Shih-Hao},
  year = 2020,
  month = dec,
  doi = {10.1109/ics51289.2020.00039}
}

@misc{liaoPerfNetRTPlatformAwarePerformance2020a,
  title = {{{PerfNetRT}}: {{Platform-Aware Performance Modeling}} for {{Optimized Deep Neural Networks}}},
  author = {Liao, Ying-Chiao and Wang, Chuan-Chi and Tu, Chia-Heng and Kao, Ming-Chang and Liang, Wen-Yew and Hung, Shih-Hao},
  year = 2020,
  month = dec,
  pages = {153--158}
}

@article{liCMEYOLOv5EfficientObject2022,
  title = {{{CME-YOLOv5}}: {{An Efficient Object Detection Network}} for {{Densely Spaced Fish}} and {{Small Targets}}},
  shorttitle = {{{CME-YOLOv5}}},
  author = {Li, Jianyuan and Liu, Chunna and Lu, Xiaochun and Wu, Bilang},
  year = 2022,
  month = aug,
  journal = {Water},
  volume = {14},
  number = {15},
  pages = {2412},
  issn = {2073-4441},
  doi = {10.3390/w14152412},
  urldate = {2024-10-22},
  abstract = {Fish are indicative species with a relatively balanced ecosystem. Underwater target fish detection is of great significance to fishery resource investigations. Traditional investigation methods cannot meet the increasing requirements of environmental protection and investigation, and the existing target detection technology has few studies on the dynamic identification of underwater fish and small targets. To reduce environmental disturbances and solve the problems of many fish, dense, mutual occlusion and difficult detection of small targets, an improved CME-YOLOv5 network is proposed to detect fish in dense groups and small targets. First, the coordinate attention (CA) mechanism and cross-stage partial networks with 3 convolutions (C3) structure are fused into the C3CA module to replace the C3 module of the backbone in you only look once (YOLOv5) to improve the extraction of target feature information and detection accuracy. Second, the three detection layers are expanded to four, which enhances the model's ability to capture information in different dimensions and improves detection performance. Finally, the efficient intersection over union (EIOU) loss function is used instead of the generalized intersection over union (GIOU) loss function to optimize the convergence rate and location accuracy. Based on the actual image data and a small number of datasets obtained online, the experimental results showed that the mean average precision (mAP@0.50) of the proposed algorithm reached 94.9\%, which is 4.4 percentage points higher than that of the YOLOv5 algorithm, and the number of fish and small target detection performances was 24.6\% higher. The results show that our proposed algorithm exhibits good detection performance when applied to densely spaced fish and small targets and can be used as an alternative or supplemental method for fishery resource investigation.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/dan/Zotero/storage/679NHW5B/Li et al. - 2022 - CME-YOLOv5 An Efficient Object Detection Network .pdf}
}

@article{liEnhancingUnderwaterImage2022,
  title = {Enhancing Underwater Image via Adaptive Color and Contrast Enhancement, and Denoising},
  author = {Li, Xinjie and Hou, Guojia and Li, Kunqian and Pan, Zhenkuan},
  year = 2022,
  month = may,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {111},
  pages = {104759},
  issn = {09521976},
  doi = {10.1016/j.engappai.2022.104759},
  urldate = {2024-10-20},
  abstract = {Images captured underwater are often characterized by low contrast, color distortion, and noise. To address these visual degradations, we propose a novel scheme by constructing an adaptive color and contrast enhancement, and denoising (ACCED) framework for underwater image enhancement. In the proposed framework, Difference of Gaussian (DoG) filter and bilateral filter are respectively employed to decompose the highfrequency and low-frequency components. Benefited from this separation, we utilize soft-thresholding operation to suppress the noise in the high-frequency component. Specially, the lowfrequency component is enhanced by using an adaptive color and contrast enhancement (ACCE) strategy. The proposed ACCE is an adaptive variational framework implemented in the HSI color space, which integrates data term and regularized term, as well as introduces Gaussian weight and Heaviside function to avoid overenhancement and oversaturation. Moreover, we derive a numerical solution for ACCE, and adopt a pyramid-based strategy to accelerate the solving procedure. Experimental results demonstrate that our strategy is effective in color correction, visibility improvement, and detail revealing. Comparison with state-of-the-art techniques also validate the superiority of proposed method. Furthermore, we have verified the utility of our proposed ACCE-D for enhancing other types of degraded scenes, including foggy scene, sandstorm scene and low-light scene.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/WBEC7AMT/Li et al. - 2022 - Enhancing underwater image via adaptive color and .pdf}
}

@misc{liEnhancingUnderwaterImage2022a,
  title = {Enhancing Underwater Image via Adaptive Color and Contrast Enhancement, and Denoising},
  author = {Li, Xinjie and Hou, Guojia and Li, Kunqian and Pan, Zhenkuan},
  year = 2022,
  month = feb,
  journal = {Elsevier BV},
  volume = {111},
  pages = {104759--104759},
  doi = {10.1016/j.engappai.2022.104759}
}

@misc{liFastAccurateFish2015,
  title = {Fast Accurate Fish Detection and Recognition of Underwater Images with {{Fast R-CNN}}},
  author = {Li, Xiu and Shang, Min and Qin, Hongwei and Chen, Liansheng},
  year = 2015,
  month = oct,
  pages = {1--5}
}

@misc{liFastAccurateFish2015a,
  title = {Fast Accurate Fish Detection and Recognition of Underwater Images with {{Fast R-CNN}}},
  author = {Li, Xiu and Shang, Min and Qin, Hongwei and Chen, Liansheng},
  year = 2015,
  month = oct,
  doi = {10.23919/oceans.2015.7404464}
}

@article{liFPGAbasedObjectDetection2022,
  title = {{{FPGA-based Object Detection Acceleration Architecture Design}}},
  author = {Li, Wenhao and Hu, Huaixiang},
  year = 2022,
  month = dec,
  journal = {J. Phys.: Conf. Ser.},
  volume = {2405},
  number = {1},
  pages = {012011},
  issn = {1742-6588, 1742-6596},
  doi = {10.1088/1742-6596/2405/1/012011},
  urldate = {2024-07-17},
  abstract = {Abstract             To solve the problems of insufficient acceleration capability and high power consumption of the existing embedded terminal target detection, a multi-strategy optimization hardware accelerator based on the YOLOv2 model is designed by taking advantage of the high concurrency characteristics of the CPU+FPGA structure. The accelerator improves its parallelism and real-time performance while reducing power consumption and resource consumption through floating-point quantization, adder optimization, and various HLS optimization strategies. Tested on Xilinx's PYNQ-Z2 platform, the overall computing power of the accelerator reaches 27.1GOP/s, the average detection accuracy is 80.6\%, and the overall power consumption is 2.609W. Compared with CPU and GPU, the obtained accuracy only loses 2\%, but the power consumption is greatly reduced. It has strong practical significance in edge target detection that requires high real-time performance and low power consumption.}
}

@misc{liPruningFiltersEfficient2016,
  title = {Pruning {{Filters}} for {{Efficient ConvNets}}},
  author = {Li, Hao and Kadav, Asim and {\DJ}ur{\dj}anovi{\'c}, Igor and Samet, Hanan and Graf, Hans Peter},
  year = 2016,
  month = jan
}

@misc{liPruningFiltersEfficient2017,
  title = {Pruning {{Filters}} for {{Efficient ConvNets}}},
  author = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  year = 2017,
  month = mar,
  number = {arXiv:1608.08710},
  eprint = {1608.08710},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-21},
  abstract = {The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34\% and ResNet-110 by up to 38\% on CIFAR10 while regaining close to the original accuracy by retraining the networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Published as a conference paper at ICLR 2017},
  file = {/Users/dan/Zotero/storage/QEKNCLY9/Li et al. - 2017 - Pruning Filters for Efficient ConvNets.pdf;/Users/dan/Zotero/storage/9GSANJI7/1608.html}
}

@misc{lite2021a,
  title = {{{TensorFlow}} Lite \textbar{} Machine Learning for Mobile and Edge Devices},
  author = {Lite, TensorFlow},
  year = 2021,
  citation-number = {60},
  langid = {english}
}

@misc{lite2021a,
  title = {{{TensorFlow}} Lite \textbar{} Machine Learning for Mobile and Edge Devices},
  author = {Lite, TensorFlow},
  year = 2021,
  citation-number = {60},
  langid = {english}
}

@misc{lite2021a,
  title = {{{TensorFlow}} Lite \textbar{} Machine Learning for Mobile and Edge Devices},
  author = {Lite, TensorFlow},
  year = 2021,
  citation-number = {60},
  langid = {english}
}

@article{littleEuropeanComputerDriving2006,
  title = {The {{European}} Computer Driving License: Are We All Aware?},
  shorttitle = {The {{European}} Computer Driving License},
  author = {Little, N. and Rogers, B.},
  year = 2006,
  month = jun,
  journal = {Br J Hosp Med (Lond)},
  volume = {67},
  number = {6},
  pages = {327},
  issn = {1750-8460},
  langid = {english},
  pmid = {16821750},
  keywords = {Computers,Educational Measurement,Health Personnel,Humans,Licensure,Medical Records Systems Computerized}
}

@inproceedings{liu2016a,
  title = {{SSD: Single shot multibox detector}},
  booktitle = {{European conference on computer vision}},
  author = {Liu, W.},
  year = 2016,
  pages = {21--37},
  publisher = {Springer},
  citation-number = {28},
  langid = {danish}
}

@inproceedings{liu2016a,
  title = {{SSD: Single shot multibox detector}},
  booktitle = {{European conference on computer vision}},
  author = {Liu, W.},
  year = 2016,
  pages = {21--37},
  publisher = {Springer},
  citation-number = {28},
  langid = {danish}
}

@inproceedings{liu2016a,
  title = {{SSD: Single shot multibox detector}},
  booktitle = {{European conference on computer vision}},
  author = {Liu, W.},
  year = 2016,
  pages = {21--37},
  publisher = {Springer},
  citation-number = {28},
  langid = {danish}
}

@article{liu2020a,
  title = {Deep Learning for Generic Object Detection: {{A}} Survey},
  author = {Liu, L.},
  year = 2020,
  journal = {International Journal of Computer Vision},
  volume = {128},
  number = {2},
  pages = {261--318},
  citation-number = {26},
  langid = {english}
}

@article{liu2020a,
  title = {Deep Learning for Generic Object Detection: {{A}} Survey},
  author = {Liu, L.},
  year = 2020,
  journal = {International Journal of Computer Vision},
  volume = {128},
  number = {2},
  pages = {261--318},
  citation-number = {26},
  langid = {english}
}

@article{liu2020a,
  title = {Deep Learning for Generic Object Detection: {{A}} Survey},
  author = {Liu, L.},
  year = 2020,
  journal = {International Journal of Computer Vision},
  volume = {128},
  number = {2},
  pages = {261--318},
  citation-number = {26},
  langid = {english}
}

@misc{liuLearningEfficientConvolutional2017,
  title = {Learning {{Efficient Convolutional Networks}} through {{Network Slimming}}},
  author = {Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  year = 2017,
  month = aug,
  number = {arXiv:1708.06519},
  eprint = {1708.06519},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-21},
  abstract = {The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20x reduction in model size and a 5x reduction in computing operations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Accepted by ICCV 2017},
  file = {/Users/dan/Zotero/storage/VVXD3AUP/Liu et al. - 2017 - Learning Efficient Convolutional Networks through .pdf;/Users/dan/Zotero/storage/BJUYPUKA/1708.html}
}

@misc{liuLearningEfficientConvolutional2017a,
  title = {Learning {{Efficient Convolutional Networks}} through {{Network Slimming}}},
  author = {Liu, Zhuang and Jiang, Fang and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  year = 2017,
  month = oct
}

@misc{liuLearningEfficientConvolutional2017b,
  title = {Learning {{Efficient Convolutional Networks}} through {{Network Slimming}}},
  author = {Liu, Zhuang and Jiang, Fang and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  year = 2017,
  month = oct,
  doi = {10.1109/iccv.2017.298}
}

@misc{liuTensorRTAccelerationBased2022,
  title = {{{TensorRT}} Acceleration Based on Deep Learning {{OFDM}} Channel Compensation},
  author = {Liu, Zhongqian and Ding, Dan},
  year = 2022,
  month = jul,
  journal = {IOP Publishing},
  volume = {2303},
  number = {1},
  pages = {012047--012047},
  doi = {10.1088/1742-6596/2303/1/012047}
}

@misc{liuTensorRTAccelerationBased2022a,
  title = {{{TensorRT}} Acceleration Based on Deep Learning {{OFDM}} Channel Compensation},
  author = {Liu, Zhongqian and Ding, Dan},
  year = 2022,
  month = jul,
  volume = {2303},
  number = {1},
  pages = {012047--012047}
}

@article{liuUnderwaterObjectDetection2023,
  title = {Underwater {{Object Detection Using TC-YOLO}} with {{Attention Mechanisms}}},
  author = {Liu, Kun and Peng, Lei and Tang, Shanran},
  year = 2023,
  month = feb,
  journal = {Sensors},
  volume = {23},
  number = {5},
  pages = {2567},
  issn = {1424-8220},
  doi = {10.3390/s23052567},
  urldate = {2024-10-22},
  abstract = {Underwater object detection is a key technology in the development of intelligent underwater vehicles. Object detection faces unique challenges in underwater applications: blurry underwater images; small and dense targets; and limited computational capacity available on the deployed platforms. To improve the performance of underwater object detection, we proposed a new object detection approach that combines a new detection neural network called TC-YOLO, an image enhancement technique using an adaptive histogram equalization algorithm, and the optimal transport scheme for label assignment. The proposed TC-YOLO network was developed based on YOLOv5s. Transformer self-attention and coordinate attention were adopted in the backbone and neck of the new network, respectively, to enhance feature extraction for underwater objects. The application of optimal transport label assignment enables a significant reduction in the number of fuzzy boxes and improves the utilization of training data. Our tests using the RUIE2020 dataset and ablation experiments demonstrate that the proposed approach performs better than the original YOLOv5s and other similar networks for underwater object detection tasks; moreover, the size and computational cost of the proposed model remain small for underwater mobile applications.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/dan/Zotero/storage/DXCCM6YM/Liu et al. - 2023 - Underwater Object Detection Using TC-YOLO with Att.pdf}
}

@misc{liuYOLOStereo3DStepBack2021,
  title = {{{YOLOStereo3D}}: {{A Step Back}} to {{2D}} for {{Efficient Stereo 3D Detection}}},
  shorttitle = {{{YOLOStereo3D}}},
  author = {Liu, Yuxuan and Wang, Lujia and Liu, Ming},
  year = 2021,
  month = mar,
  number = {arXiv:2103.09422},
  eprint = {2103.09422},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-17},
  abstract = {Object detection in 3D with stereo cameras is an important problem in computer vision, and is particularly crucial in low-cost autonomous mobile robots without LiDARs. Nowadays, most of the best-performing frameworks for stereo 3D object detection are based on dense depth reconstruction from disparity estimation, making them extremely computationally expensive. To enable real-world deployments of vision detection with binocular images, we take a step back to gain insights from 2D image-based detection frameworks and enhance them with stereo features. We incorporate knowledge and the inference structure from real-time one-stage 2D/3D object detector and introduce a light-weight stereo matching module. Our proposed framework, YOLOStereo3D, is trained on one single GPU and runs at more than ten fps. It demonstrates performance comparable to state-of-the-art stereo 3D detection frameworks without usage of LiDAR data. The code will be published in https://github.com/Owen-Liuyuxuan/visualDet3D.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Accepcted by ICRA 2021. The arxiv version contains slightly more information than the final ICRA version due to limit in the page number},
  file = {/Users/dan/Zotero/storage/WGZFK86Y/Liu et al. - 2021 - YOLOStereo3D A Step Back to 2D for Efficient Ster.pdf;/Users/dan/Zotero/storage/AFCQ9N5H/2103.html}
}

@misc{liYOLOv6SingleStageObject2022,
  title = {{{YOLOv6}}: {{A Single-Stage Object Detection Framework}} for {{Industrial Applications}}},
  shorttitle = {{{YOLOv6}}},
  author = {Li, Chuyi and Li, Lulu and Jiang, Hongliang and Weng, Kaiheng and Geng, Yifei and Li, Liang and Ke, Zaidan and Li, Qingyuan and Cheng, Meng and Nie, Weiqiang and Li, Yiduo and Zhang, Bo and Liang, Yufei and Zhou, Linyuan and Xu, Xiaoming and Chu, Xiangxiang and Wei, Xiaoming and Wei, Xiaolin},
  year = 2022,
  month = sep,
  number = {arXiv:2209.02976},
  eprint = {2209.02976},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-19},
  abstract = {For years, the YOLO series has been the de facto industry-level standard for efficient object detection. The YOLO community has prospered overwhelmingly to enrich its use in a multitude of hardware platforms and abundant scenarios. In this technical report, we strive to push its limits to the next level, stepping forward with an unwavering mindset for industry application. Considering the diverse requirements for speed and accuracy in the real environment, we extensively examine the up-to-date object detection advancements either from industry or academia. Specifically, we heavily assimilate ideas from recent network design, training strategies, testing techniques, quantization, and optimization methods. On top of this, we integrate our thoughts and practice to build a suite of deployment-ready networks at various scales to accommodate diversified use cases. With the generous permission of YOLO authors, we name it YOLOv6. We also express our warm welcome to users and contributors for further enhancement. For a glimpse of performance, our YOLOv6-N hits 35.9\% AP on the COCO dataset at a throughput of 1234 FPS on an NVIDIA Tesla T4 GPU. YOLOv6-S strikes 43.5\% AP at 495 FPS, outperforming other mainstream detectors at the same scale\textasciitilde (YOLOv5-S, YOLOX-S, and PPYOLOE-S). Our quantized version of YOLOv6-S even brings a new state-of-the-art 43.3\% AP at 869 FPS. Furthermore, YOLOv6-M/L also achieves better accuracy performance (i.e., 49.5\%/52.3\%) than other detectors with a similar inference speed. We carefully conducted experiments to validate the effectiveness of each component. Our code is made available at https://github.com/meituan/YOLOv6.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: technical report},
  file = {/Users/dan/Zotero/storage/X3CMVD5K/Li et al. - 2022 - YOLOv6 A Single-Stage Object Detection Framework .pdf}
}

@misc{longPPYOLOEffectiveEfficient2020,
  title = {{{PP-YOLO}}: {{An Effective}} and {{Efficient Implementation}} of {{Object Detector}}},
  shorttitle = {{{PP-YOLO}}},
  author = {Long, Xiang and Deng, Kaipeng and Wang, Guanzhong and Zhang, Yang and Dang, Qingqing and Gao, Yuan and Shen, Hui and Ren, Jianguo and Han, Shumin and Ding, Errui and Wen, Shilei},
  year = 2020,
  month = aug,
  number = {arXiv:2007.12099},
  eprint = {2007.12099},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-20},
  abstract = {Object detection is one of the most important areas in computer vision, which plays a key role in various practical scenarios. Due to limitation of hardware, it is often necessary to sacrifice accuracy to ensure the infer speed of the detector in practice. Therefore, the balance between effectiveness and efficiency of object detector must be considered. The goal of this paper is to implement an object detector with relatively balanced effectiveness and efficiency that can be directly applied in actual application scenarios, rather than propose a novel detection model. Considering that YOLOv3 has been widely used in practice, we develop a new object detector based on YOLOv3. We mainly try to combine various existing tricks that almost not increase the number of model parameters and FLOPs, to achieve the goal of improving the accuracy of detector as much as possible while ensuring that the speed is almost unchanged. Since all experiments in this paper are conducted based on PaddlePaddle, we call it PPYOLO. By combining multiple tricks, PP-YOLO can achieve a better balance between effectiveness (45.2\% mAP) and efficiency (72.9 FPS), surpassing the existing state-of-theart detectors such as EfficientDet and YOLOv4. Source code is at https://github.com/PaddlePaddle/ PaddleDetection.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dan/Zotero/storage/IJQLK9WK/Long et al. - 2020 - PP-YOLO An Effective and Efficient Implementation.pdf}
}

@article{loOnlineToolsPredicting2024,
  title = {Online Tools for Predicting Melanoma Survival: {{Including}} Sentinel Node Status as a Variable Improves Prediction Accuracy},
  shorttitle = {Online Tools for Predicting Melanoma Survival},
  author = {Lo, Serigne N. and Varey, Alexander H. R. and El Sharouni, Mary-Ann and Scolyer, Richard A. and Thompson, John F.},
  year = 2024,
  journal = {Journal of the European Academy of Dermatology and Venereology},
  volume = {38},
  number = {2},
  pages = {e182-e184},
  issn = {1468-3083},
  doi = {10.1111/jdv.19524},
  urldate = {2024-04-19},
  langid = {english},
  file = {/Users/dan/Zotero/storage/XCZPNVWB/Lo et al. - 2024 - Online tools for predicting melanoma survival Inc.pdf;/Users/dan/Zotero/storage/P6XRYR6S/jdv.html}
}

@article{loOnlineToolsPredicting2024a,
  title = {Online Tools for Predicting Melanoma Survival: {{Including}} Sentinel Node Status as a Variable Improves Prediction Accuracy},
  shorttitle = {Online Tools for Predicting Melanoma Survival},
  author = {Lo, Serigne N. and Varey, Alexander H. R. and El Sharouni, Mary-Ann and Scolyer, Richard A. and Thompson, John F.},
  year = 2024,
  journal = {Journal of the European Academy of Dermatology and Venereology},
  volume = {38},
  number = {2},
  pages = {e182-e184},
  issn = {1468-3083},
  doi = {10.1111/jdv.19524},
  urldate = {2024-04-19},
  langid = {english},
  file = {/Users/dan/Zotero/storage/8G3PGVY4/Lo et al. - 2024 - Online tools for predicting melanoma survival Inc.pdf;/Users/dan/Zotero/storage/IMN7IM2T/jdv.html}
}

@article{lowe2004a,
  title = {Distinctive Image Features from Scale-Invariant Keypoints},
  author = {Lowe, D.G.},
  year = 2004,
  journal = {International Journal of Computer Vision},
  volume = {60},
  number = {2},
  pages = {91--110},
  citation-number = {25},
  langid = {english}
}

@article{lowe2004a,
  title = {Distinctive Image Features from Scale-Invariant Keypoints},
  author = {Lowe, D.G.},
  year = 2004,
  journal = {International Journal of Computer Vision},
  volume = {60},
  number = {2},
  pages = {91--110},
  citation-number = {25},
  langid = {english}
}

@article{lowe2004a,
  title = {Distinctive Image Features from Scale-Invariant Keypoints},
  author = {Lowe, D.G.},
  year = 2004,
  journal = {International Journal of Computer Vision},
  volume = {60},
  number = {2},
  pages = {91--110},
  citation-number = {25},
  langid = {english}
}

@article{lu2013a,
  title = {Underwater Image Enhancement Using Guided Trigonometric Bilateral Filter and Fast Automatic Color Correction},
  author = {Lu, H. and Li, Y. and Serikawa, S.},
  year = 2013,
  journal = {Sensors},
  volume = {13},
  number = {12},
  pages = {17064--17085},
  citation-number = {20},
  langid = {english}
}

@misc{luoThiNetFilterLevel2017,
  title = {{{ThiNet}}: {{A Filter Level Pruning Method}} for {{Deep Neural Network Compression}}},
  shorttitle = {{{ThiNet}}},
  author = {Luo, Jian-Hao and Wu, Jianxin and Lin, Weiyao},
  year = 2017,
  month = jul,
  number = {arXiv:1707.06342},
  eprint = {1707.06342},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-21},
  abstract = {We propose an efficient and unified framework, namely ThiNet, to simultaneously accelerate and compress CNN models in both training and inference stages. We focus on the filter level pruning, i.e., the whole filter would be discarded if it is less important. Our method does not change the original network structure, thus it can be perfectly supported by any off-the-shelf deep learning libraries. We formally establish filter pruning as an optimization problem, and reveal that we need to prune filters based on statistics information computed from its next layer, not the current layer, which differentiates ThiNet from existing methods. Experimental results demonstrate the effectiveness of this strategy, which has advanced the state-of-the-art. We also show the performance of ThiNet on ILSVRC-12 benchmark. ThiNet achieves 3.31\$\textbackslash times\$ FLOPs reduction and 16.63\$\textbackslash times\$ compression on VGG-16, with only 0.52\$\textbackslash\%\$ top-5 accuracy drop. Similar experiments with ResNet-50 reveal that even for a compact network, ThiNet can also reduce more than half of the parameters and FLOPs, at the cost of roughly 1\$\textbackslash\%\$ top-5 accuracy drop. Moreover, the original VGG-16 model can be further pruned into a very small model with only 5.05MB model size, preserving AlexNet level accuracy but showing much stronger generalization ability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: To appear in ICCV 2017},
  file = {/Users/dan/Zotero/storage/YU9EWMY7/Luo et al. - 2017 - ThiNet A Filter Level Pruning Method for Deep Neu.pdf;/Users/dan/Zotero/storage/955IH324/1707.html}
}

@misc{luoThiNetFilterLevel2017a,
  title = {{{ThiNet}}: {{A Filter Level Pruning Method}} for {{Deep Neural Network Compression}}},
  author = {Luo, Jian-Hao and Wu, Jianxin and Lin, Weiyao},
  year = 2017,
  month = oct
}

@article{luUnderwaterOpticalImage2017,
  title = {Underwater {{Optical Image Processing}}: A {{Comprehensive Review}}},
  shorttitle = {Underwater {{Optical Image Processing}}},
  author = {Lu, Huimin and Li, Yujie and Zhang, Yudong and Chen, Min and Serikawa, Seiichi and Kim, Hyoungseop},
  year = 2017,
  month = dec,
  journal = {Mobile Netw Appl},
  volume = {22},
  number = {6},
  pages = {1204--1211},
  issn = {1383-469X, 1572-8153},
  doi = {10.1007/s11036-017-0863-4},
  urldate = {2024-10-19},
  abstract = {Underwater cameras are widely used to observe the sea floor. They are usually included in autonomous underwater vehicles (AUVs), unmanned underwater vehicles (UUVs), and in situ ocean sensor networks. Despite being an important sensor for monitoring underwater scenes, there exist many issues with recent underwater camera sensors. Because of light's transportation characteristics in water and the biological activity at the sea floor, the acquired underwater images often suffer from scatters and large amounts of noise. Over the last five years, many methods have been proposed to overcome traditional underwater imaging problems. This paper aims to review the state-of-the-art techniques in underwater image processing by highlighting the contributions and challenges presented in over 40 papers. We present an overview of various underwater image-processing approaches, such as underwater image de-scattering, underwater image color restoration, and underwater image quality assessments. Finally, we summarize the future trends and challenges in designing and processing underwater imaging sensors.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/4PHKTS29/Lu et al. - 2017 - Underwater Optical Image Processing a Comprehensi.pdf}
}

@article{macleod2010a,
  title = {Time to Automate Identification},
  author = {MacLeod, N. and Benfield, M. and Culverhouse, P.},
  year = 2010,
  journal = {Nature},
  volume = {467},
  number = {7312},
  pages = {154--155},
  citation-number = {10},
  langid = {english}
}

@misc{marichalEvaluationArchitectureawareOptimization2023,
  title = {Evaluation of Architecture-Aware Optimization Techniques for {{Convolutional Neural Networks}}},
  author = {Marichal, Ra{\'u}l and Toyos, Guillermo and Dufrechou, Ernesto and Ezzatti, Pablo},
  year = 2023,
  month = mar,
  doi = {10.1109/pdp59025.2023.00036}
}

@misc{marichalEvaluationArchitectureawareOptimization2023a,
  title = {Evaluation of Architecture-Aware Optimization Techniques for {{Convolutional Neural Networks}}},
  author = {Marichal, Ra{\'u}l and Toyos, Guillermo and Dufrechou, Ernesto and Ezzatti, Pablo},
  year = 2023,
  month = mar,
  pages = {177--184}
}

@article{matos-carvalhoStaticDynamicAlgorithms2019,
  title = {Static and {{Dynamic Algorithms}} for {{Terrain Classification}} in {{UAV Aerial Imagery}}},
  author = {{Matos-Carvalho}, J. P. and Moutinho, Filipe and Salvado, Ana Beatriz and Carrasqueira, Tiago and {Campos-Rebelo}, Rogerio and Pedro, D{\'a}rio and Campos, Lu{\'i}s Miguel and Fonseca, Jos{\'e} M. and Mora, Andr{\'e}},
  year = 2019,
  month = oct,
  journal = {Remote Sensing},
  volume = {11},
  number = {21},
  pages = {2501},
  issn = {2072-4292},
  doi = {10.3390/rs11212501},
  urldate = {2024-06-27},
  abstract = {The ability to precisely classify different types of terrain is extremely important for Unmanned Aerial Vehicles (UAVs). There are multiple situations in which terrain classification is fundamental for achieving a UAV's mission success, such as emergency landing, aerial mapping, decision making, and cooperation between UAVs in autonomous navigation. Previous research works describe different terrain classification approaches mainly using static features from RGB images taken onboard UAVs. In these works, the terrain is classified from each image taken as a whole, not divided into blocks; this approach has an obvious drawback when applied to images with multiple terrain types. This paper proposes a robust computer vision system to classify terrain types using three main algorithms, which extract features from UAV's downwash effect: Static texturesGray-Level Co-Occurrence Matrix (GLCM), Gray-Level Run Length Matrix (GLRLM) and Dynamic textures- Optical Flow method. This system has been fully implemented using the OpenCV library, and the GLCM algorithm has also been partially specified in a Hardware Description Language (VHDL) and implemented in a Field Programmable Gate Array (FPGA)-based platform. In addition to these feature extraction algorithms, a neural network was designed with the aim of classifying the terrain into one of four classes. Lastly, in order to store and access all the classified terrain information, a dynamic map, with this information was generated. The system was validated using videos acquired onboard a UAV with an RGB camera.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/dan/Downloads/remotesensing-11-02501-v2.pdf}
}

@incollection{mcglamery1979a,
  title = {A Computer Model for Underwater Camera Systems},
  booktitle = {Ocean Optics},
  author = {McGlamery, B.L.},
  year = 1979,
  volume = {VI (Vol. 208},
  pages = {221--231},
  publisher = {{International Society for Optics and Photonics}},
  citation-number = {19},
  langid = {english}
}

@inproceedings{mcglameryComputerModelUnderwater1980,
  title = {A {{Computer Model For Underwater Camera Systems}}},
  booktitle = {Ocean {{Optics VI}}},
  author = {McGlamery, B. L.},
  editor = {Duntley, Seibert Q.},
  year = 1980,
  month = mar,
  pages = {221--231},
  address = {Monterey},
  doi = {10.1117/12.958279},
  urldate = {2024-10-21}
}

@article{mcquadeSoftwareNeverDone2019,
  title = {Software {{Is Never Done}}: {{Refactoring}} the {{Acquisition Code}} for {{Competitive Advantage}}},
  author = {McQuade, J Michael and Murray, Richard M and Louie, Gilman and Medin, Milo and Pahlka, Jennifer and Stephens, Trae},
  year = 2019,
  langid = {english},
  file = {/Users/dan/Downloads/SWAP.REPORT_MAIN.BODY.3.21.19.pdf}
}

@article{mcquadeSoftwareNeverDone2019a,
  title = {Software {{Is Never Done}}: {{Refactoring}} the {{Acquisition Code}} for {{Competitive Advantage}}},
  author = {McQuade, J Michael and Murray, Richard M and Louie, Gilman and Medin, Milo and Pahlka, Jennifer and Stephens, Trae},
  year = 2019,
  langid = {english},
  file = {/Users/dan/Downloads/SWAP.REPORT_MAIN.BODY.3.21.19.pdf}
}

@article{mertens2009a,
  title = {Exposure Fusion: {{A}} Simple and Practical Alternative to High Dynamic Range Photography},
  author = {Mertens, T. and Kautz, J. and Reeth, F.},
  year = 2009,
  journal = {Computer Graphics Forum},
  volume = {28},
  number = {1},
  pages = {161--171},
  citation-number = {35},
  langid = {english}
}

@article{mertens2009a,
  title = {Exposure Fusion: {{A}} Simple and Practical Alternative to High Dynamic Range Photography},
  author = {Mertens, T.},
  year = 2009,
  journal = {Computer Graphics Forum},
  volume = {28},
  number = {1},
  pages = {161--171},
  citation-number = {24},
  langid = {english}
}

@article{mertens2009a,
  title = {Exposure Fusion: {{A}} Simple and Practical Alternative to High Dynamic Range Photography},
  author = {Mertens, T.},
  year = 2009,
  journal = {Computer Graphics Forum},
  volume = {28},
  number = {1},
  pages = {161--171},
  citation-number = {24},
  langid = {english}
}

@article{mertens2009a,
  title = {Exposure Fusion: {{A}} Simple and Practical Alternative to High Dynamic Range Photography},
  author = {Mertens, T.},
  year = 2009,
  journal = {Computer Graphics Forum},
  volume = {28},
  number = {1},
  pages = {161--171},
  citation-number = {24},
  langid = {english}
}

@article{mertensExposureFusionSimple2009,
  title = {Exposure {{Fusion}}: {{A Simple}} and {{Practical Alternative}} to {{High Dynamic Range Photography}}},
  shorttitle = {Exposure {{Fusion}}},
  author = {Mertens, T. and Kautz, J. and Van Reeth, F.},
  year = 2009,
  month = mar,
  journal = {Computer Graphics Forum},
  volume = {28},
  number = {1},
  pages = {161--171},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/j.1467-8659.2008.01171.x},
  urldate = {2024-10-22},
  abstract = {We propose a technique for fusing a bracketed exposure sequence into a high quality image, without converting to High dynamic range (HDR) first. Skipping the physically based HDR assembly step simplifies the acquisition pipeline. This avoids camera response curve calibration and is computationally efficient. It also allows for including flash images in the sequence. Our technique blends multiple exposures, guided by simple quality measures like saturation and contrast. This is done in a multiresolution fashion to account for the brightness variation in the sequence. The resulting image quality is comparable to existing tone mapping operators.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/E48MQ36T/Mertens et al. - 2009 - Exposure Fusion A Simple and Practical Alternativ.pdf}
}

@article{mittal2018a,
  title = {A Survey on Optimized Implementation of Deep Learning Models on the {{NVIDIA Jetson}} Platform},
  author = {Mittal, S.},
  year = 2018,
  journal = {Journal of Systems Architecture},
  volume = {97},
  pages = {428--442},
  citation-number = {53},
  langid = {english}
}

@article{mittal2018a,
  title = {A Survey on Optimized Implementation of Deep Learning Models on the {{NVIDIA Jetson}} Platform},
  author = {Mittal, S.},
  year = 2018,
  journal = {Journal of Systems Architecture},
  volume = {97},
  pages = {428--442},
  citation-number = {53},
  langid = {english}
}

@article{mittal2018a,
  title = {A Survey on Optimized Implementation of Deep Learning Models on the {{NVIDIA Jetson}} Platform},
  author = {Mittal, S.},
  year = 2018,
  journal = {Journal of Systems Architecture},
  volume = {97},
  pages = {428--442},
  citation-number = {53},
  langid = {english}
}

@article{mocellinSupportVectorMachine2006,
  title = {Support {{Vector Machine Learning Model}} for the {{Prediction}} of {{Sentinel Node Status}} in {{Patients With Cutaneous Melanoma}}},
  author = {Mocellin, Simone and Ambrosi, Alessandro and Montesco, Maria Cristina and Foletto, Mirto and Zavagno, Giorgio and Nitti, Donato and Lise, Mario and Rossi, Carlo Riccardo},
  year = 2006,
  month = aug,
  journal = {Ann Surg Oncol},
  volume = {13},
  number = {8},
  pages = {1113--1122},
  issn = {1068-9265, 1534-4681},
  doi = {10.1245/ASO.2006.03.019},
  urldate = {2024-04-21},
  abstract = {Background: Currently, approximately 80\% of melanoma patients undergoing sentinel node biopsy (SNB) have negative sentinel lymph nodes (SLNs), and no prediction system is reliable enough to be implemented in the clinical setting to reduce the number of SNB procedures. In this study, the predictive power of support vector machine (SVM)-based statistical analysis was tested. Methods: The clinical records of 246 patients who underwent SNB at our institution were used for this analysis. The following clinicopathologic variables were considered: the patient\~Os age and sex and the tumor\~Os histological subtype, Breslow thickness, Clark level, ulceration, mitotic index, lymphocyte infiltration, regression, angiolymphatic invasion, microsatellitosis, and growth phase. The results of SVM-based prediction of SLN status were compared with those achieved with logistic regression. Results: The SLN positivity rate was 22\% (52 of 234). When the accuracy was \ddag 80\%, the negative predictive value, positive predictive value, specificity, and sensitivity were 98\%, 54\%, 94\%, and 77\% and 82\%, 41\%, 69\%, and 93\% by using SVM and logistic regression, respectively. Moreover, SVM and logistic regression were associated with a diagnostic error and an SNB percentage reduction of (1) 1\% and 60\% and (2) 15\% and 73\%, respectively. Conclusions: The results from this pilot study suggest that SVM-based prediction of SLN status might be evaluated as a prognostic method to avoid the SNB procedure in 60\% of patients currently eligible, with a very low error rate. If validated in larger series, this strategy would lead to obvious advantages in terms of both patient quality of life and costs for the health care system.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/dan/Zotero/storage/N5CNHBVI/Mocellin et al. - 2006 - Support Vector Machine Learning Model for the Pred.pdf}
}

@article{mohdazmiNaturalbasedUnderwaterImage2019,
  title = {Natural-Based Underwater Image Color Enhancement through Fusion of Swarm-Intelligence Algorithm},
  author = {Mohd Azmi, Kamil Zakwan and Abdul Ghani, Ahmad Shahrizan and Md Yusof, Zulkifli and Ibrahim, Zuwairie},
  year = 2019,
  month = dec,
  journal = {Applied Soft Computing},
  volume = {85},
  pages = {105810},
  issn = {15684946},
  doi = {10.1016/j.asoc.2019.105810},
  urldate = {2024-10-12},
  abstract = {Underwater imagery suffers from severe effects due to selective attenuation and scattering effects when light travels through water medium. Such damages limit the ability of vision tasks and reduce image quality. There are a lot of enhancement methods to improve the quality of underwater image. However, most of the methods produce distortion effects in the output images. The proposed naturalbased underwater image color enhancement (NUCE) method consists of four steps. The first step introduces a new approach to neutralize underwater color cast. The inferior color channels are enhanced based on gain factors, which are calculated by considering the differences between the superior and inferior color channels. In the second step, the dual-intensity images fusion based on average of mean and median values is proposed to produce lower-stretched and upper-stretched histograms. The composition between these histograms improves the image contrast significantly. Next, the swarm-intelligence based mean equalization is proposed to improve the naturalness of the output image. Through the fusion of swarm intelligence algorithm, the mean values of inferior color channels are adjusted to be closed to the mean value of superior color channel. Lastly, the unsharp masking technique is applied to sharpen the overall image. Experiments on underwater images that are captured under various conditions indicate that the proposed NUCE method produces better output image quality, while significantly overcoming other state-of-the-art methods.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/65NKYKM2/Mohd Azmi et al. - 2019 - Natural-based underwater image color enhancement t.pdf}
}

@misc{molchanovPruningConvolutionalNeural2017,
  title = {Pruning {{Convolutional Neural Networks}} for {{Resource Efficient Inference}}},
  author = {Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  year = 2017,
  month = jun,
  number = {arXiv:1611.06440},
  eprint = {1611.06440},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-22},
  abstract = {We propose a new formulation for pruning convolutional kernels in neural networks to enable efficient inference. We interleave greedy criteria-based pruning with finetuning by backpropagation---a computationally efficient procedure that maintains good generalization in the pruned network. We propose a new criterion based on Taylor expansion that approximates the change in the cost function induced by pruning network parameters. We focus on transfer learning, where large pretrained networks are adapted to specialized tasks. The proposed criterion demonstrates superior performance compared to other criteria, e.g. the norm of kernel weights or feature map activation, for pruning large CNNs after adaptation to fine-grained classification tasks (Birds-200 and Flowers-102) relaying only on the first order gradient information. We also show that pruning can lead to more than 10\texttimes{} theoretical reduction in adapted 3D-convolutional filters with a small drop in accuracy in a recurrent gesture classifier. Finally, we show results for the largescale ImageNet dataset to emphasize the flexibility of our approach.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: 17 pages, 14 figures, ICLR 2017 paper},
  file = {/Users/dan/Zotero/storage/D8KMG6J3/Molchanov et al. - 2017 - Pruning Convolutional Neural Networks for Resource.pdf}
}

@incollection{montavonLayerWiseRelevancePropagation2019,
  title = {Layer-{{Wise Relevance Propagation}}: {{An Overview}}},
  shorttitle = {Layer-{{Wise Relevance Propagation}}},
  booktitle = {Explainable {{AI}}: {{Interpreting}}, {{Explaining}} and {{Visualizing Deep Learning}}},
  author = {Montavon, Gr{\'e}goire and Binder, Alexander and Lapuschkin, Sebastian and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  editor = {Samek, Wojciech and Montavon, Gr{\'e}goire and Vedaldi, Andrea and Hansen, Lars Kai and M{\"u}ller, Klaus-Robert},
  year = 2019,
  volume = {11700},
  pages = {193--209},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-28954-6_10},
  urldate = {2024-09-23},
  abstract = {For a machine learning model to generalize well, one needs to ensure that its decisions are supported by meaningful patterns in the input data. A prerequisite is however for the model to be able to explain itself, e.g. by highlighting which input features it uses to support its prediction. Layer-wise Relevance Propagation (LRP) is a technique that brings such explainability and scales to potentially highly complex deep neural networks. It operates by propagating the prediction backward in the neural network, using a set of purposely designed propagation rules. In this chapter, we give a concise introduction to LRP with a discussion of (1) how to implement propagation rules easily and efficiently, (2) how the propagation procedure can be theoretically justified as a `deep Taylor decomposition', (3) how to choose the propagation rules at each layer to deliver high explanation quality, and (4) how LRP can be extended to handle a variety of machine learning scenarios beyond deep neural networks.},
  isbn = {978-3-030-28953-9 978-3-030-28954-6},
  langid = {english},
  file = {/Users/dan/Zotero/storage/76GIUV44/Montavon et al. - 2019 - Layer-Wise Relevance Propagation An Overview.pdf}
}

@misc{montavonLayerWiseRelevancePropagation2019a,
  title = {Layer-{{Wise Relevance Propagation}}: {{An Overview}}},
  author = {Montavon, Gr{\'e}goire and Binder, Alexander and Lapuschkin, Sebastian and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  year = 2019,
  month = jan,
  volume = {11700},
  pages = {193--209}
}

@misc{montavonLayerWiseRelevancePropagation2019b,
  title = {Layer-{{Wise Relevance Propagation}}: {{An Overview}}},
  author = {Montavon, Gr{\'e}goire and Binder, Alexander and Lapuschkin, Sebastian and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  year = 2019,
  month = jan,
  journal = {Springer Science+Business Media},
  pages = {193--209},
  doi = {10.1007/978-3-030-28954-6_10}
}

@inproceedings{moosmannTinyissimoYOLOQuantizedLowMemory2023,
  title = {{{TinyissimoYOLO}}: {{A Quantized}}, {{Low-Memory Footprint}}, {{TinyML Object Detection Network}} for {{Low Power Microcontrollers}}},
  shorttitle = {{{TinyissimoYOLO}}},
  booktitle = {2023 {{IEEE}} 5th {{International Conference}} on {{Artificial Intelligence Circuits}} and {{Systems}} ({{AICAS}})},
  author = {Moosmann, Julian and Giordano, Marco and Vogt, Christian and Magno, Michele},
  year = 2023,
  month = jun,
  eprint = {2306.00001},
  primaryclass = {cs, eess},
  pages = {1--5},
  doi = {10.1109/AICAS57966.2023.10168657},
  urldate = {2024-09-08},
  abstract = {This paper introduces a highly flexible, quantized, memory-efficient, and ultra-lightweight object detection network, called TinyissimoYOLO. It aims to enable object detection on microcontrollers in the power domain of milliwatts, with less than 0.5 MB memory available for storing convolutional neural network (CNN) weights. The proposed quantized network architecture with 422 k parameters, enables real-time object detection on embedded microcontrollers, and it has been evaluated to exploit CNN accelerators. In particular, the proposed network has been deployed on the MAX78000 microcontroller achieving high frame-rate of up to 180 fps and an ultra-low energy consumption of only 196 \textmu J per inference with an inference efficiency of more than 106 MAC/Cycle. TinyissimoYOLO can be trained for any multi-object detection. However, considering the small network size, adding object detection classes will increase the size and memory consumption of the network, thus object detection with up to 3 classes is demonstrated. Furthermore, the network is trained using quantization-aware training and deployed with 8-bit quantization on different microcontrollers, such as STM32H7A3, STM32L4R9, Apollo4b and on the MAX78000's CNN accelerator. Performance evaluations are presented in this paper.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Hardware Architecture,Electrical Engineering and Systems Science - Image and Video Processing},
  note = {Comment: Published In: 2023 IEEE 5th International Conference on Artificial Intelligence Circuits and Systems (AICAS)},
  file = {/Users/dan/Zotero/storage/RA3BILTL/Moosmann et al. - 2023 - TinyissimoYOLO A Quantized, Low-Memory Footprint,.pdf}
}

@misc{moosmannTinyissimoYOLOQuantizedLowMemory2023a,
  title = {{{TinyissimoYOLO}}: {{A Quantized}}, {{Low-Memory Footprint}}, {{TinyML Object Detection Network}} for {{Low Power Microcontrollers}}},
  author = {Moosmann, Julian and Giordano, Marco and Vogt, Christian and Magno, Michele},
  year = 2023,
  month = jun,
  pages = {1--5}
}

@misc{moosmannTinyissimoYOLOQuantizedLowMemory2023b,
  title = {{{TinyissimoYOLO}}: {{A Quantized}}, {{Low-Memory Footprint}}, {{TinyML Object Detection Network}} for {{Low Power Microcontrollers}}},
  author = {Moosmann, Julian and Giordano, Marco and Vogt, Christian and Magno, Michele},
  year = 2023,
  month = jun,
  doi = {10.1109/aicas57966.2023.10168657}
}

@article{mora2011a,
  title = {How Many Species Are There on {{Earth}} and in the Ocean?},
  author = {Mora, C. and Tittensor, D.P. and Adl, S. and Simpson, A.G. and Worm, B.},
  year = 2011,
  journal = {PLoS Biology},
  volume = {9},
  number = {8},
  pages = {1001127},
  citation-number = {1},
  langid = {english}
}

@article{moraHowManySpecies2011,
  title = {How {{Many Species Are There}} on {{Earth}} and in the {{Ocean}}?},
  author = {Mora, Camilo and Tittensor, Derek P. and Adl, Sina and Simpson, Alastair G. B. and Worm, Boris},
  editor = {Mace, Georgina M.},
  year = 2011,
  month = aug,
  journal = {PLoS Biol},
  volume = {9},
  number = {8},
  pages = {e1001127},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1001127},
  urldate = {2024-10-20},
  abstract = {The diversity of life is one of the most striking aspects of our planet; hence knowing how many species inhabit Earth is among the most fundamental questions in science. Yet the answer to this question remains enigmatic, as efforts to sample the world's biodiversity to date have been limited and thus have precluded direct quantification of global species richness, and because indirect estimates rely on assumptions that have proven highly controversial. Here we show that the higher taxonomic classification of species (i.e., the assignment of species to phylum, class, order, family, and genus) follows a consistent and predictable pattern from which the total number of species in a taxonomic group can be estimated. This approach was validated against well-known taxa, and when applied to all domains of life, it predicts ,8.7 million (61.3 million SE) eukaryotic species globally, of which ,2.2 million (60.18 million SE) are marine. In spite of 250 years of taxonomic classification and over 1.2 million species already catalogued in a central database, our results suggest that some 86\% of existing species on Earth and 91\% of species in the ocean still await description. Renewed interest in further exploration and taxonomy is required if this significant gap in our knowledge of life on Earth is to be closed.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/WC48NDYY/Mora et al. - 2011 - How Many Species Are There on Earth and in the Oce.pdf}
}

@inproceedings{neubeck2006a,
  title = {Efficient Non-Maximum Suppression},
  booktitle = {18th International Conference on Pattern Recognition},
  author = {Neubeck, A. and Gool, L.},
  year = 2006,
  volume = {3},
  pages = {850--855},
  publisher = {IEEE},
  citation-number = {38},
  langid = {english}
}

@inproceedings{neubeck2006a,
  title = {Efficient Non-Maximum Suppression},
  booktitle = {18th International Conference on Pattern Recognition},
  author = {Neubeck, A. and Gool, L.},
  year = 2006,
  volume = {3},
  pages = {850--855},
  publisher = {IEEE},
  citation-number = {38},
  langid = {english}
}

@inproceedings{neubeck2006a,
  title = {Efficient Non-Maximum Suppression},
  booktitle = {18th International Conference on Pattern Recognition},
  author = {Neubeck, A. and Gool, L.},
  year = 2006,
  volume = {3},
  pages = {850--855},
  publisher = {IEEE},
  citation-number = {38},
  langid = {english}
}

@misc{niuLearningTrustworthyModel2023,
  title = {Learning {{Trustworthy Model}} from {{Noisy Labels}} Based on {{Rough Set}} for {{Surface Defect Detection}}},
  author = {Niu, Tongzhi and Li, Bin and Li, Kai and Lin, Yufeng and Li, Yuwei and Li, Weifeng and Wang, Zhenrong},
  year = 2023,
  month = jan,
  number = {arXiv:2301.10441},
  eprint = {2301.10441},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-19},
  abstract = {In the surface defect detection, there are some suspicious regions that cannot be uniquely classified as abnormal or normal. The annotating of suspicious regions is easily affected by factors such as workers' emotional fluctuations and judgment standard, resulting in noisy labels, which in turn leads to missing and false detections, and ultimately leads to inconsistent judgments of product quality. Unlike the usual noisy labels, the ones used for surface defect detection appear to be inconsistent rather than mislabeled. The noise occurs in almost every label and is difficult to correct or evaluate. In this paper, we proposed a framework that learns trustworthy models from noisy labels for surface defect defection. At first, to avoid the negative impact of noisy labels on the model, we represent the suspicious regions with consistent and precise elements at the pixel-level and redesign the loss function. Secondly, without changing network structure and adding any extra labels, pluggable spatially correlated Bayesian module is proposed. Finally, the defect discrimination confidence is proposed to measure the uncertainty, with which anomalies can be identified as defects. Our results indicate not only the effectiveness of the proposed method in learning from noisy labels, but also robustness and real-time performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  note = {Comment: 12 pages, 8figures},
  file = {/Users/dan/Zotero/storage/B32H2FIT/Niu et al. - 2023 - Learning Trustworthy Model from Noisy Labels based.pdf}
}

@article{oliverMachineLearningDirected2022,
  title = {Machine Learning Directed Sentinel Lymph Node Biopsy in Cutaneous Head and Neck Melanoma},
  author = {Oliver, Jamie R. and Karadaghy, Omar A. and Fassas, Scott N. and Arambula, Zack and Bur, Andr{\'e}s M.},
  year = 2022,
  month = apr,
  journal = {Head \& Neck},
  volume = {44},
  number = {4},
  pages = {975--988},
  issn = {1043-3074, 1097-0347},
  doi = {10.1002/hed.26993},
  urldate = {2024-04-21},
  abstract = {Background: The specificity of sentinel lymph node biopsy (SLNB) for detecting lymph node metastasis in head and neck melanoma (HNM) is low under current National Comprehensive Cancer Network (NCCN) treatment guidelines. Methods: Multiple machine learning (ML) algorithms were developed to identify HNM patients at very low risk of occult nodal metastasis using National Cancer Database (NCDB) data from 8466 clinically node negative HNM patients who underwent SLNB. SLNB performance under NCCN guidelines and ML algorithm recommendations was compared on independent test data from the NCDB (n = 2117) and an academic medical center (n = 96). Results: The top-performing ML algorithm (AUC = 0.734) recommendations obtained significantly higher specificity compared to the NCCN guidelines in both internal (25.8\% vs. 11.3\%, p {$<$} 0.001) and external test populations (30.1\% vs. 7.1\%, p {$<$} 0.001), while achieving sensitivity {$>$}97\%. Conclusion: Machine learning can identify clinically node negative HNM patients at very low risk of nodal metastasis, who may not benefit from SLNB.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/2Z3VK7YH/Oliver et al. - 2022 - Machine learning directed sentinel lymph node biop.pdf}
}

@article{padillacarrascoTYOLOTinyVehicle2023,
  title = {T-{{YOLO}}: {{Tiny Vehicle Detection Based}} on {{YOLO}} and {{Multi-Scale Convolutional Neural Networks}}},
  shorttitle = {T-{{YOLO}}},
  author = {Padilla Carrasco, Daniel and Rashwan, Hatem A. and Garc{\'i}a, Miguel {\'A}ngel and Puig, Dom{\`e}nec},
  year = 2023,
  journal = {IEEE Access},
  volume = {11},
  pages = {22430--22440},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3137638},
  urldate = {2024-09-23},
  abstract = {To solve real-life problems for different smart city applications, using deep Neural Network, such as parking occupancy detection, requires fine-tuning of these networks. For large parking, it is desirable to use a cenital-plane camera located at a high distance that allows the monitoring of the entire parking space or a large parking area with only one camera. Today's most popular object detection models, such as YOLO, achieve good precision scores at real-time speed. However, if we use our own data different from that of the general-purpose datasets, such as COCO and ImageNet, we have a large margin for improvisation. In this paper, we propose a modified, yet lightweight, deep object detection model based on the YOLO-v5 architecture. The proposed model can detect large, small, and tiny objects. Specifically, we propose the use of a multi-scale mechanism to learn deep discriminative feature representations at different scales and automatically determine the most suitable scales for detecting objects in a scene (i.e., in our case vehicles). The proposed multi-scale module reduces the number of trainable parameters compared to the original YOLO-v5 architecture. The experimental results also demonstrate that precision is improved by a large margin. In fact, as shown in the experiments, the results show a small reduction from 7.28 million parameters of the YOLO-v5-S profile to 7.26 million parameters in our model. In addition, we reduced the detection speed by inferring 30 fps compared to the YOLO-v5-L/X profiles. In addition, the tiny vehicle detection performance was significantly improved by 33\% compared to the YOLO-v5-X profile.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {/Users/dan/Zotero/storage/PZAZGFQN/Padilla Carrasco et al. - 2023 - T-YOLO Tiny Vehicle Detection Based on YOLO and M.pdf}
}

@article{pagireDeepLearningApproach2024,
  title = {A Deep Learning Approach for Underwater Fish Detection},
  author = {Pagire, Vrushali and Phadke, Anuradha and Hemant, J.},
  year = 2024,
  journal = {Journal of Integrated Science and Technology},
  volume = {12},
  number = {3},
  pages = {765--765},
  issn = {2321-4635},
  doi = {10.62110/sciencein.jist.2024.v12.765},
  urldate = {2024-04-22},
  abstract = {Identifying moving objects in video sequences is crucial for various applications, including underwater surveillance, biomedical detection, threat identification, defense, and navy. Oceanic environments often have deteriorating images due to water medium properties. In static conditions, the background remains stationary, while in dynamic conditions, both the background and foreground exhibit motion, making it difficult to differentiate between them. Automation of detection systems is essential for these applications. Therefore, the suggested system offers a deep learning-based solution for underwater fish recognition that employs three models: YOLO V3, SSD Mobile net V2, and Faster RCNN Resnet 50, all of which were trained on a bespoke dataset called Fish4knowledge. The algorithms have been taught to recognize and reliably pinpoint fish species in underwater photos and videos. To improve performance, data pre-treatment, model selection, and hyperparameter adjustment are carried out. The best-performing model is chosen after evaluation on a different validation dataset. The proposed method provides a viable solution for real-time underwater fish detection, which will aid in marine biology research, environmental monitoring, and autonomous underwater vehicles (AUVs). Model updates and adaption to changing undersea conditions are required for long-term accuracy and performance enhancement. The research results demonstrate that the SSD Mobilenet v2 model gives the highest precision value of 98.21\% as compared to the YOLO V3 and Faster RCNN Resnet50 models. URN:NBN:sciencein.jist.2024.v12.765},
  copyright = {Copyright (c) 2023 Vrushali Pagire, Anuradha Phadke, J Hemant},
  langid = {english},
  keywords = {anchor box,Deep learning,Faster R-CNN,MobileNet,Object Detection,underwater images,YOLOv3},
  file = {/Users/dan/Zotero/storage/45M4EUDE/Pagire et al. - 2024 - A deep learning approach for underwater fish detec.pdf}
}

@misc{panettaHumanVisualSystemInspiredUnderwaterImage2015,
  title = {Human-{{Visual-System-Inspired Underwater Image Quality Measures}}},
  author = {Panetta, Karen and Gao, Chen and Agaian, Sos {\cyrchar\CYRS}.},
  year = 2015,
  month = oct,
  journal = {Institute of Electrical and Electronics Engineers},
  volume = {41},
  number = {3},
  pages = {541--551},
  doi = {10.1109/joe.2015.2469915}
}

@article{panettaHumanVisualSystemInspiredUnderwaterImage2016,
  title = {Human-{{Visual-System-Inspired Underwater Image Quality Measures}}},
  author = {Panetta, Karen and Gao, Chen and Agaian, Sos},
  year = 2016,
  month = jul,
  journal = {IEEE J. Oceanic Eng.},
  volume = {41},
  number = {3},
  pages = {541--551},
  issn = {0364-9059, 1558-1691, 2373-7786},
  doi = {10.1109/JOE.2015.2469915},
  urldate = {2024-10-12},
  abstract = {Underwater images suffer from blurring effects, low contrast, and grayed out colors due to the absorption and scattering effects under the water. Many image enhancement algorithms for improving the visual quality of underwater images have been developed. Unfortunately, no well-accepted objective measure exists that can evaluate the quality of underwater images similar to human perception. Predominant underwater image processing algorithms use either a subjective evaluation, which is time consuming and biased, or a generic image quality measure, which fails to consider the properties of underwater images. To address this problem, a new nonreference underwater image quality measure (UIQM) is presented in this paper. The UIQM comprises three underwater image attribute measures: the underwater image colorfulness measure (UICM), the underwater image sharpness measure (UISM), and the underwater image contrast measure (UIConM). Each attribute is selected for evaluating one aspect of the underwater image degradation, and each presented attribute measure is inspired by the properties of human visual systems (HVSs). The experimental results demonstrate that the measures effectively evaluate the underwater image quality in accordance with the human perceptions. These measures are also used on the AirAsia 8501 wreckage images to show their importance in practical applications.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/Users/dan/Zotero/storage/R4WVNGYZ/Panetta et al. - 2016 - Human-Visual-System-Inspired Underwater Image Qual.pdf}
}

@article{paraschivClassificationUnderwaterFish2022,
  title = {Classification of {{Underwater Fish Images}} and {{Videos}} via {{Very Small Convolutional Neural Networks}}},
  author = {Paraschiv, Marius and Padrino, Ricardo and Casari, Paolo and Bigal, Eyal and Scheinin, Aviad and Tchernov, Dan and Fern{\'a}ndez Anta, Antonio},
  year = 2022,
  month = jun,
  journal = {Journal of Marine Science and Engineering},
  volume = {10},
  number = {6},
  pages = {736},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2077-1312},
  doi = {10.3390/jmse10060736},
  urldate = {2024-04-23},
  abstract = {The automatic classification of fish species appearing in images and videos from underwater cameras is a challenging task, albeit one with a large potential impact in environment conservation, marine fauna health assessment, and fishing policy. Deep neural network models, such as convolutional neural networks, are a popular solution to image recognition problems. However, such models typically require very large datasets to train millions of model parameters. Because underwater fish image and video datasets are scarce, non-uniform, and often extremely unbalanced, deep neural networks may be inadequately trained, and undergo a much larger risk of overfitting. In this paper, we propose small convolutional neural networks as a practical engineering solution that helps tackle fish image classification. The concept of ``small'' refers to the number of parameters of the resulting models: smaller models are lighter to run on low-power devices, and drain fewer resources per execution. This is especially relevant for fish recognition systems that run unattended on offshore platforms, often on embedded hardware. Here, established deep neural network models would require too many computational resources. We show that even networks with little more than 12,000 parameters provide an acceptable working degree of accuracy in the classification task (almost 42\% for six fish species), even when trained on small and unbalanced datasets. If the fish images come from videos, we augment the data via a low-complexity object tracking algorithm, increasing the accuracy to almost 49\% for six fish species. We tested the networks with images obtained from the deployments of an experimental system in the Mediterranean sea, showing a good level of accuracy given the low quality of the dataset.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {accuracy,fish classification,neural networks,optimized architectures,sea trial,underwater images},
  file = {/Users/dan/Zotero/storage/7F69GQAH/Paraschiv et al. - 2022 - Classification of Underwater Fish Images and Video.pdf}
}

@misc{paraschivClassificationUnderwaterFish2022a,
  title = {Classification of {{Underwater Fish Images}} and {{Videos}} via {{Very Small Convolutional Neural Networks}}},
  author = {Paraschiv, Marius and Padrino, Ricardo and Casari, Paolo and Bigal, Eyal and Scheinin, Aviad and Tchernov, Dan and Anta, Antonio Fern{\'a}ndez},
  year = 2022,
  month = may,
  volume = {10},
  number = {6},
  pages = {736}
}

@misc{paraschivClassificationUnderwaterFish2022b,
  title = {Classification of {{Underwater Fish Images}} and {{Videos}} via {{Very Small Convolutional Neural Networks}}},
  author = {Paraschiv, Marius and Padrino, Ricardo and Casari, Paolo and Bigal, Eyal and Scheinin, Aviad and Tchernov, Dan and Anta, Antonio Fern{\'a}ndez},
  year = 2022,
  month = may,
  journal = {Multidisciplinary Digital Publishing Institute},
  volume = {10},
  number = {6},
  pages = {736--736},
  doi = {10.3390/jmse10060736}
}

@article{paulySustainabilityWorldFisheries2002,
  title = {Towards Sustainability in World Fisheries},
  author = {Pauly, Daniel and Christensen, Villy and Gu{\'e}nette, Sylvie and Pitcher, Tony J. and Sumaila, U. Rashid and Walters, Carl J. and Watson, R. and Zeller, Dirk},
  year = 2002,
  month = aug,
  journal = {Nature},
  volume = {418},
  number = {6898},
  pages = {689--695},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature01017},
  urldate = {2024-10-20},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/dan/Zotero/storage/Y4C83JDY/Pauly et al. - 2002 - Towards sustainability in world fisheries.pdf}
}

@article{pizer1987a,
  title = {Adaptive Histogram Equalization and Its Variations},
  author = {Pizer, S.M. and Amburn, E.P. and Austin, J.D. and Cromartie, R. and Geselowitz, A. and Greer, T. and Zimmerman, J.B.},
  year = 1987,
  journal = {Computer Vision, Graphics, and Image Processing},
  volume = {39},
  number = {3},
  pages = {355--368},
  citation-number = {34},
  langid = {english}
}

@incollection{platt1999a,
  title = {Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods},
  booktitle = {Advances in Large Margin Classifiers},
  author = {Platt, J.},
  year = 1999,
  pages = {61--74},
  publisher = {MIT Press},
  citation-number = {44},
  langid = {english}
}

@incollection{platt1999a,
  title = {Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods},
  booktitle = {Advances in Large Margin Classifiers},
  author = {Platt, J.},
  year = 1999,
  pages = {61--74},
  publisher = {MIT Press},
  citation-number = {44},
  langid = {english}
}

@incollection{platt1999a,
  title = {Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods},
  booktitle = {Advances in Large Margin Classifiers},
  author = {Platt, J.},
  year = 1999,
  pages = {61--74},
  publisher = {MIT Press},
  citation-number = {44},
  langid = {english}
}

@article{prat-varelaImprovedBaitedRemote2023,
  title = {Improved {{Baited Remote Underwater Video}} ({{BRUV}}) for 24 h {{Real-Time Monitoring}} of {{Pelagic}} and {{Demersal Marine Species}} from the {{Epipelagic Zone}}},
  author = {{Prat-Varela}, Alejandro and Torres, Agusti and Cervantes, Daniel and {Aquino-Baleyt{\'o}}, Marc and Abril, Ana-Maria and Clua, Eric E. G.},
  year = 2023,
  month = jun,
  journal = {Journal of Marine Science and Engineering},
  volume = {11},
  number = {6},
  pages = {1182},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2077-1312},
  doi = {10.3390/jmse11061182},
  urldate = {2024-04-22},
  abstract = {Bait-based remote underwater video (BRUV) systems are effective devices for remotely observing fish and other marine organisms in challenging environments. The development of a long duration (24 h) surface BRUV observation surveys allowed the monitoring of scarce and elusive pelagic sharks and the direct impact on non-targeted species of longline fishing in the Western Mediterranean. Technological limitations, such as the limited storage capacity and a single surface camera, were improved by (i) adding a deep camera equipped with light (below 80 m depth) and (ii) replacing Gopros with a multi-camera video surveillance system (surface and depth) with a storage capacity of several days and access to real-time observation. Based on a deployment effort of 1884 h video data, we identified 11 blue sharks (Prionace glauca) and one bluntnose sixgill shark (Hexanchus griseus), a deep-sea species that scarcely swims at the surface. The real-time observation capability was a powerful tool for reducing logistical costs and for raising environmental awareness in educational and outreach programmes.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {blue shark <i>Prionace glauca</i>,bluntnose sixgill shark <i>Hexanchus griseus</i>,BRUV,demersal species,marine megafauna,Mediterranean sea,pelagic species,real-time observation,remote},
  file = {/Users/dan/Zotero/storage/DN8GKWPP/Prat-Varela et al. - 2023 - Improved Baited Remote Underwater Video (BRUV) for.pdf}
}

@misc{prat-varelaImprovedBaitedRemote2023a,
  title = {Improved {{Baited Remote Underwater Video}} ({{BRUV}}) for 24 h {{Real-Time Monitoring}} of {{Pelagic}} and {{Demersal Marine Species}} from the {{Epipelagic Zone}}},
  author = {{Prat-Varela}, Alejandro and Torres, Agusti and Cervantes, Daniel and Aquino-Baleyt{\'o}, Marc and Abril, Ana-Maria and Clua, {\'E}ric},
  year = 2023,
  month = jun,
  volume = {11},
  number = {6},
  pages = {1182}
}

@misc{prat-varelaImprovedBaitedRemote2023b,
  title = {Improved {{Baited Remote Underwater Video}} ({{BRUV}}) for 24 h {{Real-Time Monitoring}} of {{Pelagic}} and {{Demersal Marine Species}} from the {{Epipelagic Zone}}},
  author = {{Prat-Varela}, Alejandro and Torres, Agusti and Cervantes, Daniel and Aquino-Baleyt{\'o}, Marc and Abril, Ana-Maria and Clua, {\'E}ric},
  year = 2023,
  month = jun,
  journal = {Multidisciplinary Digital Publishing Institute},
  volume = {11},
  number = {6},
  pages = {1182--1182},
  doi = {10.3390/jmse11061182}
}

@book{pratt2007a,
  title = {Digital Image Processing: {{PIKS}} Scientific Inside},
  author = {Pratt, W.K.},
  year = 2007,
  publisher = {John Wiley \& Sons},
  citation-number = {36},
  langid = {english}
}

@article{premsankar2018a,
  title = {Edge Computing for the {{Internet}} of {{Things}}: {{A}} Case Study},
  author = {Premsankar, G. and Francesco, M. and Taleb, T.},
  year = 2018,
  journal = {IEEE Internet of Things Journal},
  volume = {5},
  number = {2},
  pages = {1275--1284},
  citation-number = {31},
  langid = {english}
}

@article{priorEstimatingPrecisionAccuracy2023,
  title = {Estimating Precision and Accuracy of Automated Video Post-Processing: {{A}} Step towards Implementation of {{AI}}/{{ML}} for Optics-Based Fish Sampling},
  shorttitle = {Estimating Precision and Accuracy of Automated Video Post-Processing},
  author = {Prior, Jack H. and Campbell, Matthew D. and Dawkins, Matthew and Mickle, Paul F. and Moorhead, Robert J. and Alaba, Simegnew Y. and Shah, Chiranjibi and Salisbury, Joseph R. and Rademacher, Kevin R. and Felts, A. Paul and Wallace, Farron},
  year = 2023,
  month = apr,
  journal = {Front. Mar. Sci.},
  volume = {10},
  pages = {1150651},
  issn = {2296-7745},
  doi = {10.3389/fmars.2023.1150651},
  urldate = {2024-04-23},
  abstract = {Increased necessity to monitor vital fish habitat has resulted in proliferation of camera-based observation methods and advancements in camera and processing technology. Automated image analysis through computer vision algorithms has emerged as a tool for fisheries to address big data needs, reduce human intervention, lower costs, and improve timeliness. Models have been developed in this study with the goal to implement such automated image analysis for commercially important Gulf of Mexico fish species and habitats. Further, this study proposes adapting comparative otolith aging methods and metrics for gauging model performance by comparing automated counts to validation set counts in addition to traditional metrics used to gauge AI/ML model performance (such as mean average precision - mAP). To evaluate model performance we calculated percent of stations matching ground-truthed counts, ratios of false-positive/negative detections, and coefficient of variation (CV) for each species over a range of filtered outputs using model generated confidence thresholds (CTs) for each detected and classified fish. Model performance generally improved with increased annotations per species, and false-positive detections were greatly reduced with a second iteration of model training. For all species and model combinations, false-positives were easily identified and removed by increasing the CT to classify more restrictively. Issues with occluded fish images and reduced performance were most prevalent for schooling species, whereas for other species lack of training data was likely limiting. For 23 of the examined species, only 7 achieved a CV less than 25\%. Thus, for most species, improvements to the training library will be needed and next steps will include a queried learning approach to bring balance to the models and focus during training. Importantly, for select species such as Red Snapper (               Lutjanus campechanus               ) current models are sufficiently precise to begin utilization to filter videos for automated, versus fully manual processing. The adaption of the otolith aging QA/QC process for this process is a first step towards giving researchers the ability to track model performance through time, thereby giving researchers who engage with the models, raw data, and derived products confidence in analyses and resultant management decisions.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/6AP7HCGZ/Prior et al. - 2023 - Estimating precision and accuracy of automated vid.pdf}
}

@misc{priorEstimatingPrecisionAccuracy2023a,
  title = {Estimating Precision and Accuracy of Automated Video Post-Processing: {{A}} Step towards Implementation of {{AI}}/{{ML}} for Optics-Based Fish Sampling},
  author = {Prior, Jack and Campbell, Matthew D. and Dawkins, Matthew and Mickle, Paul F. and Moorhead, Robert and Alaba, Simegnew Yihunie and Shah, Chiranjibi and Salisbury, Joseph and Rademacher, Kevin R. and Felts, A. Paul and Wallace, Farron},
  year = 2023,
  month = apr,
  volume = {10},
  pages = {1150651}
}

@misc{priorEstimatingPrecisionAccuracy2023b,
  title = {Estimating Precision and Accuracy of Automated Video Post-Processing: {{A}} Step towards Implementation of {{AI}}/{{ML}} for Optics-Based Fish Sampling},
  author = {Prior, Jack and Campbell, Matthew D. and Dawkins, Matthew and Mickle, Paul F. and Moorhead, Robert and Alaba, Simegnew Yihunie and Shah, Chiranjibi and Salisbury, Joseph and Rademacher, Kevin R. and Felts, A. Paul and Wallace, Farron},
  year = 2023,
  month = apr,
  journal = {Frontiers Media},
  volume = {10},
  doi = {10.3389/fmars.2023.1150651}
}

@incollection{qian2016a,
  title = {An Underwater Fish Image Dataset for Fair Evaluation of Recognition Algorithms},
  booktitle = {2016 {{IEEE}} Underwater Technology},
  author = {Qian, R.},
  year = 2016,
  pages = {1--5},
  publisher = {IEEE},
  citation-number = {3},
  langid = {english}
}

@incollection{qian2016a,
  title = {An Underwater Fish Image Dataset for Fair Evaluation of Recognition Algorithms},
  booktitle = {2016 {{IEEE}} Underwater Technology},
  author = {Qian, R.},
  year = 2016,
  pages = {1--5},
  publisher = {IEEE},
  citation-number = {3},
  langid = {english}
}

@incollection{qian2016a,
  title = {An Underwater Fish Image Dataset for Fair Evaluation of Recognition Algorithms},
  booktitle = {2016 {{IEEE}} Underwater Technology},
  author = {Qian, R.},
  year = 2016,
  pages = {1--5},
  publisher = {IEEE},
  citation-number = {3},
  langid = {english}
}

@article{RAISSI2019686,
  title = {Physics-Informed Neural Networks: {{A}} Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations},
  author = {Raissi, M. and Perdikaris, P. and Karniadakis, G.E.},
  year = 2019,
  journal = {Journal of Computational Physics},
  volume = {378},
  pages = {686--707},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2018.10.045},
  abstract = {We introduce physics-informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge--Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction--diffusion systems, and the propagation of nonlinear shallow-water waves.},
  keywords = {Data-driven scientific computing,Machine learning,Nonlinear dynamics,Predictive modeling,Runge-Kutta methods}
}

@article{raissiPhysicsinformedNeuralNetworks2019,
  title = {Physics-Informed Neural Networks: {{A}} Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations},
  shorttitle = {Physics-Informed Neural Networks},
  author = {Raissi, M. and Perdikaris, P. and Karniadakis, G.E.},
  year = 2019,
  month = feb,
  journal = {Journal of Computational Physics},
  volume = {378},
  pages = {686--707},
  issn = {00219991},
  doi = {10.1016/j.jcp.2018.10.045},
  urldate = {2024-12-09},
  langid = {english},
  file = {/Users/dan/Zotero/storage/YLPILM86/Raissi et al. - 2019 - Physics-informed neural networks A deep learning .pdf}
}

@article{rallapalli2016a,
  title = {Energy-Efficient Object Detection and Classification on Low-Power Embedded Devices},
  author = {Rallapalli, S.},
  year = 2016,
  journal = {IEEE Transactions on Multimedia},
  volume = {18},
  number = {9},
  pages = {1728--1739},
  citation-number = {51},
  langid = {english}
}

@article{rallapalli2016a,
  title = {Energy-Efficient Object Detection and Classification on Low-Power Embedded Devices},
  author = {Rallapalli, S.},
  year = 2016,
  journal = {IEEE Transactions on Multimedia},
  volume = {18},
  number = {9},
  pages = {1728--1739},
  citation-number = {51},
  langid = {english}
}

@article{rallapalli2016a,
  title = {Energy-Efficient Object Detection and Classification on Low-Power Embedded Devices},
  author = {Rallapalli, S.},
  year = 2016,
  journal = {IEEE Transactions on Multimedia},
  volume = {18},
  number = {9},
  pages = {1728--1739},
  citation-number = {51},
  langid = {english}
}

@article{raulmarichalEvaluationArchitectureawareOptimization2023,
  title = {Evaluation of Architecture-Aware Optimization Techniques for {{Convolutional Neural Networks}}},
  author = {{Ra\'ul Marichal} and {Guillermo Toyos} and {Ernesto Dufrechu} and {P. Ezzatti}},
  year = 2023,
  month = mar,
  journal = {2023 31st Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)},
  pages = {177--184},
  doi = {10.1109/PDP59025.2023.00036},
  file = {/Users/dan/Zotero/storage/BK8WB9IM/Ral Marichal et al. - 2023 - Evaluation of architecture-aware optimization tech.pdf}
}

@article{ravanbakhshAutomatedFishDetection2015,
  title = {Automated {{Fish Detection}} in {{Underwater Images Using Shape}}-{{Based Level Sets}}},
  author = {Ravanbakhsh, Mehdi and Shortis, Mark R. and Shafait, Faisal and Mian, Ajmal and Harvey, Euan S. and Seager, James W.},
  year = 2015,
  month = mar,
  journal = {The Photogrammetric Record},
  volume = {30},
  number = {149},
  pages = {46--62},
  issn = {0031-868X, 1477-9730},
  doi = {10.1111/phor.12091},
  urldate = {2024-04-23},
  abstract = {Underwater stereo-video systems are widely used for the measurement of fish. However, the effectiveness of stereo-video measurement has been limited because most operational systems still rely on a human operator. In this paper an automated approach for fish detection, using a shape-based level-sets framework, is presented. Knowledge of the shape of fish is modelled by principal component analysis (PCA). The Haar classifier is used for precise localisation of the fish head and snout in the image, which is vital information for close-proximity initialisation of the shape model. The approach has been tested on underwater images representing a variety of challenging situations typical of the underwater environment, such as background interference and poor contrast boundaries. The results obtained demonstrate that the approach is capable of overcoming these difficulties and capturing the fish outline to sub-pixel accuracy.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/YSY6DM42/Ravanbakhsh et al. - 2015 - Automated Fish Detection in Underwater Images Usin.pdf}
}

@misc{ravanbakhshAutomatedFishDetection2015a,
  title = {Automated {{Fish Detection}} in {{Underwater Images Using Shape}}-{{Based Level Sets}}},
  author = {Ravanbakhsh, Mehdi and Shortis, Mark R. and Shafait, Faisal and Mian, Ajmal and Harvey, Euan S. and Seager, James},
  year = 2015,
  month = mar,
  volume = {30},
  number = {149},
  pages = {46--62}
}

@misc{ravanbakhshAUTOMATEDFISHDETECTION2015b,
  title = {{{AUTOMATED FISH DETECTION IN UNDERWATER IMAGES USING SHAPE-BASED LEVEL SETS}}},
  author = {Ravanbakhsh, Mehdi and Shortis, Mark R. and Shafait, Faisal and Mian, Ajmal and Harvey, Euan S. and Seager, James W.},
  year = 2015,
  month = mar
}

@inproceedings{redmon2017a,
  title = {{{YOLO9000}}: {{Better}}, Faster, Stronger},
  booktitle = {2017 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Redmon, J. and Farhadi, A.},
  year = 2017,
  pages = {6517--6525},
  publisher = {IEEE},
  citation-number = {34},
  langid = {english}
}

@inproceedings{redmon2017a,
  title = {{{YOLO9000}}: {{Better}}, Faster, Stronger},
  booktitle = {2017 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Redmon, J. and Farhadi, A.},
  year = 2017,
  pages = {6517--6525},
  publisher = {IEEE},
  citation-number = {34},
  langid = {english}
}

@inproceedings{redmon2017a,
  title = {{{YOLO9000}}: {{Better}}, Faster, Stronger},
  booktitle = {2017 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Redmon, J. and Farhadi, A.},
  year = 2017,
  pages = {6517--6525},
  publisher = {IEEE},
  citation-number = {34},
  langid = {english}
}

@misc{redmon2018a,
  title = {{YOLOv3: An incremental improvement}},
  author = {Redmon, J. and Farhadi, A.},
  year = 2018,
  eprint = {1804.02767},
  archiveprefix = {arXiv},
  citation-number = {39},
  langid = {ht},
  note = {arXiv preprint arXiv:1804.02767.}
}

@misc{redmon2018a,
  title = {{YOLOv3: An incremental improvement}},
  author = {Redmon, J. and Farhadi, A.},
  year = 2018,
  eprint = {1804.02767},
  archiveprefix = {arXiv},
  citation-number = {31},
  langid = {ht},
  note = {arXiv preprint arXiv:1804.02767.}
}

@misc{redmon2018a,
  title = {{YOLOv3: An incremental improvement}},
  author = {Redmon, J. and Farhadi, A.},
  year = 2018,
  eprint = {1804.02767},
  archiveprefix = {arXiv},
  citation-number = {31},
  langid = {ht},
  note = {arXiv preprint arXiv:1804.02767.}
}

@misc{redmonYOLO9000BetterFaster2016,
  title = {{{YOLO9000}}: {{Better}}, {{Faster}}, {{Stronger}}},
  shorttitle = {{{YOLO9000}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  year = 2016,
  month = dec,
  number = {arXiv:1612.08242},
  eprint = {1612.08242},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-19},
  abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dan/Zotero/storage/FUV8QGWE/Redmon and Farhadi - 2016 - YOLO9000 Better, Faster, Stronger.pdf}
}

@misc{redmonYOLOv3IncrementalImprovement2018,
  title = {{{YOLOv3}}: {{An Incremental Improvement}}},
  shorttitle = {{{YOLOv3}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  year = 2018,
  month = apr,
  number = {arXiv:1804.02767},
  eprint = {1804.02767},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-02},
  abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320 \texttimes{} 320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 AP50 in 51 ms on a Titan X, compared to 57.5 AP50 in 198 ms by RetinaNet, similar performance but 3.8\texttimes{} faster. As always, all the code is online at https://pjreddie.com/yolo/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Tech Report},
  file = {/Users/dan/Zotero/storage/AJYU4UUH/Redmon and Farhadi - 2018 - YOLOv3 An Incremental Improvement.pdf}
}

@misc{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = 2016,
  month = may,
  number = {arXiv:1506.02640},
  eprint = {1506.02640},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-19},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dan/Zotero/storage/KX88NYGH/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf}
}

@incollection{ren2015a,
  title = {Faster {{R-CNN}}: {{Towards}} Real-Time Object Detection with Region Proposal Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ren, S.},
  year = 2015,
  pages = {91--99},
  citation-number = {27},
  langid = {english}
}

@incollection{ren2015a,
  title = {Faster {{R-CNN}}: {{Towards}} Real-Time Object Detection with Region Proposal Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ren, S.},
  year = 2015,
  pages = {91--99},
  citation-number = {27},
  langid = {english}
}

@incollection{ren2015a,
  title = {Faster {{R-CNN}}: {{Towards}} Real-Time Object Detection with Region Proposal Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ren, S.},
  year = 2015,
  pages = {91--99},
  citation-number = {27},
  langid = {english}
}

@article{rezaRealizationContrastLimited2004,
  title = {Realization of the {{Contrast Limited Adaptive Histogram Equalization}} ({{CLAHE}}) for {{Real-Time Image Enhancement}}},
  author = {Reza, Ali M.},
  year = 2004,
  month = aug,
  journal = {The Journal of VLSI Signal Processing-Systems for Signal, Image, and Video Technology},
  volume = {38},
  number = {1},
  pages = {35--44},
  issn = {0922-5773},
  doi = {10.1023/B:VLSI.0000028532.53893.82},
  urldate = {2024-10-22},
  abstract = {Acquired real-time image sequences, in their original form may not have good viewing quality due to lack of proper lighting or inherent noise. For example, in X-ray imaging, when continuous exposure is used to obtain an image sequence or video, usually low-level exposure is administered until the region of interest is identified. In this case, and many other similar situations, it is desired to improve the image quality in real-time. One particular method of interest, which extensively is used for enhancement of still images, is Contrast Limited Adaptive Histogram Equalization (CLAHE) proposed in [1] and summarized in [2]. This approach is computationally extensive and it is usually used for off-line image enhancement. Because of its performance, hardware implementation of this algorithm for enhancement of real-time image sequences is sought. In this paper, a system level realization of CLAHE is proposed, which is suitable for VLSI or FPGA implementation. The goal for this realization is to minimize the latency without sacrificing precision.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/6C9THFDE/Reza - 2004 - Realization of the Contrast Limited Adaptive Histo.pdf}
}

@inproceedings{richterModernizingAnalyticsMelanoma2017,
  title = {Modernizing {{Analytics}} for {{Melanoma}} with a {{Large-Scale Research Dataset}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Information Reuse}} and {{Integration}} ({{IRI}})},
  author = {Richter, Aaron N. and Khoshgoftaar, Taghi M.},
  year = 2017,
  month = aug,
  pages = {551--558},
  publisher = {IEEE Press},
  address = {San Diego, CA, USA},
  doi = {10.1109/IRI.2017.45},
  urldate = {2024-04-19},
  abstract = {We present the Modernizing Analytics for MELanoma (MAMEL) dataset: a real-world, dermatologyspecific research dataset specifically crafted to advance data mining and machine learning research in the field of melanoma diagnosis, analysis, and treatment. This dataset was collected and curated from Modernizing Medicine\&\#x2019;s EMA DermatologyTM application, a cloud-based Electronic Health Record (EHR) platform. A big data processing architecture, built on Apache Hadoop and Apache Spark, was used to collect all patient data, identify patients for the MAMEL dataset, and create and document all data elements. This paper outlines the application and data processing architectures, provides an exploratory analysis of data elements available in MAMEL, and discusses avenues for using this dataset in clinical decision support applications for melanoma.}
}

@inproceedings{richterModernizingAnalyticsMelanoma2017a,
  title = {Modernizing {{Analytics}} for {{Melanoma}} with a {{Large-Scale Research Dataset}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Information Reuse}} and {{Integration}} ({{IRI}})},
  author = {Richter, Aaron N. and Khoshgoftaar, Taghi M.},
  year = 2017,
  month = aug,
  pages = {551--558},
  publisher = {IEEE},
  address = {San Diego, CA},
  doi = {10.1109/IRI.2017.45},
  urldate = {2024-04-21},
  abstract = {We present the Modernizing Analytics for MELanoma (MAMEL) dataset: a real-world, dermatologyspecific research dataset specifically crafted to advance data mining and machine learning research in the field of melanoma diagnosis, analysis, and treatment. This dataset was collected and curated from Modernizing Medicine's EMA DermatologyTM application, a cloud-based Electronic Health Record (EHR) platform. A big data processing architecture, built on Apache Hadoop and Apache Spark, was used to collect all patient data, identify patients for the MAMEL dataset, and create and document all data elements. This paper outlines the application and data processing architectures, provides an exploratory analysis of data elements available in MAMEL, and discusses avenues for using this dataset in clinical decision support applications for melanoma.},
  isbn = {978-1-5386-1562-1},
  langid = {english},
  file = {/Users/dan/Zotero/storage/BJ7V9TQQ/Richter and Khoshgoftaar - 2017 - Modernizing Analytics for Melanoma with a Large-Sc.pdf}
}

@inproceedings{richterPredictingSentinelNode2017,
  title = {Predicting Sentinel Node Status in Melanoma from a Real-World {{EHR}} Dataset},
  booktitle = {2017 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Richter, Aaron N. and Khoshgoftaar, Taghi M.},
  year = 2017,
  month = nov,
  pages = {1872--1878},
  doi = {10.1109/BIBM.2017.8217945},
  urldate = {2024-04-19},
  abstract = {Melanoma is the fastest growing cancer worldwide, and 1 in 50 Americans will develop it in their lifetime. Sentinel lymph node (SLN) metastasis is one of the most important prognostic indicators for melanoma survival. We present several machine learning models for predicting SLN metastasis using data from a real-world dermatology electronic health record (EHR) system. The class label is the result of a sentinel lymph node biopsy, an elective procedure that can be performed for newly-diagnosed melanoma patients to determine if there is metastasis in the nearest lymph node. We show that a simple model, using solely Breslow thickness, can achieve predictive performance (AUC=0.769) comparable to a logistic regression model using 5 features (AUC=0.772, p=0.518). Current clinical recommendations are to perform a biopsy for patients with melanomas thicker than 1mm, however, when applying this 1mm threshold to the simple thickness model, it achieves 0\% sensitivity for melanomas {$<$};1mm. Using a random forest model, we achieve 78.9\% sensitivity (p{$<$};0.001) for melanomas {$<$};1mm. Our study shows that the probability of sentinel lymph node positivity is indeed linearly correlated to the tumor thickness (R2=0.934), and that machine learning models can effectively detect thin melanomas that warrant an SLN biopsy.},
  keywords = {Biological system modeling,Biopsy,clinical decision support,Lymph nodes,machine learning,Malignant tumors,melanoma,Metastasis,Predictive models,sentinel node,skin cancer},
  file = {/Users/dan/Zotero/storage/ZEJGFKTS/8217945.html}
}

@inproceedings{richterPredictingSentinelNode2017a,
  title = {Predicting Sentinel Node Status in Melanoma from a Real-World {{EHR}} Dataset},
  booktitle = {2017 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Richter, Aaron N. and Khoshgoftaar, Taghi M.},
  year = 2017,
  month = nov,
  pages = {1872--1878},
  publisher = {IEEE},
  address = {Kansas City, MO},
  doi = {10.1109/BIBM.2017.8217945},
  urldate = {2024-04-21},
  abstract = {Melanoma is the fastest growing cancer worldwide, and 1 in 50 Americans will develop it in their lifetime. Sentinel lymph node (SLN) metastasis is one of the most important prognostic indicators for melanoma survival. We present several machine learning models for predicting SLN metastasis using data from a real-world dermatology electronic health record (EHR) system. The class label is the result of a sentinel lymph node biopsy, an elective procedure that can be performed for newly-diagnosed melanoma patients to determine if there is metastasis in the nearest lymph node. We show that a simple model, using solely Breslow thickness, can achieve predictive performance (AUC=0.769) comparable to a logistic regression model using 5 features (AUC=0.772, p=0.518). Current clinical recommendations are to perform a biopsy for patients with melanomas thicker than 1mm, however, when applying this 1mm threshold to the simple thickness model, it achieves 0\% sensitivity for melanomas {$<$}1mm. Using a random forest model, we achieve 78.9\% sensitivity (p{$<$}0.001) for melanomas {$<$}1mm. Our study shows that the probability of sentinel lymph node positivity is indeed linearly correlated to the tumor thickness (R2=0.934), and that machine learning models can effectively detect thin melanomas that warrant an SLN biopsy.},
  isbn = {978-1-5090-3050-7},
  langid = {english},
  file = {/Users/dan/Zotero/storage/3ZUL64NT/Richter and Khoshgoftaar - 2017 - Predicting sentinel node status in melanoma from a.pdf}
}

@inproceedings{routProminentObjectDetection2020,
  title = {Prominent {{Object Detection}} in {{Underwater Environment}} Using a {{Dual-feature Framework}}},
  booktitle = {Global {{Oceans}} 2020: {{Singapore}} -- {{U}}.{{S}}. {{Gulf Coast}}},
  author = {Rout, Deepak Kumar and Subudhi, Badri Narayan and Veerakumar, T. and Chaudhury, Santanu},
  year = 2020,
  month = oct,
  pages = {1--5},
  publisher = {IEEE},
  address = {Biloxi, MS, USA},
  doi = {10.1109/IEEECONF38699.2020.9389401},
  urldate = {2024-10-19},
  abstract = {Tracking of a fish or some specific fishes in a school of fish is quite a challenging task. This could help in understanding the behavior of a fish or a small group of fish in a crowd of different varieties of fishes. In this paper we propose a technique to detect prominent objects among a large group of fishes. The problem is formulated with a stationary camera setup. The moving objects are initially detected by a spatio-contextual Gaussian mixture model based background subtraction method. Further, all the detected objects are analyzed to determine a predefined number of the most prominent objects in the scene of view. To characterize the objects we have employed a dual-feature framework, which includes color and texture features. The overall feature strength is computed by combining the two featurestrengths in an adaptive way so that, the color gets more weight if color degradation is less otherwise texture gets more weight. This weight is adaptively computed with the prior information of color degradation phenomena in underwater environment. The proposed technique is tested with a large number of underwater videos and found to perform satisfactorily.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-7281-5446-6},
  langid = {english},
  file = {/Users/dan/Zotero/storage/7KL28E57/Rout et al. - 2020 - Prominent Object Detection in Underwater Environme.pdf}
}

@misc{royWilDectYOLOEfficientRobust2022,
  title = {{{WilDect-YOLO}}: {{An}} Efficient and Robust Computer Vision-Based Accurate Object Localization Model for Automated Endangered Wildlife Detection},
  author = {Roy, Arunabha M. and Bhaduri, Jayabrata and Kumar, Teerath and Raj, Kislay},
  year = 2022,
  month = nov,
  volume = {75},
  pages = {101919}
}

@misc{royWilDectYOLOEfficientRobust2022a,
  title = {{{WilDect-YOLO}}: {{An}} Efficient and Robust Computer Vision-Based Accurate Object Localization Model for Automated Endangered Wildlife Detection},
  author = {Roy, Arunabha M. and Bhaduri, Jayabrata and Kumar, Teerath and Raj, Kislay},
  year = 2022,
  month = nov,
  journal = {Elsevier BV},
  volume = {75},
  pages = {101919--101919},
  doi = {10.1016/j.ecoinf.2022.101919}
}

@article{royWilDectYOLOEfficientRobust2023,
  title = {{{WilDect-YOLO}}: {{An}} Efficient and Robust Computer Vision-Based Accurate Object Localization Model for Automated Endangered Wildlife Detection},
  shorttitle = {{{WilDect-YOLO}}},
  author = {Roy, Arunabha M. and Bhaduri, Jayabrata and Kumar, Teerath and Raj, Kislay},
  year = 2023,
  month = jul,
  journal = {Ecological Informatics},
  volume = {75},
  pages = {101919},
  issn = {1574-9541},
  doi = {10.1016/j.ecoinf.2022.101919},
  urldate = {2024-07-31},
  abstract = {Objective. With climatic instability, various ecological disturbances, and human actions threaten the existence of various endangered wildlife species. Therefore, an up-to-date accurate and detailed detection process plays an important role in protecting biodiversity losses, conservation, and ecosystem management. Current state-of-the-art wildlife detection models, however, often lack superior feature extraction capability in complex environments, limiting the development of accurate and reliable detection models. Method. To this end, we present WilDect-YOLO, a deep learning (DL)-based automated high-performance detection model for real-time endangered wildlife detection. In the model, we introduce a residual block in the CSPDarknet53 backbone for strong and discriminating deep spatial features extraction and integrate DenseNet blocks to improve in preserving critical feature information. To enhance receptive field representation, preserve fine-grain localized information, and improve feature fusion, a Spatial Pyramid Pooling (SPP) and modified Path Aggregation Network (PANet) have been implemented that results in superior detection under various challenging environments. Results. Evaluating the model performance in a custom endangered wildlife dataset considering high variability and complex backgrounds, WilDect-YOLO obtains a mean average precision (mAP) value of 96.89\%, F1-score of 97.87\%, and precision value of 97.18\% at a detection rate of 59.20 FPS outperforming current state-of-the-art models. Significance. The present research provides an effective and efficient detection framework addressing the shortcoming of existing DL-based wildlife detection models by providing highly accurate species-level localized bounding box prediction. Current work constitutes a step toward a non-invasive, fully automated animal observation system in real-time in-field applications.},
  keywords = {Computer vision,Deep learning (DL),Endangered wildlife detection,Object detection (OD),Wildlife preservation,You only look once (YOLOv4) algorithm},
  file = {/Users/dan/Zotero/storage/JWEHKJ2D/S1574954122003697.html}
}

@article{royWilDectYOLOEfficientRobust2023a,
  title = {{{WilDect-YOLO}}: {{An}} Efficient and Robust Computer Vision-Based Accurate Object Localization Model for Automated Endangered Wildlife Detection},
  shorttitle = {{{WilDect-YOLO}}},
  author = {Roy, Arunabha M. and Bhaduri, Jayabrata and Kumar, Teerath and Raj, Kislay},
  year = 2023,
  month = jul,
  journal = {Ecological Informatics},
  volume = {75},
  pages = {101919},
  issn = {1574-9541},
  doi = {10.1016/j.ecoinf.2022.101919},
  urldate = {2024-07-31},
  abstract = {Objective. With climatic instability, various ecological disturbances, and human actions threaten the existence of various endangered wildlife species. Therefore, an up-to-date accurate and detailed detection process plays an important role in protecting biodiversity losses, conservation, and ecosystem management. Current state-of-the-art wildlife detection models, however, often lack superior feature extraction capability in complex environments, limiting the development of accurate and reliable detection models. Method. To this end, we present WilDect-YOLO, a deep learning (DL)-based automated high-performance detection model for real-time endangered wildlife detection. In the model, we introduce a residual block in the CSPDarknet53 backbone for strong and discriminating deep spatial features extraction and integrate DenseNet blocks to improve in preserving critical feature information. To enhance receptive field representation, preserve fine-grain localized information, and improve feature fusion, a Spatial Pyramid Pooling (SPP) and modified Path Aggregation Network (PANet) have been implemented that results in superior detection under various challenging environments. Results. Evaluating the model performance in a custom endangered wildlife dataset considering high variability and complex backgrounds, WilDect-YOLO obtains a mean average precision (mAP) value of 96.89\%, F1-score of 97.87\%, and precision value of 97.18\% at a detection rate of 59.20 FPS outperforming current state-of-the-art models. Significance. The present research provides an effective and efficient detection framework addressing the shortcoming of existing DL-based wildlife detection models by providing highly accurate species-level localized bounding box prediction. Current work constitutes a step toward a non-invasive, fully automated animal observation system in real-time in-field applications.},
  keywords = {Computer vision,Deep learning (DL),Endangered wildlife detection,Object detection (OD),Wildlife preservation,You only look once (YOLOv4) algorithm},
  file = {/Users/dan/Zotero/storage/NEVZF3GN/S1574954122003697.html}
}

@article{royWilDectYOLOEfficientRobust2023b,
  title = {{{WilDect-YOLO}}: {{An}} Efficient and Robust Computer Vision-Based Accurate Object Localization Model for Automated Endangered Wildlife Detection},
  shorttitle = {{{WilDect-YOLO}}},
  author = {Roy, Arunabha M. and Bhaduri, Jayabrata and Kumar, Teerath and Raj, Kislay},
  year = 2023,
  month = jul,
  journal = {Ecological Informatics},
  volume = {75},
  pages = {101919},
  issn = {15749541},
  doi = {10.1016/j.ecoinf.2022.101919},
  urldate = {2024-07-31},
  abstract = {Objective. With climatic instability, various ecological disturbances, and human actions threaten the existence of various endangered wildlife species. Therefore, an up-to-date accurate and detailed detection process plays an important role in protecting biodiversity losses, conservation, and ecosystem management. Current state-of-theart wildlife detection models, however, often lack superior feature extraction capability in complex environ\- ments, limiting the development of accurate and reliable detection models. Method. To this end, we present WilDect-YOLO, a deep learning (DL)-based automated high-performance detection model for real-time endan\- gered wildlife detection. In the model, we introduce a residual block in the CSPDarknet53 backbone for strong and discriminating deep spatial features extraction and integrate DenseNet blocks to improve in preserving critical feature information. To enhance receptive field representation, preserve fine-grain localized information, and improve feature fusion, a Spatial Pyramid Pooling (SPP) and modified Path Aggregation Network (PANet) have been implemented that results in superior detection under various challenging environments. Results. Evaluating the model performance in a custom endangered wildlife dataset considering high variability and complex backgrounds, WilDect-YOLO obtains a mean average precision (mAP) value of 96.89\%, F1-score of 97.87\%, and precision value of 97.18\% at a detection rate of 59.20 FPS outperforming current state-of-the-art models. Significance. The present research provides an effective and efficient detection framework addressing the shortcoming of existing DL-based wildlife detection models by providing highly accurate species-level localized bounding box prediction. Current work constitutes a step toward a non-invasive, fully automated an\- imal observation system in real-time in-field applications.},
  langid = {english},
  file = {/Users/dan/Downloads/1-s2.0-S1574954122003697-main.pdf}
}

@article{russakovsky2015a,
  title = {{{ImageNet}} Large Scale Visual Recognition Challenge},
  author = {Russakovsky, O.},
  year = 2015,
  journal = {International Journal of Computer Vision},
  volume = {115},
  number = {3},
  pages = {211--252},
  citation-number = {33},
  langid = {english}
}

@article{russakovsky2015a,
  title = {{{ImageNet}} Large Scale Visual Recognition Challenge},
  author = {Russakovsky, O.},
  year = 2015,
  journal = {International Journal of Computer Vision},
  volume = {115},
  number = {3},
  pages = {211--252},
  citation-number = {33},
  langid = {english}
}

@article{russakovsky2015a,
  title = {{{ImageNet}} Large Scale Visual Recognition Challenge},
  author = {Russakovsky, O.},
  year = 2015,
  journal = {International Journal of Computer Vision},
  volume = {115},
  number = {3},
  pages = {211--252},
  citation-number = {33},
  langid = {english}
}

@misc{sainiPeekHiddenLayers2018,
  title = {A {{Peek Into}} the {{Hidden Layers}} of a {{Convolutional Neural Network Through}} a {{Factorization Lens}}},
  author = {Saini, Uday Singh and Papalexakis, Evangelos E.},
  year = 2018,
  month = jun,
  number = {arXiv:1806.02012},
  eprint = {1806.02012},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-09-23},
  abstract = {Despite their increasing popularity and success in a variety of supervised learning problems, deep neural networks are extremely hard to interpret and debug: Given an already trained deep neural network, and a set of test inputs, how can we gain insight into how those inputs interact with different layers of the neural network? Furthermore, can we characterize a given deep neural network based on its observed behavior on different inputs? In this paper, we propose a novel factorization-based approach on understanding how different deep neural networks operate. In our preliminary results, we identify fascinating patterns that link the factorization rank (typically used as a measure of interestingness in unsupervised data analysis) with how well or poorly the deep network has been trained. Finally, our proposed approach can help provide visual insights on how high-level, interpretable patterns of the network's input behave inside the hidden layers of the deep network.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/dan/Zotero/storage/SEZ6QPQ6/Saini and Papalexakis - 2018 - A Peek Into the Hidden Layers of a Convolutional N.pdf}
}

@misc{sainiPeekHiddenLayers2018a,
  title = {A {{Peek Into}} the {{Hidden Layers}} of a {{Convolutional Neural Network Through}} a {{Factorization Lens}}},
  author = {Saini, Uday Singh and Papalexakis, Evangelos E.},
  year = 2018,
  month = jan
}

@misc{sainiPeekHiddenLayers2018b,
  title = {A {{Peek Into}} the {{Hidden Layers}} of a {{Convolutional Neural Network Through}} a {{Factorization Lens}}},
  author = {Saini, Uday Singh and Papalexakis, Evangelos E.},
  year = 2018,
  month = jan,
  journal = {Cornell University},
  doi = {10.48550/arxiv.1806.02012}
}

@article{salehRealisticFishhabitatDataset2020,
  title = {A Realistic Fish-Habitat Dataset to Evaluate Algorithms for Underwater Visual Analysis},
  author = {Saleh, Alzayat and Laradji, Issam H. and Konovalov, Dmitry A. and Bradley, Michael and Vazquez, David and Sheaves, Marcus},
  year = 2020,
  month = sep,
  journal = {Sci Rep},
  volume = {10},
  number = {1},
  pages = {14671},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-71639-x},
  urldate = {2024-04-22},
  abstract = {Visual analysis of complex fish habitats is an important step towards sustainable fisheries for human consumption and environmental protection. Deep Learning methods have shown great promise for scene analysis when trained on large-scale datasets. However, current datasets for fish analysis tend to focus on the classification task within constrained, plain environments which do not capture the complexity of underwater fish habitats. To address this limitation, we present DeepFish as a benchmark suite with a large-scale dataset to train and test methods for several computer vision tasks. The dataset consists of approximately 40 thousand images collected underwater from 20 habitats in the marine-environments of tropical Australia. The dataset originally contained only classification labels. Thus, we collected point-level and segmentation labels to have a more comprehensive fish analysis benchmark. These labels enable models to learn to automatically monitor fish count, identify their locations, and estimate their sizes. Our experiments provide an in-depth analysis of the dataset characteristics, and the performance evaluation of several state-of-the-art approaches based on our benchmark. Although models pre-trained on ImageNet have successfully performed on this benchmark, there is still room for improvement. Therefore, this benchmark serves as a testbed to motivate further development in this challenging domain of underwater computer vision.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Animal behaviour,Behavioural ecology,Computer science,Information technology},
  file = {/Users/dan/Zotero/storage/4EDL6D88/Saleh et al. - 2020 - A realistic fish-habitat dataset to evaluate algor.pdf}
}

@misc{salehRealisticFishhabitatDataset2020a,
  title = {A Realistic Fish-Habitat Dataset to Evaluate Algorithms for Underwater Visual Analysis},
  author = {Saleh, Alzayat and Laradji, Issam and Konovalov, Dmitry A. and Bradley, Michael and V{\'a}zquez, David and Sheaves, Marcus},
  year = 2020,
  month = sep,
  volume = {10},
  number = {1},
  pages = {14671}
}

@misc{salehRealisticFishhabitatDataset2020b,
  title = {A Realistic Fish-habitat Dataset to Evaluate Algorithms for Underwater Visual Analysis},
  author = {Saleh, Alzayat and Laradji, Issam H. and Konovalov, Dmitry A. and Bradley, Michael and Vazquez, David and Sheaves, Marcus},
  year = 2020,
  month = oct
}

@article{salman2016a,
  title = {Fish Species Classification in Unconstrained Underwater Environments Based on Deep Learning},
  author = {Salman, A. and Jalal, A. and Shafait, F. and Mian, A. and Shortis, M.R. and Culverhouse, P.F.},
  year = 2016,
  journal = {Limnology and Oceanography: Methods},
  volume = {14},
  number = {9},
  pages = {570--585},
  citation-number = {14},
  langid = {english}
}

@inproceedings{sanilaUnderwaterImageEnhancement2019,
  title = {Underwater {{Image Enhancement Using White Balance}}, {{USM}} and {{CLHE}}},
  booktitle = {2019 {{International Symposium}} on {{Ocean Technology}} ({{SYMPOL}})},
  author = {Sanila, K. H. and Balakrishnan, Arun A. and Supriya, M. H.},
  year = 2019,
  month = dec,
  pages = {106--116},
  publisher = {IEEE},
  address = {Ernakulam, India},
  doi = {10.1109/SYMPOL48207.2019.9005301},
  urldate = {2024-10-08},
  abstract = {Enhancement of the underwater images is essential because of the poor illumination, dispersion and scattering losses of the environment. This paper proposes a luminosity conserving and contrast enhancing method for enhancement of the underwater images. In the proposed method, initially, the images are subjected to white balance in order to remove the unwanted colour cast. A modified approach adopted from Gray World algorithm is used for colour correction. The processed image is subsequently subjected to unsharp masking and contrast limited histogram equalization to ensure the enhancement of edges and contrast respectively. The experimental results demonstrate that the proposed method can enhance the images.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-7281-5326-1},
  langid = {english},
  file = {/Users/dan/Zotero/storage/WRVAV2HY/Sanila et al. - 2019 - Underwater Image Enhancement Using White Balance, .pdf}
}

@inproceedings{saqibPersonHeadDetection2018,
  title = {Person {{Head Detection}} in {{Multiple Scales Using Deep Convolutional Neural Networks}}},
  booktitle = {2018 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Saqib, Muhammad and Khan, Sultan Daud and Sharma, Nabin and Blumenstein, Michael},
  year = 2018,
  month = jul,
  pages = {1--7},
  publisher = {IEEE},
  address = {Rio de Janeiro},
  doi = {10.1109/IJCNN.2018.8489367},
  urldate = {2024-10-20},
  isbn = {978-1-5090-6014-6},
  file = {/Users/dan/Zotero/storage/I54BQAUN/Saqib et al. - 2018 - Person Head Detection in Multiple Scales Using Dee.pdf}
}

@article{satyanarayanan2017a,
  title = {The Emergence of Edge Computing},
  author = {Satyanarayanan, M.},
  year = 2017,
  journal = {Computer},
  volume = {50},
  number = {1},
  pages = {30--39},
  citation-number = {29},
  langid = {english}
}

@article{satyanarayanan2017a,
  title = {The Emergence of Edge Computing},
  author = {Satyanarayanan, M.},
  year = 2017,
  journal = {Computer},
  volume = {50},
  number = {1},
  pages = {30--39},
  citation-number = {54},
  langid = {english}
}

@article{satyanarayanan2017a,
  title = {The Emergence of Edge Computing},
  author = {Satyanarayanan, M.},
  year = 2017,
  journal = {Computer},
  volume = {50},
  number = {1},
  pages = {30--39},
  citation-number = {54},
  langid = {english}
}

@article{satyanarayanan2017a,
  title = {The Emergence of Edge Computing},
  author = {Satyanarayanan, M.},
  year = 2017,
  journal = {Computer},
  volume = {50},
  number = {1},
  pages = {30--39},
  citation-number = {54},
  langid = {english}
}

@article{schettini2010a,
  title = {Underwater Image Processing: {{State}} of the Art of Restoration and Image Enhancement Methods},
  author = {Schettini, R. and Corchs, S.},
  year = 2010,
  journal = {EURASIP Journal on Advances in Signal Processing},
  pages = {1--14},
  citation-number = {15},
  langid = {english}
}

@article{schettiniUnderwaterImageProcessing2010,
  title = {Underwater {{Image Processing}}: {{State}} of the {{Art}} of {{Restoration}} and {{Image Enhancement Methods}}},
  shorttitle = {Underwater {{Image Processing}}},
  author = {Schettini, Raimondo and Corchs, Silvia},
  year = 2010,
  month = dec,
  journal = {EURASIP J. Adv. Signal Process.},
  volume = {2010},
  number = {1},
  pages = {746052},
  issn = {1687-6180},
  doi = {10.1155/2010/746052},
  urldate = {2024-10-19},
  langid = {english},
  file = {/Users/dan/Zotero/storage/EQE69LCM/Schettini and Corchs - 2010 - Underwater Image Processing State of the Art of R.pdf}
}

@inproceedings{sharmaYOLOrsliteLightweightCNN2021,
  title = {{{YOLOrs-lite}}: {{A Lightweight CNN For Real-Time Object Detection}} in {{Remote-Sensing}}},
  shorttitle = {{{YOLOrs-lite}}},
  booktitle = {2021 {{IEEE International Geoscience}} and {{Remote Sensing Symposium IGARSS}}},
  author = {Sharma, Manish and Markopoulos, Panos P. and Saber, Eli},
  year = 2021,
  month = jul,
  pages = {2604--2607},
  publisher = {IEEE},
  address = {Brussels, Belgium},
  doi = {10.1109/IGARSS47720.2021.9554418},
  urldate = {2024-04-23},
  abstract = {Detection CNN architectures often exhibit over-parameterization which results in excessive computational and storage overhead, but also undesired overfitting and reduced performance. In this work we focus on YOLOrs, a state-of-theart CNN for target detection in remote sensing imagery, and counteract over-parameterization by enforcing Tensor-Train (TT) structure to its convolutional kernels. While TT has been successfully used before for compressing classification CNNs, this work is the first one that uses it to compress a detection CNN. We refer to the resulting network as YOLOrslite and compare its performance against standard YOLOrs as well as other state-of-the-art detection networks. Our numerical studies show that the proposed network attains superior detection performance, with storage savings as high as 70\%. The proposed network combines light storage with real-time inference, making it quite promising for edge deployment.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-6654-0369-6},
  langid = {english},
  file = {/Users/dan/Zotero/storage/XINUBYGQ/Sharma et al. - 2021 - YOLOrs-lite A Lightweight CNN For Real-Time Objec.pdf}
}

@misc{sharmaYOLOrsliteLightweightCNN2021a,
  title = {{{YOLOrs-lite}}: {{A Lightweight CNN For Real-Time Object Detection}} in {{Remote-Sensing}}},
  author = {Sharma, Manish and Markopoulos, Panos P. and Tekalp, A. Murat},
  year = 2021,
  month = jul,
  pages = {2604--2607}
}

@misc{sharmaYOLOrsliteLightweightCNN2021b,
  title = {{{YOLOrs-lite}}: {{A Lightweight CNN For Real-Time Object Detection}} in {{Remote-Sensing}}},
  author = {Sharma, Manish and Markopoulos, Panos P. and Tekalp, A. Murat},
  year = 2021,
  month = jul,
  doi = {10.1109/igarss47720.2021.9554418}
}

@misc{sharmaYOLOrsObjectDetection2020,
  title = {{{YOLOrs}}: {{Object Detection}} in {{Multimodal Remote Sensing Imagery}}},
  author = {Sharma, Manish and Dhanaraj, Mayur and Karnam, Srivallabha and Chachlakis, Dimitris G. and Ptucha, Raymond and Markopoulos, Panos P. and Tekalp, A. Murat},
  year = 2020,
  month = nov,
  volume = {14},
  pages = {1497--1508}
}

@misc{sharmaYOLOrsObjectDetection2020a,
  title = {{{YOLOrs}}: {{Object Detection}} in {{Multimodal Remote Sensing Imagery}}},
  author = {Sharma, Manish and Dhanaraj, Mayur and Karnam, Srivallabha and Chachlakis, Dimitris G. and Ptucha, Raymond and Markopoulos, Panos P. and Tekalp, A. Murat},
  year = 2020,
  month = nov,
  journal = {Institute of Electrical and Electronics Engineers},
  volume = {14},
  pages = {1497--1508},
  doi = {10.1109/jstars.2020.3041316}
}

@article{sharmaYOLOrsObjectDetection2021,
  title = {{{YOLOrs}}: {{Object Detection}} in {{Multimodal Remote Sensing Imagery}}},
  shorttitle = {{{YOLOrs}}},
  author = {Sharma, Manish and Dhanaraj, Mayur and Karnam, Srivallabha and Chachlakis, Dimitris G. and Ptucha, Raymond and Markopoulos, Panos P. and Saber, Eli},
  year = 2021,
  journal = {IEEE J. Sel. Top. Appl. Earth Observations Remote Sensing},
  volume = {14},
  pages = {1497--1508},
  issn = {1939-1404, 2151-1535},
  doi = {10.1109/JSTARS.2020.3041316},
  urldate = {2024-04-23},
  abstract = {Deep-learning object detection methods that are designed for computer vision applications tend to underperform when applied to remote sensing data. This is because contrary to computer vision, in remote sensing, training data are harder to collect and targets can be very small, occupying only a few pixels in the entire image, and exhibit arbitrary perspective transformations. Detection performance can improve by fusing data from multiple remote sensing modalities, including red, green, blue, infrared, hyperspectral, multispectral, synthetic aperture radar, and light detection and ranging, to name a few. In this article, we propose YOLOrs: a new convolutional neural network, specifically designed for real-time object detection in multimodal remote sensing imagery. YOLOrs can detect objects at multiple scales, with smaller receptive fields to account for small targets, as well as predict target orientations. In addition, YOLOrs introduces a novel mid-level fusion architecture that renders it applicable to multimodal aerial imagery. Our experimental studies compare YOLOrs with contemporary alternatives and corroborate its merits.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {/Users/dan/Zotero/storage/DQLGRE7R/Sharma et al. - 2021 - YOLOrs Object Detection in Multimodal Remote Sens.pdf}
}

@article{shi2016a,
  title = {Edge Computing: {{Vision}} and Challenges},
  author = {Shi, W. and Cao, J. and Zhang, Q. and Li, Y. and Xu, L.},
  year = 2016,
  journal = {IEEE Internet of Things Journal},
  volume = {3},
  number = {5},
  pages = {637--646},
  citation-number = {30},
  langid = {english}
}

@article{shi2016a,
  title = {Edge Computing: {{Vision}} and Challenges},
  author = {Shi, W.},
  year = 2016,
  journal = {IEEE Internet of Things Journal},
  volume = {3},
  number = {5},
  pages = {637--646},
  citation-number = {55},
  langid = {english}
}

@article{shi2016a,
  title = {Edge Computing: {{Vision}} and Challenges},
  author = {Shi, W.},
  year = 2016,
  journal = {IEEE Internet of Things Journal},
  volume = {3},
  number = {5},
  pages = {637--646},
  citation-number = {55},
  langid = {english}
}

@article{shi2016a,
  title = {Edge Computing: {{Vision}} and Challenges},
  author = {Shi, W.},
  year = 2016,
  journal = {IEEE Internet of Things Journal},
  volume = {3},
  number = {5},
  pages = {637--646},
  citation-number = {55},
  langid = {english}
}

@incollection{shihavuddin2013a,
  title = {Automated Detection of Benthic Crustaceans Using Multiple Visual Cues in Underwater Color Images},
  booktitle = {{{OCEANS}} 2013 {{MTS}}/{{IEEE}} San Diego},
  author = {Shihavuddin, A. and Gracias, N. and Garcia, R. and Gleason, A. and Gintert, B. and Eustice, R.},
  year = 2013,
  pages = {1--9},
  publisher = {IEEE},
  citation-number = {25},
  langid = {english}
}

@inproceedings{shihavuddin2013a,
  title = {Automatic Annotation for Marine Biology: {{Fish}} Detection and Species Recognition},
  booktitle = {2013 {{IEEE}} International Conference on Image Processing},
  author = {Shihavuddin, A.S.M.},
  year = 2013,
  pages = {5052--5056},
  publisher = {IEEE},
  citation-number = {5},
  langid = {english}
}

@inproceedings{shihavuddin2013a,
  title = {Automatic Annotation for Marine Biology: {{Fish}} Detection and Species Recognition},
  booktitle = {2013 {{IEEE}} International Conference on Image Processing},
  author = {Shihavuddin, A.S.M.},
  year = 2013,
  pages = {5052--5056},
  publisher = {IEEE},
  citation-number = {5},
  langid = {english}
}

@inproceedings{shihavuddin2013a,
  title = {Automatic Annotation for Marine Biology: {{Fish}} Detection and Species Recognition},
  booktitle = {2013 {{IEEE}} International Conference on Image Processing},
  author = {Shihavuddin, A.S.M.},
  year = 2013,
  pages = {5052--5056},
  publisher = {IEEE},
  citation-number = {5},
  langid = {english}
}

@inproceedings{shiraiFundamentalStudyReal2000,
  title = {Fundamental Study on Real Time Measurement of Altitude Data with Accelerometer and Vehicle Speed Sensor},
  booktitle = {International {{Archives}} of the {{Photogrammetry}}, {{Remote Sensing}} and {{Spatial Information Sciences}} - {{ISPRS Archives}}},
  author = {Shirai, Y. and Koizumi, T. and Takemoto, A. and Adachi, I.},
  year = 2000,
  volume = {33},
  pages = {301--306},
  note = {Export Date: 17 July 2024; Cited By: 1}
}

@article{shorten2019a,
  title = {A Survey on Image Data Augmentation for Deep Learning},
  author = {Shorten, C. and Khoshgoftaar, T.M.},
  year = 2019,
  journal = {Journal of Big Data},
  volume = {6},
  number = {1},
  pages = {60},
  citation-number = {8},
  langid = {english}
}

@article{shorten2019a,
  title = {A Survey on Image Data Augmentation for Deep Learning},
  author = {Shorten, C. and Khoshgoftaar, T.M.},
  year = 2019,
  journal = {Journal of Big Data},
  volume = {6},
  number = {1},
  pages = {60},
  citation-number = {8},
  langid = {english}
}

@article{shorten2019a,
  title = {A Survey on Image Data Augmentation for Deep Learning},
  author = {Shorten, C. and Khoshgoftaar, T.M.},
  year = 2019,
  journal = {Journal of Big Data},
  volume = {6},
  number = {1},
  pages = {60},
  citation-number = {8},
  langid = {english}
}

@article{singh2004a,
  title = {Advances in Large-Area Photographic and Optical Mosaic Techniques for Underwater Surveying},
  author = {Singh, H. and Howland, J. and Pizarro, O.},
  year = 2004,
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {29},
  number = {3},
  pages = {872--886},
  citation-number = {11},
  langid = {english}
}

@incollection{singhUAVBasedTerrainFollowingMapping2023,
  title = {{{UAV-Based Terrain-Following Mapping Using LiDAR}} in {{High Undulating Catastrophic Areas}}},
  booktitle = {Proceedings of {{UASG}} 2021: {{Wings}} 4 {{Sustainability}}},
  author = {Singh, Chandra Has and Jain, Kamal and Mishra, Vishal},
  editor = {Jain, Kamal and Mishra, Vishal and Pradhan, Biswajeet},
  year = 2023,
  volume = {304},
  pages = {21--37},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-19309-5_3},
  urldate = {2024-06-27},
  isbn = {978-3-031-19308-8 978-3-031-19309-5},
  langid = {english}
}

@article{songReviewArtificialIntelligence2023,
  title = {A Review of Artificial Intelligence in Marine Science},
  author = {Song, Tao and Pang, Cong and Hou, Boyang and Xu, Guangxu and Xue, Junyu and Sun, Handan and Meng, Fan},
  year = 2023,
  month = feb,
  journal = {Front. Earth Sci.},
  volume = {11},
  pages = {1090185},
  issn = {2296-6463},
  doi = {10.3389/feart.2023.1090185},
  urldate = {2024-12-09},
  abstract = {Utilization and exploitation of marine resources by humans have contributed to the growth of marine research. As technology progresses, artificial intelligence (AI) approaches are progressively being applied to maritime research, complementing traditional marine forecasting models and observation techniques to some degree. This article takes the artificial intelligence algorithmic model as its starting point, references several application trials, and methodically elaborates on the emerging research trend of mixing machine learning and physical modeling concepts. This article discusses the evolution of methodologies for the building of ocean observations, the application of artificial intelligence to remote sensing satellites, smart sensors, and intelligent underwater robots, and the construction of ocean big data. We also cover the method of identifying internal waves (IW), heatwaves, El Ni\~no-Southern Oscillation (ENSO), and sea ice using artificial intelligence algorithms. In addition, we analyze the applications of artificial intelligence models in the prediction of ocean components, including physics-driven numerical models, model-driven statistical models, traditional machine learning models, data-driven deep learning models, and physical models combined with artificial intelligence models. This review shows the growth routes of the application of artificial intelligence in ocean observation, ocean phenomena identification, and ocean elements forecasting, with examples and forecasts of their future development trends from several angles and points of view, by categorizing the various uses of artificial intelligence in the ocean sector.},
  langid = {english},
  file = {/Users/dan/Downloads/feart-11-1090185.pdf}
}

@article{stevens2009a,
  title = {Animal Camouflage: {{Current}} Issues and New Perspectives},
  author = {Stevens, M. and Merilaita, S.},
  year = 2009,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {364},
  number = {1516},
  pages = {423--427},
  citation-number = {24},
  langid = {english}
}

@article{stojanovic2009a,
  title = {Underwater Acoustic Communication Channels: {{Propagation}} Models and Statistical Characterization},
  author = {Stojanovic, M. and Preisig, J.},
  year = 2009,
  journal = {IEEE Communications Magazine},
  volume = {47},
  number = {1},
  pages = {84--89},
  citation-number = {57},
  langid = {english}
}

@article{stojanovic2009a,
  title = {Underwater Acoustic Communication Channels: {{Propagation}} Models and Statistical Characterization},
  author = {Stojanovic, M. and Preisig, J.},
  year = 2009,
  journal = {IEEE Communications Magazine},
  volume = {47},
  number = {1},
  pages = {84--89},
  citation-number = {57},
  langid = {english}
}

@article{stojanovic2009a,
  title = {Underwater Acoustic Communication Channels: {{Propagation}} Models and Statistical Characterization},
  author = {Stojanovic, M. and Preisig, J.},
  year = 2009,
  journal = {IEEE Communications Magazine},
  volume = {47},
  number = {1},
  pages = {84--89},
  citation-number = {57},
  langid = {english}
}

@article{tervenComprehensiveReviewYOLO2023,
  title = {A {{Comprehensive Review}} of {{YOLO Architectures}} in {{Computer Vision}}: {{From YOLOv1}} to {{YOLOv8}} and {{YOLO-NAS}}},
  shorttitle = {A {{Comprehensive Review}} of {{YOLO Architectures}} in {{Computer Vision}}},
  author = {Terven, Juan and {Cordova-Esparza}, Diana},
  year = 2023,
  month = nov,
  journal = {MAKE},
  volume = {5},
  number = {4},
  eprint = {2304.00501},
  primaryclass = {cs},
  pages = {1680--1716},
  issn = {2504-4990},
  doi = {10.3390/make5040083},
  urldate = {2024-09-26},
  abstract = {YOLO has become a central real-time object detection system for robotics, driverless cars, and video monitoring applications. We present a comprehensive analysis of YOLO's evolution, examining the innovations and contributions in each iteration from the original YOLO up to YOLOv8, YOLO-NAS, and YOLO with Transformers. We start by describing the standard metrics and postprocessing; then, we discuss the major changes in network architecture and training tricks for each model. Finally, we summarize the essential lessons from YOLO's development and provide a perspective on its future, highlighting potential research directions to enhance real-time object detection systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,I.2.10},
  note = {Comment: 36 pages, 21 figures, 4 tables, published in Machine Learning and Knowledge Extraction. This version contains the last changes made to the published version},
  file = {/Users/dan/Zotero/storage/VYUXHUJC/Terven and Cordova-Esparza - 2023 - A Comprehensive Review of YOLO Architectures in Co.pdf}
}

@inproceedings{tzeng2017a,
  title = {Adversarial Discriminative Domain Adaptation},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Tzeng, E.},
  year = 2017,
  pages = {7167--7176},
  citation-number = {37},
  langid = {english}
}

@inproceedings{tzeng2017a,
  title = {Adversarial Discriminative Domain Adaptation},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Tzeng, E.},
  year = 2017,
  pages = {7167--7176},
  citation-number = {37},
  langid = {english}
}

@inproceedings{tzeng2017a,
  title = {Adversarial Discriminative Domain Adaptation},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Tzeng, E.},
  year = 2017,
  pages = {7167--7176},
  citation-number = {37},
  langid = {english}
}

@misc{ulyanovInstanceNormalizationMissing2017,
  title = {Instance {{Normalization}}: {{The Missing Ingredient}} for {{Fast Stylization}}},
  shorttitle = {Instance {{Normalization}}},
  author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  year = 2017,
  month = nov,
  number = {arXiv:1607.08022},
  eprint = {1607.08022},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-19},
  abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code is available at https://github.com/DmitryUlyanov/texture\_nets. Full paper can be found at https://arxiv.org/abs/1701.02096.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dan/Zotero/storage/LAB48VBG/Ulyanov et al. - 2017 - Instance Normalization The Missing Ingredient for.pdf}
}

@article{vareyPredictingSentinelNode2020,
  title = {Predicting {{Sentinel Node Status}} in {{Patients With Melanoma}}: {{Does Gene Expression Profiling Improve Accuracy}}?},
  shorttitle = {Predicting {{Sentinel Node Status}} in {{Patients With Melanoma}}},
  author = {Varey, Alexander H. R. and Lo, Serigne N. and Scolyer, Richard A. and Thompson, John F.},
  year = 2020,
  month = dec,
  journal = {JCO Precis Oncol},
  number = {4},
  pages = {990--991},
  publisher = {Wolters Kluwer},
  doi = {10.1200/PO.20.00160},
  urldate = {2024-04-19}
}

@article{veerappanFishCountingUnderwater2023,
  title = {Fish Counting through Underwater Fish Detection Using Deep Learning Techniques},
  author = {Veerappan, Sundari and Sella Veluswami, Jansi Rani},
  year = 2023,
  month = dec,
  journal = {RRIA},
  volume = {33},
  number = {4},
  pages = {69--80},
  issn = {12201758, 18414303},
  doi = {10.33436/v33i4y202306},
  urldate = {2024-04-22},
  file = {/Users/dan/Zotero/storage/8TRSAK6G/Veerappan and Sella Veluswami - 2023 - Fish counting through underwater fish detection us.pdf}
}

@misc{veerappanFishCountingUnderwater2023a,
  title = {Fish Counting through Underwater Fish Detection Using Deep Learning Techniques},
  author = {VEERAPPAN, Sundari and VELUSWAMI, Jansi Rani SELLA},
  year = 2023,
  month = dec,
  volume = {33},
  number = {4},
  pages = {69--80}
}

@misc{veerappanFishCountingUnderwater2023b,
  title = {Fish Counting through Underwater Fish Detection Using Deep Learning Techniques},
  author = {VEERAPPAN, Sundari and VELUSWAMI, Jansi Rani SELLA},
  year = 2023,
  month = dec,
  journal = {ICI Publishing House},
  volume = {33},
  number = {4},
  pages = {69--80},
  doi = {10.33436/v33i4y202306}
}

@inproceedings{villon2018a,
  title = {Coral Reef Fish Detection and Recognition in Underwater Videos by Supervised Machine Learning: {{Comparison}} between Deep Learning and {{HOG}}+{{SVM}} Methods},
  booktitle = {2018 {{IEEE}} International Conference on Image Processing},
  author = {Villon, S.},
  year = 2018,
  pages = {3783--3787},
  publisher = {IEEE},
  citation-number = {4},
  langid = {english}
}

@inproceedings{villon2018a,
  title = {Coral Reef Fish Detection and Recognition in Underwater Videos by Supervised Machine Learning: {{Comparison}} between Deep Learning and {{HOG}}+{{SVM}} Methods},
  booktitle = {2018 {{IEEE}} International Conference on Image Processing},
  author = {Villon, S.},
  year = 2018,
  pages = {3783--3787},
  publisher = {IEEE},
  citation-number = {4},
  langid = {english}
}

@inproceedings{villon2018a,
  title = {Coral Reef Fish Detection and Recognition in Underwater Videos by Supervised Machine Learning: {{Comparison}} between Deep Learning and {{HOG}}+{{SVM}} Methods},
  booktitle = {2018 {{IEEE}} International Conference on Image Processing},
  author = {Villon, S.},
  year = 2018,
  pages = {3783--3787},
  publisher = {IEEE},
  citation-number = {4},
  langid = {english}
}

@incollection{villonCoralReefFish2016,
  title = {Coral {{Reef Fish Detection}} and {{Recognition}} in {{Underwater Videos}} by {{Supervised Machine Learning}}: {{Comparison Between Deep Learning}} and {{HOG}}+{{SVM Methods}}},
  shorttitle = {Coral {{Reef Fish Detection}} and {{Recognition}} in {{Underwater Videos}} by {{Supervised Machine Learning}}},
  booktitle = {Advanced {{Concepts}} for {{Intelligent Vision Systems}}},
  author = {Villon, S{\'e}bastien and Chaumont, Marc and Subsol, G{\'e}rard and Vill{\'e}ger, S{\'e}bastien and Claverie, Thomas and Mouillot, David},
  editor = {{Blanc-Talon}, Jacques and Distante, Cosimo and Philips, Wilfried and Popescu, Dan and Scheunders, Paul},
  year = 2016,
  volume = {10016},
  pages = {160--171},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-48680-2_15},
  urldate = {2024-04-23},
  abstract = {In this paper, we present two supervised machine learning methods to automatically detect and recognize coral reef fishes in underwater HD videos. The first method relies on a traditional two-step approach: extraction of HOG features and use of a SVM classifier. The second method is based on Deep Learning. We compare the results of the two methods on real data and discuss their strengths and weaknesses.},
  isbn = {978-3-319-48679-6 978-3-319-48680-2},
  langid = {english},
  file = {/Users/dan/Zotero/storage/BNUWVRVA/Villon et al. - 2016 - Coral Reef Fish Detection and Recognition in Under.pdf}
}

@misc{villonCoralReefFish2016a,
  title = {Coral {{Reef Fish Detection}} and {{Recognition}} in {{Underwater Videos}} by {{Supervised Machine Learning}}: {{Comparison Between Deep Learning}} and {{HOG}}+{{SVM Methods}}},
  author = {Villon, S{\'e}bastien and Chaumont, Marc and Subsol, G{\'e}rard and Vill{\'e}ger, S{\'e}bastien and Claverie, Thomas and Mouillot, David},
  year = 2016,
  month = jan,
  volume = {10016},
  pages = {160--171}
}

@misc{villonCoralReefFish2016b,
  title = {Coral {{Reef Fish Detection}} and {{Recognition}} in {{Underwater Videos}} by {{Supervised Machine Learning}}: {{Comparison Between Deep Learning}} and {{HOG}}+{{SVM Methods}}},
  author = {Villon, S{\'e}bastien and Chaumont, Marc and Subsol, G{\'e}rard and Vill{\'e}ger, S{\'e}bastien and Claverie, Thomas and Mouillot, David},
  year = 2016,
  month = jan,
  journal = {Springer Science+Business Media},
  pages = {160--171},
  doi = {10.1007/978-3-319-48680-2_15}
}

@misc{voAutomaticDataCuration,
  title = {Automatic {{Data Curation}} for {{Self-Supervised Learning}}: {{A Clustering-Based Approach}}},
  author = {Vo, Huy and Khalidov, Vasil and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Smetanin, Nikita and Szafraniec, Marc and Touvron, Hugo and Couprie, Camille and Oquab, Maxime and Joulin, Armand and J{\'e}gou, Herv{\'e} and Labatut, Patrick and Bojanowski, Piotr},
  volume = {Self-supervised features are the cornerstone of modern machine learning systems. They are typically pre-trained on data collections whose construction and curation typically require extensive human effort. This manual process has some limitations similar to those encountered in supervised learning, e.g., the crowd-sourced selection of data is costly and time-consuming, preventing scaling the dataset size. In this work, we consider the problem of automatic curation of high-quality datasets for self-supervised pre-training. We posit that such datasets should be large, diverse and balanced, and propose a clustering-based approach for building ones satisfying all these criteria. Our method involves successive and hierarchical applications of k-means on a large and diverse data repository to obtain clusters that distribute uniformly among data concepts, followed by a hierarchical, balanced sampling step from these clusters. Extensive experiments on three different data domains including web-based images, satellite images and text show that features trained on our automatically curated datasets outperform those trained on uncurated data while being on par or better than ones trained on manually curated data.}
}

@misc{voAutomaticDataCuration2024,
  title = {Automatic {{Data Curation}} for {{Self-Supervised Learning}}: {{A Clustering-Based Approach}}},
  shorttitle = {Automatic {{Data Curation}} for {{Self-Supervised Learning}}},
  author = {Vo, Huy V. and Khalidov, Vasil and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Smetanin, Nikita and Szafraniec, Marc and Touvron, Hugo and Couprie, Camille and Oquab, Maxime and Joulin, Armand and J{\'e}gou, Herv{\'e} and Labatut, Patrick and Bojanowski, Piotr},
  year = 2024,
  month = may,
  number = {arXiv:2405.15613},
  eprint = {2405.15613},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-20},
  abstract = {Self-supervised features are the cornerstone of modern machine learning systems. They are typically pre-trained on data collections whose construction and curation typically require extensive human effort. This manual process has some limitations similar to those encountered in supervised learning, e.g., the crowd-sourced selection of data is costly and time-consuming, preventing scaling the dataset size. In this work, we consider the problem of automatic curation of high-quality datasets for self-supervised pre-training. We posit that such datasets should be large, diverse and balanced, and propose a clustering-based approach for building ones satisfying all these criteria. Our method involves successive and hierarchical applications of \$k\$-means on a large and diverse data repository to obtain clusters that distribute uniformly among data concepts, followed by a hierarchical, balanced sampling step from these clusters. Extensive experiments on three different data domains including web-based images, satellite images and text show that features trained on our automatically curated datasets outperform those trained on uncurated data while being on par or better than ones trained on manually curated data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/dan/Zotero/storage/GI7RAELG/Vo et al. - 2024 - Automatic Data Curation for Self-Supervised Learni.pdf;/Users/dan/Zotero/storage/KIGRZ4YM/2405.html}
}

@misc{voAutomaticDataCuration2024a,
  title = {Automatic {{Data Curation}} for {{Self-Supervised Learning}}: {{A Clustering-Based Approach}}},
  author = {Vo, Huy V. and Khalidov, Vasil and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Smetanin, Nikita and Szafraniec, Marc and Touvron, Hugo and Couprie, Camille and Oquab, Maxime and Joulin, Armand and J{\'e}gou, Herv{\'e} and Labatut, Patrick and Bojanowski, Piotr},
  year = 2024,
  month = may
}

@inproceedings{wang2017a,
  title = {Effective Deep Learning-Based Approach for Underwater Fish Detection},
  booktitle = {2017 {{IEEE}} International Conference on Signal Processing, Communications and Computing},
  author = {Wang, G.},
  year = 2017,
  pages = {1--5},
  publisher = {IEEE},
  citation-number = {6},
  langid = {english}
}

@inproceedings{wang2017a,
  title = {Effective Deep Learning-Based Approach for Underwater Fish Detection},
  booktitle = {2017 {{IEEE}} International Conference on Signal Processing, Communications and Computing},
  author = {Wang, G.},
  year = 2017,
  pages = {1--5},
  publisher = {IEEE},
  citation-number = {6},
  langid = {english}
}

@inproceedings{wang2017a,
  title = {Effective Deep Learning-Based Approach for Underwater Fish Detection},
  booktitle = {2017 {{IEEE}} International Conference on Signal Processing, Communications and Computing},
  author = {Wang, G.},
  year = 2017,
  pages = {1--5},
  publisher = {IEEE},
  citation-number = {6},
  langid = {english}
}

@misc{wangCSPNetNewBackbone2019,
  title = {{{CSPNet}}: {{A New Backbone}} That Can {{Enhance Learning Capability}} of {{CNN}}},
  shorttitle = {{{CSPNet}}},
  author = {Wang, Chien-Yao and Liao, Hong-Yuan Mark and Yeh, I.-Hau and Wu, Yueh-Hua and Chen, Ping-Yang and Hsieh, Jun-Wei},
  year = 2019,
  month = nov,
  number = {arXiv:1911.11929},
  eprint = {1911.11929},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-22},
  abstract = {Neural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection. However, such success greatly relies on costly computation resources, which hinders people with cheap devices from appreciating the advanced technology. In this paper, we propose Cross Stage Partial Network (CSPNet) to mitigate the problem that previous works require heavy inference computations from the network architecture perspective. We attribute the problem to the duplicate gradient information within network optimization. The proposed networks respect the variability of the gradients by integrating feature maps from the beginning and the end of a network stage, which, in our experiments, reduces computations by 20\% with equivalent or even superior accuracy on the ImageNet dataset, and significantly outperforms state-of-the-art approaches in terms of AP50 on the MS COCO object detection dataset. The CSPNet is easy to implement and general enough to cope with architectures based on ResNet, ResNeXt, and DenseNet. Source code is at https://github.com/WongKinYiu/CrossStagePartialNetworks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dan/Zotero/storage/5HMVLZMJ/Wang et al. - 2019 - CSPNet A New Backbone that can Enhance Learning C.pdf}
}

@article{wangExperimentalBasedReviewImage2019,
  title = {An {{Experimental-Based Review}} of {{Image Enhancement}} and {{Image Restoration Methods}} for {{Underwater Imaging}}},
  author = {Wang, Yan and Song, Wei and Fortino, Giancarlo and Qi, Li-Zhe and Zhang, Wenqiang and Liotta, Antonio},
  year = 2019,
  journal = {IEEE Access},
  volume = {7},
  pages = {140233--140251},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2932130},
  urldate = {2024-10-19},
  abstract = {Underwater images play a key role in ocean exploration, but often suffer from severe quality degradation due to light absorption and scattering in water medium. Although major breakthroughs have been made recently in the general area of image enhancement and restoration, the applicability of new methods for improving the quality of underwater images has not specifically been captured. In this paper, we review the image enhancement and restoration methods that tackle typical underwater image impairments, including some extreme degradations and distortions. Firstly, we introduce the key causes of quality reduction in underwater images, in terms of the underwater image formation model (IFM). Then, we review underwater restoration methods, considering both the IFM-free and the IFM-based approaches. Next, we present an experimental-based comparative evaluation of state-of-the-art IFM-free and IFM-based methods, considering also the prior-based parameter estimation algorithms of the IFM-based methods, using both subjective and objective analysis (the used code is freely available at https://github.com/wangyanckxx/Single-UnderwaterImage-Enhancement-and-Color-Restoration). Starting from this study, we pinpoint the key shortcomings of existing methods, drawing recommendations for future research in this area. Our review of underwater image enhancement and restoration provides researchers with the necessary background to appreciate challenges and opportunities in this important field.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {/Users/dan/Zotero/storage/YD62QK28/Wang et al. - 2019 - An Experimental-Based Review of Image Enhancement .pdf}
}

@article{wangPatchStructureRepresentationMethod2015,
  title = {A {{Patch-Structure Representation Method}} for {{Quality Assessment}} of {{Contrast Changed Images}}},
  author = {Wang, Shiqi and Ma, Kede and Yeganeh, Hojatollah and Wang, Zhou and Lin, Weisi},
  year = 2015,
  month = dec,
  journal = {IEEE Signal Process. Lett.},
  volume = {22},
  number = {12},
  pages = {2387--2390},
  issn = {1070-9908, 1558-2361},
  doi = {10.1109/LSP.2015.2487369},
  urldate = {2024-10-12},
  abstract = {Contrast is a fundamental attribute of images that plays an important role in human visual perception of image quality. With numerous approaches proposed to enhance image contrast, much less work has been dedicated to automatic quality assessment of contrast changed images. Existing approaches rely on global statistics to estimate contrast quality. Here we propose a novel local patch-based objective quality assessment method using an adaptive representation of local patch structure, which allows us to decompose any image patch into its mean intensity, signal strength and signal structure components and then evaluate their perceptual distortions in different ways. A unique feature that differentiates the proposed method from previous contrast quality models is the capability to produce a local contrast quality map, which predicts local quality variations over space and may be employed to guide contrast enhancement algorithms. Validations based on four publicly available databases show that the proposed patch-based contrast quality index (PCQI) method provides accurate predictions on the human perception of contrast variations.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/Users/dan/Zotero/storage/3I2LVFXR/Wang et al. - 2015 - A Patch-Structure Representation Method for Qualit.pdf}
}

@misc{wangPatchStructureRepresentationMethod2015a,
  title = {A {{Patch-Structure Representation Method}} for {{Quality Assessment}} of {{Contrast Changed Images}}},
  author = {Wang, Shiqi and Ma, Kede and Yeganeh, Hojatollah and Wang, Zhou and Lin, Weisi},
  year = 2015,
  month = oct,
  journal = {Institute of Electrical and Electronics Engineers},
  volume = {22},
  number = {12},
  pages = {2387--2390},
  doi = {10.1109/lsp.2015.2487369}
}

@misc{wangYOLONanoUnderwater,
  title = {{{YOLO}} Nano Underwater: {{A}} Fast and Compact Object Detector for Embedded Device},
  author = {Wang, Lin and Ye, Xiufen and Xing, Huiming and Wang, Zhengyang and Li, Peng},
  pages = {1--4}
}

@inproceedings{wangYOLONanoUnderwater2020,
  title = {{{YOLO Nano Underwater}}: {{A Fast}} and {{Compact Object Detector}} for {{Embedded Device}}},
  shorttitle = {{{YOLO Nano Underwater}}},
  booktitle = {Global {{Oceans}} 2020: {{Singapore}} -- {{U}}.{{S}}. {{Gulf Coast}}},
  author = {Wang, Lin and Ye, Xiufen and Xing, Huiming and Wang, Zhengyang and Li, Peng},
  year = 2020,
  month = oct,
  pages = {1--4},
  publisher = {IEEE},
  address = {Biloxi, MS, USA},
  doi = {10.1109/IEEECONF38699.2020.9389213},
  urldate = {2024-10-06},
  abstract = {Recent researches on Underwater object detection have progressed with the development of deep learning methods. A large portion of ROVs and AUVs are working on constrained environments with limited power supply and computing capability. In this paper, we propose a fast and compact object detector for several marine products, such as scallop, starfish, echinus and holothurian. The novel proposed model named YOLO Nano Underwater is modified based on YOLO Nano architecture to reduce the inference time. And instance normalization is introduced to replace the batch normalization in some early layers for a precision boost. The model achieves a competitive performance on edge computing devices, like NVIDIA Jetson Nano. Comparing to YOLO v3, our model can get competitive performance in precision, but with only 5\% computation. Meanwhile, we re-annotated the training part of the URPC 2019 dataset and the refined annotations can be public available at https://github.com/wangsssky/Refinedtraining-set-of-URPC2019/.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-7281-5446-6},
  langid = {english},
  file = {/Users/dan/Zotero/storage/GWXQL66W/Wang et al. - 2020 - YOLO Nano Underwater A Fast and Compact Object De.pdf}
}

@misc{wangYOLONanoUnderwater2020a,
  title = {{{YOLO Nano Underwater}}: {{A Fast}} and {{Compact Object Detector}} for {{Embedded Device}}},
  author = {Wang, Lin and Ye, Xiufen and Xing, Huiming and Wang, Zhengyang and Li, Peng},
  year = 2020,
  month = oct,
  pages = {1--4},
  doi = {10.1109/ieeeconf38699.2020.9389213}
}

@misc{wangYOLONanoUnderwater2020b,
  title = {{{YOLO Nano Underwater}}: {{A Fast}} and {{Compact Object Detector}} for {{Embedded Device}}},
  author = {Wang, Lin and Ye, Xiufen and Xing, Huiming and Wang, Zhengyang and Li, Peng},
  year = 2020,
  month = oct,
  pages = {1--4},
  doi = {10.1109/ieeeconf38699.2020.9389213}
}

@misc{wangYOLOv10RealTimeEndEnd2024,
  title = {{{YOLOv10}}: {{Real-Time End-to-End Object Detection}}},
  shorttitle = {{{YOLOv10}}},
  author = {Wang, Ao and Chen, Hui and Liu, Lihao and Chen, Kai and Lin, Zijia and Han, Jungong and Ding, Guiguang},
  year = 2024,
  month = may,
  number = {arXiv:2405.14458},
  eprint = {2405.14458},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-19},
  abstract = {Over the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model's capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and the model architecture. To this end, we first present the consistent dual assignments for NMS-free training of YOLOs, which brings the competitive performance and low inference latency simultaneously. Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. We comprehensively optimize various components of YOLOs from both the efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome of our effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves the stateof-the-art performance and efficiency across various model scales. For example, our YOLOv10-S is 1.8\texttimes{} faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying 2.8\texttimes{} smaller number of parameters and FLOPs. Compared with YOLOv9-C, YOLOv10-B has 46\% less latency and 25\% fewer parameters for the same performance. Code: https://github.com/THU-MIG/yolov10.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Code: https://github.com/THU-MIG/yolov10},
  file = {/Users/dan/Downloads/2405.14458v1.pdf}
}

@misc{wangYOLOv7TrainableBagfreebies2022,
  title = {{{YOLOv7}}: {{Trainable}} Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors},
  shorttitle = {{{YOLOv7}}},
  author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
  year = 2022,
  month = jul,
  number = {arXiv:2207.02696},
  eprint = {2207.02696},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-19},
  abstract = {YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8\% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9\% AP) outperforms both transformer-based detector SWINL Cascade-Mask R-CNN (9.2 FPS A100, 53.9\% AP) by 509\% in speed and 2\% in accuracy, and convolutionalbased detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2\% AP) by 551\% in speed and 0.7\% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https:// github.com/ WongKinYiu/ yolov7.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dan/Zotero/storage/7J6WZ36E/Wang et al. - 2022 - YOLOv7 Trainable bag-of-freebies sets new state-o.pdf}
}

@article{wangYOLOv9LearningWhat2024,
  title = {{{YOLOv9}}: {{Learning What You Want}} to {{Learn Using Programmable Gradient Information}}},
  author = {Wang, Chien-Yao and Liao, Hong-Yuan Mark},
  year = 2024,
  journal = {arXiv preprint arXiv:2402.13616},
  eprint = {2402.13616},
  archiveprefix = {arXiv}
}

@misc{wangYOLOv9LearningWhat2024a,
  title = {{{YOLOv9}}: {{Learning What You Want}} to {{Learn Using Programmable Gradient Information}}},
  shorttitle = {{{YOLOv9}}},
  author = {Wang, Chien-Yao and Yeh, I.-Hau and Liao, Hong-Yuan Mark},
  year = 2024,
  month = feb,
  number = {arXiv:2402.13616},
  eprint = {2402.13616},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-19},
  abstract = {Today's deep learning methods focus on how to design the most appropriate objective functions so that the prediction results of the model can be closest to the ground truth. Meanwhile, an appropriate architecture that can facilitate acquisition of enough information for prediction has to be designed. Existing methods ignore a fact that when input data undergoes layer-by-layer feature extraction and spatial transformation, large amount of information will be lost. This paper will delve into the important issues of data loss when data is transmitted through deep networks, namely information bottleneck and reversible functions. We proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives. PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights. In addition, a new lightweight network architecture -- Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed. GELAN's architecture confirms that PGI has gained superior results on lightweight models. We verified the proposed GELAN and PGI on MS COCO dataset based object detection. The results show that GELAN only uses conventional convolution operators to achieve better parameter utilization than the state-of-the-art methods developed based on depth-wise convolution. PGI can be used for variety of models from lightweight to large. It can be used to obtain complete information, so that train-fromscratch models can achieve better results than state-of-theart models pre-trained using large datasets, the comparison results are shown in Figure 1. The source codes are at: https://github.com/WongKinYiu/yolov9.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dan/Zotero/storage/KF2LFFII/Wang et al. - 2024 - YOLOv9 Learning What You Want to Learn Using Prog.pdf}
}

@article{watson2010a,
  title = {Assessing Reef Fish Assemblage Structure: {{How}} Do Different Stereo-Video Techniques Compare?},
  author = {Watson, D.L. and Harvey, E.S. and Fitzpatrick, B.M. and Langlois, T.J. and Shedrawi, G.},
  year = 2010,
  journal = {Marine Biology},
  volume = {157},
  number = {6},
  pages = {1237--1250},
  citation-number = {9},
  langid = {english}
}

@misc{wistubaRenateLibraryRealWorld2023,
  title = {Renate: {{A Library}} for {{Real-World Continual Learning}}},
  shorttitle = {Renate},
  author = {Wistuba, Martin and Ferianc, Martin and Balles, Lukas and Archambeau, Cedric and Zappella, Giovanni},
  year = 2023,
  month = apr,
  number = {arXiv:2304.12067},
  eprint = {2304.12067},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-28},
  abstract = {Continual learning enables the incremental training of machine learning models on non-stationary data streams.While academic interest in the topic is high, there is little indication of the use of state-of-the-art continual learning algorithms in practical machine learning deployment. This paper presents Renate, a continual learning library designed to build real-world updating pipelines for PyTorch models. We discuss requirements for the use of continual learning algorithms in practice, from which we derive design principles for Renate. We give a high-level description of the library components and interfaces. Finally, we showcase the strengths of the library by presenting experimental results. Renate may be found at https://github.com/awslabs/renate.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Paper accepted at the CLVision workshop at CVPR 2023},
  file = {/Users/dan/Zotero/storage/23PY6HH7/Wistuba et al. - 2023 - Renate A Library for Real-World Continual Learnin.pdf;/Users/dan/Zotero/storage/RJKFGWLI/2304.html}
}

@article{wongNomogramThatPredicts2005,
  title = {A {{Nomogram That Predicts}} the {{Presence}} of {{Sentinel Node Metastasis}} in {{Melanoma With Better Discrimination Than}} the {{American Joint Committee}} on {{CancerStaging System}}},
  author = {Wong, Sandra L. and Kattan, Michael W. and McMasters, Kelly M. and Coit, Daniel G.},
  year = 2005,
  month = apr,
  journal = {Ann Surg Oncol},
  volume = {12},
  number = {4},
  pages = {282--288},
  issn = {1068-9265, 1534-4681},
  doi = {10.1245/ASO.2005.05.016},
  urldate = {2024-04-21},
  abstract = {Background: The threshold and indications for sentinel lymph node (SLN) biopsy in patients with melanoma remain somewhat arbitrary. Many variables associated with SLN positivity have previously been identified, including a significant association between the American Joint Committee on Cancer (AJCC) staging system and SLN status. We developed a user-friendly nomogram that takes several characteristics into account simultaneously to more accurately predict the presence of SLN metastasis for an individual patient. Methods: A total of 979 patients who underwent successful SLN biopsy for cutaneous melanoma at a single institution between February 1991 and November 2003 were included in the analysis. Predictors were used to develop a nomogram, based on logistic regression analysis, to predict the probability of SLN positivity. A large multi-institutional trial with 3108 patients was used to validate the predictive accuracy of the nomogram compared with the AJCC staging system. Results: The nomogram was developed and found to be accurate and discriminating. The concordance index of the nomogram, a measure of predictive ability, was .694 when evaluated with the validation dataset. In contrast, the concordance index of the AJCC staging system was lower (.663; P {$<$} .001). Conclusions: Using commonly available clinicopathologic information, we developed a nomogram to accurately predict the probability of a positive SLN in patients with melanoma. This tool takes several characteristics into account simultaneously. This model should enable improved patient counseling and treatment selection.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/dan/Zotero/storage/8BIZRP98/Wong et al. - 2005 - A Nomogram That Predicts the Presence of Sentinel .pdf}
}

@article{wongSentinelLymphNode2012,
  title = {Sentinel {{Lymph Node Biopsy}} for {{Melanoma}}: {{American Society}} of {{Clinical Oncology}} and {{Society}} of {{Surgical Oncology Joint Clinical Practice Guideline}}},
  shorttitle = {Sentinel {{Lymph Node Biopsy}} for {{Melanoma}}},
  author = {Wong, Sandra L. and Balch, Charles M. and Hurley, Patricia and Agarwala, Sanjiv S. and Akhurst, Timothy J. and Cochran, Alistair and Cormier, Janice N. and Gorman, Mark and Kim, Theodore Y. and McMasters, Kelly M. and Noyes, R. Dirk and Schuchter, Lynn M. and Valsecchi, Matias E. and Weaver, Donald L. and Lyman, Gary H.},
  year = 2012,
  month = aug,
  journal = {JCO},
  volume = {30},
  number = {23},
  pages = {2912--2918},
  publisher = {Wolters Kluwer},
  issn = {0732-183X},
  doi = {10.1200/JCO.2011.40.3519},
  urldate = {2024-04-19},
  abstract = {Purpose The American Society of Clinical Oncology (ASCO) and Society of Surgical Oncology (SSO) sought to provide an evidence-based guideline on the use of lymphatic mapping and sentinel lymph node (SLN) biopsy in staging patients with newly diagnosed melanoma. Methods A comprehensive systematic review of the literature published from January 1990 through August 2011 was completed using MEDLINE and EMBASE. Abstracts from ASCO and SSO annual meetings were included in the evidence review. An Expert Panel was convened to review the evidence and develop guideline recommendations. Results Seventy-three studies met full eligibility criteria. The evidence review demonstrated that SLN biopsy is an acceptable method for lymph node staging of most patients with newly diagnosed melanoma. Recommendations SLN biopsy is recommended for patients with intermediate-thickness melanomas (Breslow thickness, 1 to 4 mm) of any anatomic site; use of SLN biopsy in this population provides accurate staging. Although there are few studies focusing on patients with thick melanomas (T4; Breslow thickness, {$>$} 4 mm), SLN biopsy may be recommended for staging purposes and to facilitate regional disease control. There is insufficient evidence to support routine SLN biopsy for patients with thin melanomas (T1; Breslow thickness, {$<$} 1 mm), although it may be considered in selected patients with high-risk features when staging benefits outweigh risks of the procedure. Completion lymph node dissection (CLND) is recommended for all patients with a positive SLN biopsy and achieves good regional disease control. Whether CLND after a positive SLN biopsy improves survival is the subject of the ongoing Multicenter Selective Lymphadenectomy Trial II. Copyright \copyright{} 2012 American Society of Clinical Oncology and Society of Surgical Oncology. All rights reserved. No part of this document may be reproduced or transmitted in any form or by any means, electronic or mechanical, including photocopy, recording, or any information storage and retrieval system, without written permission by the American Society of Clinical Oncology and Society of Surgical Oncology.},
  file = {/Users/dan/Zotero/storage/UJHLCFQI/Wong et al. - 2012 - Sentinel Lymph Node Biopsy for Melanoma American .pdf}
}

@article{wormGlobalPatternsPredator2005,
  title = {Global {{Patterns}} of {{Predator Diversity}} in the {{Open Oceans}}},
  author = {Worm, Boris and Sandow, Marcel and Oschlies, Andreas and Lotze, Heike K. and Myers, Ransom A.},
  year = 2005,
  month = aug,
  journal = {Science},
  volume = {309},
  number = {5739},
  pages = {1365--1369},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1113399},
  urldate = {2024-10-20},
  abstract = {The open oceans comprise most of the biosphere, yet patterns and trends of species diversity there are enigmatic. Here, we derive worldwide patterns of tuna and billfish diversity over the past 50 years, revealing distinct subtropical ``hotspots'' that appeared to hold generally for other predators and zooplankton. Diversity was positively correlated with thermal fronts and dissolved oxygen and a nonlinear function of temperature ({$\sim$}25{$^\circ$}C optimum). Diversity declined between 10 and 50\% in all oceans, a trend that coincided with increased fishing pressure, superimposed on strong El Ni\~no--Southern Oscillation--driven variability across the Pacific. We conclude that predator diversity shows a predictable yet eroding pattern signaling ecosystem-wide changes linked to climate and fishing.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/6NULQYZK/Worm et al. - 2005 - Global Patterns of Predator Diversity in the Open .pdf}
}

@article{wuFishTargetDetection2023,
  title = {Fish {{Target Detection}} in {{Underwater Blurred Scenes Based}} on {{Improved YOLOv5}}},
  author = {Wu, Fei and Cai, Zonghai and Fan, Shengli and Song, Ruiyin and Wang, Lang and Cai, Weiming},
  year = 2023,
  journal = {IEEE Access},
  volume = {11},
  pages = {122911--122925},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3328940},
  urldate = {2024-10-22},
  abstract = {In recent years, human beings have paid more and more attention to the exploration of the underwater world. As an important part of underwater resources, fish can be detected by using the fish image data collected by underwater imaging systems, which can help us better understand fish species richness and assess fish populations. In this paper, we proposed a fish target detection algorithm YOLOv5-fish for underwater blurred scenes. For underwater blurred scenes, the algorithm first uses the auto-MSRCR algorithm to enhance the acquired low-quality underwater blurred image data and obtains the enhanced fish images as the dataset for model training. Then the YOLOv5s algorithm is improved through the following methods. First, replacing the original activation function with Meta-ACON to realize the model autonomously control the nonlinearity degree of the activation function; Second, adding the Shuffle Attention mechanism to enhance the model's attention to the detection object; Third, introducing RepVGG structure to the backbone network to accelerate the model's inference speed. The experimental results show that the improved YOLOv5-fish algorithm achieves an mAP of 97.6\% and a detection speed of 84 FPS, which can achieve accurate and fast detection for fish targets in underwater blurred scenes.},
  copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/dan/Zotero/storage/G9MXKJXK/Wu et al. - 2023 - Fish Target Detection in Underwater Blurred Scenes.pdf}
}

@inproceedings{xiuliFastAccurateFish2015,
  title = {Fast Accurate Fish Detection and Recognition of Underwater Images with {{Fast R-CNN}}},
  booktitle = {{{OCEANS}} 2015 - {{MTS}}/{{IEEE Washington}}},
  author = {{Xiu Li} and {Min Shang} and Qin, Hongwei and {Liansheng Chen}},
  year = 2015,
  month = oct,
  pages = {1--5},
  publisher = {IEEE},
  address = {Washington, DC},
  doi = {10.23919/OCEANS.2015.7404464},
  urldate = {2024-04-23},
  abstract = {This paper aims at detecting and recognizing fish species from underwater images by means of Fast R-CNN (Regions with Convolutional Neural and Networks) features. Encouraged by powerful recognition results achieved by Convolutional Neural Networks (CNNs) on generic VOC and ImageNet dataset, we apply this popular deep ConvNets to domain-specific underwater environment which is more complicated than overland situation, using a new dataset of 24277 ImageCLEF fish images belonging to 12 classes. The experimental results demonstrate the promising performance of our networks. Fast R-CNN improves mean average precision (mAP) by 11.2\% relative to Deformable Parts Model (DPM) baseline-achieving a mAP of 81.4\%, and detects 80 faster than previous R-CNN on a single fish image.},
  isbn = {978-0-933957-43-5},
  langid = {english},
  file = {/Users/dan/Zotero/storage/XTYKNARC/Xiu Li et al. - 2015 - Fast accurate fish detection and recognition of un.pdf}
}

@inproceedings{xu2018a,
  title = {Enabling Deep Learning at the {{IoT}} Edge: {{Optimizations}} and Mechanisms},
  booktitle = {Proceedings of the 1st International Workshop on Edge Systems, Analytics and Networking},
  author = {Xu, Q. and Ren, P. and Wang, H. and Wang, J.},
  year = 2018,
  pages = {37--42},
  citation-number = {28},
  langid = {english}
}

@misc{xuDAMOYOLOReportRealTime2023,
  title = {{{DAMO-YOLO}} : {{A Report}} on {{Real-Time Object Detection Design}}},
  shorttitle = {{{DAMO-YOLO}}},
  author = {Xu, Xianzhe and Jiang, Yiqi and Chen, Weihua and Huang, Yilun and Zhang, Yuan and Sun, Xiuyu},
  year = 2023,
  month = apr,
  number = {arXiv:2211.15444},
  eprint = {2211.15444},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-20},
  abstract = {In this report, we present a fast and accurate object detection method dubbed DAMO-YOLO, which achieves higher performance than the state-of-the-art YOLO series. DAMO-YOLO is extended from YOLO with some new technologies, including Neural Architecture Search (NAS), efficient Reparameterized Generalized-FPN (RepGFPN), a lightweight head with AlignedOTA label assignment, and distillation enhancement. In particular, we use MAE-NAS, a method guided by the principle of maximum entropy, to search our detection backbone under the constraints of low latency and high performance, producing ResNet/CSP-like structures with spatial pyramid pooling and focus modules. In the design of necks and heads, we follow the rule of ``large neck, small head''.We import Generalized-FPN with accelerated queen-fusion to build the detector neck and upgrade its CSPNet with efficient layer aggregation networks (ELAN) and reparameterization. Then we investigate how detector head size affects detection performance and find that a heavy neck with only one task projection layer would yield better results.In addition, AlignedOTA is proposed to solve the misalignment problem in label assignment. And a distillation schema is introduced to improve performance to a higher level. Based on these new techs, we build a suite of models at various scales to meet the needs of different scenarios. For general industry requirements, we propose DAMO-YOLO-T/S/M/L. They can achieve 43.6/47.7/50.2/51.9 mAPs on COCO with the latency of 2.78/3.83/5.62/7.95 ms on T4 GPUs respectively. Additionally, for edge devices with limited computing power, we have also proposed DAMO-YOLO-Ns/Nm/Nl lightweight models. They can achieve 32.3/38.2/40.5 mAPs on COCO with the latency of 4.08/5.05/6.69 ms on X86-CPU. Our proposed general and lightweight models have outperformed other YOLO series models in their respective application scenarios.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Project Website: https://github.com/tinyvision/damo-yolo},
  file = {/Users/dan/Zotero/storage/P3ZJEGE7/Xu et al. - 2023 - DAMO-YOLO  A Report on Real-Time Object Detection.pdf}
}

@article{xushengzhaoMultiViewTensorGraph2023,
  title = {Multi-{{View Tensor Graph Neural Networks Through Reinforced Aggregation}}},
  author = {{Xusheng Zhao} and {Qiong Dai} and {Jia Wu} and {Hao Peng} and {Mingsheng Liu} and {Xu Bai} and {Jianlong Tan} and {Senzhang Wang} and {P. Yu}},
  year = 2023,
  month = apr,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {35},
  pages = {4077--4091},
  doi = {10.1109/TKDE.2022.3142179},
  file = {/Users/dan/Zotero/storage/TY3HGY4K/Xusheng Zhao et al. - 2023 - Multi-View Tensor Graph Neural Networks Through Re.pdf}
}

@article{yadavImprovedDeepLearningbased2023,
  title = {An Improved Deep Learning-Based Optimal Object Detection System from Images},
  author = {Yadav, Satya Prakash and Jindal, Muskan and Rani, Preeti and De Albuquerque, Victor Hugo C. and Dos Santos Nascimento, Caio and Kumar, Manoj},
  year = 2023,
  month = sep,
  journal = {Multimed Tools Appl},
  volume = {83},
  number = {10},
  pages = {30045--30072},
  issn = {1573-7721},
  doi = {10.1007/s11042-023-16736-5},
  urldate = {2024-10-20},
  abstract = {Abstract             Computer vision technology for detecting objects in a complex environment often includes other key technologies, including pattern recognition, artificial intelligence, and digital image processing. It has been shown that Fast Convolutional Neural Networks (CNNs) with You Only Look Once (YOLO) is optimal for differentiating similar objects, constant motion, and low image quality. The proposed study aims to resolve these issues by implementing three different object detection algorithms---You Only Look Once (YOLO), Single Stage Detector (SSD), and Faster Region-Based Convolutional Neural Networks (R-CNN). This paper compares three different deep-learning object detection methods to find the best possible combination of feature and accuracy. The R-CNN object detection techniques are performed better than single-stage detectors like Yolo (You Only Look Once) and Single Shot Detector (SSD) in term of accuracy, recall, precision and loss.},
  langid = {english},
  file = {/Users/dan/Zotero/storage/7E3SFK6S/Yadav et al. - 2023 - An improved deep learning-based optimal object det.pdf}
}

@inproceedings{yamashita2007a,
  title = {Underwater Navigation of a Mobile Robot by Using a Multi-Beam Scanning Sonar},
  booktitle = {2007 {{IEEE}} International Conference on Robotics and Automation},
  author = {Yamashita, M. and Yoshikawa, A. and Shioyama, T.},
  year = 2007,
  pages = {4184--4189},
  publisher = {IEEE},
  citation-number = {22},
  langid = {english}
}

@incollection{yamashita2017a,
  title = {A New Underwater Imaging Method for Fish Monitoring},
  booktitle = {{{OCEANS}} 2017 - Aberdeen},
  author = {Yamashita, M. and Yoshikawa, A. and Sato, N.},
  year = 2017,
  pages = {1--4},
  publisher = {IEEE},
  citation-number = {32},
  langid = {english}
}

@incollection{yamashita2017a,
  title = {A New Underwater Imaging Method for Fish Monitoring},
  booktitle = {{{OCEANS}} 2017 - Aberdeen},
  author = {Yamashita, M.},
  year = 2017,
  pages = {1--4},
  publisher = {IEEE},
  citation-number = {22},
  langid = {english}
}

@incollection{yamashita2017a,
  title = {A New Underwater Imaging Method for Fish Monitoring},
  booktitle = {{{OCEANS}} 2017 - Aberdeen},
  author = {Yamashita, M.},
  year = 2017,
  pages = {1--4},
  publisher = {IEEE},
  citation-number = {22},
  langid = {english}
}

@article{ying-chiaoliaoPerfNetRTPlatformAwarePerformance2020,
  title = {{{PerfNetRT}}: {{Platform-Aware Performance Modeling}} for {{Optimized Deep Neural Networks}}},
  author = {{Ying-Chiao Liao} and {Chuan-Chi Wang} and {Chia-Heng Tu} and {Ming-Chang Kao} and {Wen-Yew Liang} and {Shih-Hao Hung}},
  year = 2020,
  month = dec,
  journal = {2020 International Computer Symposium (ICS)},
  pages = {153--158},
  doi = {10.1109/ICS51289.2020.00039},
  file = {/Users/dan/Zotero/storage/73GS986W/Ying-Chiao Liao et al. - 2020 - PerfNetRT Platform-Aware Performance Modeling for.pdf}
}

@article{yuxiaozhouExploringTensorRTImprove2022,
  title = {Exploring {{TensorRT}} to {{Improve Real-Time Inference}} for {{Deep Learning}}},
  author = {{Yuxiao Zhou} and {Kecheng Yang}},
  year = 2022,
  month = dec,
  journal = {2022 IEEE 24th Int Conf on High Performance Computing \& Communications; 8th Int Conf on Data Science \& Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud \& Big Data Systems \& Application (HPCC/DSS/SmartCity/DependSys)},
  pages = {2011--2018},
  doi = {10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00299},
  file = {/Users/dan/Zotero/storage/GR94WBAQ/Yuxiao Zhou and Kecheng Yang - 2022 - Exploring TensorRT to Improve Real-Time Inference .pdf}
}

@article{zhaiUnderwaterSeaCucumber2022,
  title = {Underwater {{Sea Cucumber Identification Based}} on {{Improved YOLOv5}}},
  author = {Zhai, Xianyi and Wei, Honglei and He, Yuyang and Shang, Yetong and Liu, Chenghao},
  year = 2022,
  month = sep,
  journal = {Applied Sciences},
  volume = {12},
  number = {18},
  pages = {9105},
  issn = {2076-3417},
  doi = {10.3390/app12189105},
  urldate = {2024-10-22},
  abstract = {In order to develop an underwater sea cucumber collecting robot, it is necessary to use the machine vision method to realize sea cucumber recognition and location. An identification and location method of underwater sea cucumber based on improved You Only Look Once version 5 (YOLOv5) is proposed. Due to the low contrast between sea cucumbers and the underwater environment, the Multi-Scale Retinex with Color Restoration (MSRCR) algorithm was introduced to process the images to enhance the contrast. In order to improve the recognition precision and efficiency, the Convolutional Block Attention Module (CBAM) is added. In order to make small target recognition more precise, the Detect layer was added to the Head network of YOLOv5s. The improved YOLOv5s model and YOLOv5s, YOLOv4, and Faster-RCNN identified the same image set; the experimental results show improved YOLOv5 recognition precision level and confidence level, especially for small target recognition, which is excellent and better than other models. Compared to the other three models, the improved YOLOv5s has higher precision and detection time. Compared with the YOLOv5s, the precision and recall rate of the improved YOLOv5s model are improved by 9\% and 11.5\%, respectively.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/dan/Zotero/storage/CCA5HMH9/Zhai et al. - 2022 - Underwater Sea Cucumber Identification Based on Im.pdf}
}

@article{zhangFPGABasedFeatureExtraction2023,
  title = {{{FPGA-Based Feature Extraction}} and {{Tracking Accelerator}} for {{Real-Time Visual SLAM}}},
  author = {Zhang, Jie and Xiong, Shuai and Liu, Cheng and Geng, Yongchao and Xiong, Wei and Cheng, Song and Hu, Fang},
  year = 2023,
  month = sep,
  journal = {Sensors},
  volume = {23},
  number = {19},
  pages = {8035},
  issn = {1424-8220},
  doi = {10.3390/s23198035},
  urldate = {2024-06-27},
  abstract = {Due to its advantages of low latency, low power consumption, and high flexibility, FPGA-based acceleration technology has been more and more widely studied and applied in the field of computer vision in recent years. An FPGA-based feature extraction and tracking accelerator for real-time visual odometry (VO) and visual simultaneous localization and mapping (V-SLAM) is proposed, which can realize the complete acceleration processing capability of the image front-end. For the first time, we implement a hardware solution that combines features from accelerated segment test (FAST) feature points with Gunnar Farneback (GF) dense optical flow to achieve better feature tracking performance and provide more flexible technical route selection. In order to solve the scale invariance and rotation invariance lacking problems of FAST features, an efficient pyramid module with a five-layer thumbnail structure was designed and implemented. The accelerator was implemented on a modern Xilinx Zynq FPGA. The evaluation results showed that the accelerator could achieve stable tracking of features of violently shaking images and were consistent with the results from MATLAB code running on PCs. Compared to PC CPUs, which require seconds of processing time, the processing latency was greatly reduced to the order of milliseconds, making GF dense optical flow an efficient and practical technical solution on the edge side.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/dan/Zotero/storage/WKD7XV9X/Zhang et al. - 2023 - FPGA-Based Feature Extraction and Tracking Acceler.pdf}
}

@article{zhangImprovedYOLOv5BasedUnderwater2023,
  title = {An {{Improved YOLOv5-Based Underwater Object-Detection Framework}}},
  author = {Zhang, Jian and Zhang, Jinshuai and Zhou, Kexin and Zhang, Yonghui and Chen, Hongda and Yan, Xinyue},
  year = 2023,
  month = apr,
  journal = {Sensors},
  volume = {23},
  number = {7},
  pages = {3693},
  issn = {1424-8220},
  doi = {10.3390/s23073693},
  urldate = {2024-10-01},
  abstract = {To date, general-purpose object-detection methods have achieved a great deal. However, challenges such as degraded image quality, complex backgrounds, and the detection of marine organisms at different scales arise when identifying underwater organisms. To solve such problems and further improve the accuracy of relevant models, this study proposes a marine biological objectdetection architecture based on an improved YOLOv5 framework. First, the backbone framework of Real-Time Models for object Detection (RTMDet) is introduced. The core module, Cross-Stage Partial Layer (CSPLayer), includes a large convolution kernel, which allows the detection network to precisely capture contextual information more comprehensively. Furthermore, a common convolution layer is added to the stem layer, to extract more valuable information from the images efficiently. Then, the BoT3 module with the multi-head self-attention (MHSA) mechanism is added into the neck module of YOLOv5, such that the detection network has a better effect in scenes with dense targets and the detection accuracy is further improved. The introduction of the BoT3 module represents a key innovation of this paper. Finally, union dataset augmentation (UDA) is performed on the training set using the Minimal Color Loss and Locally Adaptive Contrast Enhancement (MLLE) image augmentation method, and the result is used as the input to the improved YOLOv5 framework. Experiments on the underwater datasets URPC2019 and URPC2020 show that the proposed framework not only alleviates the interference of underwater image degradation, but also makes the mAP@0.5 reach 79.8\% and 79.4\% and improves the mAP@0.5 by 3.8\% and 1.1\%, respectively, when compared with the original YOLOv8 on URPC2019 and URPC2020, demonstrating that the proposed framework presents superior performance for the high-precision detection of marine organisms.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/dan/Zotero/storage/SX6MTLHS/Zhang et al. - 2023 - An Improved YOLOv5-Based Underwater Object-Detecti.pdf}
}

@misc{zhangIterativeLabelingMethod2022,
  title = {An {{Iterative Labeling Method}} for {{Annotating Fisheries Imagery}}},
  author = {Zhang, Zhiyong and Kaveti, Pushyami and Singh, Hanumant and Powell, Abigail and Fruh, Erica and Clarke, M. Elizabeth},
  year = 2022,
  month = jun,
  number = {arXiv:2204.12934},
  eprint = {2204.12934},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-15},
  abstract = {In this paper, we present a methodology for fisheriesrelated data that allows us to converge on a labeled image dataset by iterating over the dataset with multiple training and production loops that can exploit crowdsourcing interfaces. We present our algorithm and its results on two separate sets of image data collected using the Seabed autonomous underwater vehicle. The first dataset comprises of 2,026 completely unlabeled images, while the second consists of 21,968 images that were point annotated by experts. Our results indicate that training with a small subset and iterating on that to build a larger set of labeled data allows us to converge to a fully annotated dataset with a small number of iterations. Even in the case of a dataset labeled by experts, a single iteration of the methodology improves the labels by discovering additional complicated examples of labels associated with fish that overlap, are very small, or obscured by the contrast limitations associated with underwater imagery.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/dan/Zotero/storage/YY7FY2FW/Zhang et al. - 2022 - An Iterative Labeling Method for Annotating Fisher.pdf}
}

@article{zhangSingleImageDefogging2019,
  title = {Single {{Image Defogging Based}} on {{Multi-Channel Convolutional MSRCR}}},
  author = {Zhang, Weidong and Dong, Lili and Pan, Xipeng and Zhou, Jingchun and Qin, Li and Xu, Wenhai},
  year = 2019,
  journal = {IEEE Access},
  volume = {7},
  pages = {72492--72504},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2920403},
  urldate = {2024-10-12},
  abstract = {In order to solve the problem of image degradation in foggy weather, a single image defogging method based on a multi-scale retinex with color restoration (MSRCR) of multi-channel convolution (MC) is proposed. The whole defogging process mainly consists of four key parts: estimation of illumination components, guided filter operation, reconstruction of fog-free images, and white balance operation. Firstly, the multi-scale Gaussian kernels are employed to extract precise features to estimate the illumination component. After that, MSRCR method is applied to enhance the global contrast, detail information, and color restoration of the image. Secondly, the smoothing constraints of both illumination component and reflected component are considered together by using guided filter twice, thus the enhanced image satisfies the smoothing constraint and the noise in the enhanced image is reduced. Thirdly, the enhanced image by MSRCR and the image processed by secondary guided filter are fused by linear weighting to reconstruct the final fog-free image. Finally, in order to eliminate the influence of illumination on the color of the defogged image, the final defogged image is processed by white balance. The experimental results demonstrated that the proposed method can outperform state-of-the-art methods in both qualitative and quantitative comparisons.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/OAPA.html},
  langid = {english},
  file = {/Users/dan/Zotero/storage/959VSJGX/Zhang et al. - 2019 - Single Image Defogging Based on Multi-Channel Conv.pdf}
}

@misc{zhangSingleImageDefogging2019a,
  title = {Single Image Defogging Based on Multi-Channel Convolutional {{MSRCR}}},
  author = {Zhang, Weidong and Dong, Lili and Pan, Xipeng and Zhou, Jingchun and Qin, Li and Xu, Wenhai},
  year = 2019,
  month = jan,
  volume = {7},
  pages = {72492--72504}
}

@article{zhangUnderwaterImageEnhancement2022,
  title = {Underwater {{Image Enhancement}} by {{Attenuated Color Channel Correction}} and {{Detail Preserved Contrast Enhancement}}},
  author = {Zhang, Weidong and Wang, Yudong and Li, Chongyi},
  year = 2022,
  month = jul,
  journal = {IEEE J. Oceanic Eng.},
  volume = {47},
  number = {3},
  pages = {718--735},
  issn = {0364-9059, 1558-1691, 2373-7786},
  doi = {10.1109/JOE.2022.3140563},
  urldate = {2024-10-09},
  abstract = {An underwater image often suffers from quality degradation issues, such as color deviations, low contrast, and blurred details, due to the absorption and scattering of light. In this article, we propose to address the aforementioned degradation issues via attenuated color channel correction and detail preserved contrast enhancement. Concretely, we first propose an underwater image color correction method. Considering the differences between superior and inferior color channels of an underwater image, the inferior color channels are compensated via especially designed attenuation matrices. We then employ a dual-histogram-based iterative threshold method and a limited histogram method with Rayleigh distribution to improve the global and local contrast of the color-corrected image, thus achieving a global contrast-enhanced version and a local contrast-enhanced version, respectively. To integrate the complementary merits between the global contrastenhanced version and the local contrast-enhanced version, we adopt a multiscale fusion strategy to fuse them. Finally, we propose a multiscale unsharp masking strategy to further sharpen the fused image for better visual quality. Extensive experiments on four underwater image enhancement benchmark data sets demonstrate that our method effectively enhances underwater images qualitatively and quantitatively. Besides, our method also generalizes well to the enhancement of low-light images and hazy images.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/Users/dan/Zotero/storage/ETUCQVPS/Zhang et al. - 2022 - Underwater Image Enhancement by Attenuated Color C.pdf}
}

@misc{zhangUnderwaterImageEnhancement2022a,
  title = {Underwater {{Image Enhancement}} by {{Attenuated Color Channel Correction}} and {{Detail Preserved Contrast Enhancement}}},
  author = {Zhang, Weidong and Wang, Yudong and Li, Chongyi},
  year = 2022,
  month = mar,
  journal = {Institute of Electrical and Electronics Engineers},
  volume = {47},
  number = {3},
  pages = {718--735},
  doi = {10.1109/joe.2022.3140563}
}

@misc{zhangYOLOSeriesTarget2023,
  title = {{{YOLO}} Series Target Detection Algorithms for Underwater Environments},
  author = {Zhang, Chenjie and Jiao, Pengcheng},
  year = 2023,
  month = sep,
  number = {arXiv:2309.03539},
  eprint = {2309.03539},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-19},
  abstract = {You Only Look Once (YOLO) algorithm is a representative target detection algorithm emerging in 2016, which is known for its balance of computing speed and accuracy, and now plays an important role in various fields of human production and life. However, there are still many limitations in the application of YOLO algorithm in underwater environments due to problems such as dim light and turbid water. With limited land area resources, the ocean must have great potential for future human development. In this paper, starting from the actual needs of marine engineering applications, taking underwater structural health monitoring (SHM) and underwater biological detection as examples, we propose improved methods for the application of underwater YOLO algorithms, and point out the problems that still exist.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/dan/Zotero/storage/M963RNAH/Zhang and Jiao - 2023 - YOLO series target detection algorithms for underw.pdf}
}

@misc{zhaoMultiViewTensorGraph2022,
  title = {Multi-{{View Tensor Graph Neural Networks Through Reinforced Aggregation}}},
  author = {Zhao, Xusheng and Dai, Qiong and Wu, Jia and Peng, Hao and Liu, Mingsheng and Bai, Xu and Tan, Jianlong and Wang, Senzhang and Yu, Philip S.},
  year = 2022,
  month = feb,
  journal = {IEEE Computer Society},
  volume = {35},
  number = {4},
  pages = {4077--4091},
  doi = {10.1109/tkde.2022.3142179}
}

@misc{zhaoMultiViewTensorGraph2022a,
  title = {Multi-{{View Tensor Graph Neural Networks Through Reinforced Aggregation}}},
  author = {Zhao, Xusheng and Dai, Qiong and Wu, Jia and Peng, Hao and Liu, Mingsheng and Bai, Xu and Tan, Jianlong and Wang, Senzhang and Yu, Philip S.},
  year = 2022,
  month = feb,
  volume = {35},
  number = {4},
  pages = {4077--4091}
}

@article{zhongqianliuTensorRTAccelerationBased2022,
  title = {{{TensorRT}} Acceleration Based on Deep Learning {{OFDM}} Channel Compensation},
  author = {{Zhongqian Liu} and {Dan Ding}},
  year = 2022,
  month = jul,
  journal = {Journal of Physics: Conference Series},
  volume = {2303},
  doi = {10.1088/1742-6596/2303/1/012047},
  file = {/Users/dan/Zotero/storage/6VQ5PRHD/Zhongqian Liu and Dan Ding - 2022 - TensorRT acceleration based on deep learning OFDM .pdf}
}

@misc{zhouExploringTensorRTImprove2022,
  title = {Exploring {{TensorRT}} to {{Improve Real-Time Inference}} for {{Deep Learning}}},
  author = {Zhou, Yuxiao and Yang, Kecheng},
  year = 2022,
  month = dec,
  doi = {10.1109/hpcc-dss-smartcity-dependsys57074.2022.00299}
}

@misc{zhouExploringTensorRTImprove2022a,
  title = {Exploring {{TensorRT}} to {{Improve Real-Time Inference}} for {{Deep Learning}}},
  author = {Zhou, Yuxiao and Yang, Kecheng},
  year = 2022,
  month = dec,
  pages = {2011--2018}
}

@inproceedings{zhouExploringTensorRTImprove2022b,
  title = {Exploring {{TensorRT}} to {{Improve Real-Time Inference}} for {{Deep Learning}}},
  booktitle = {2022 {{IEEE}} 24th {{Int Conf}} on {{High Performance Computing}} \& {{Communications}}; 8th {{Int Conf}} on {{Data Science}} \& {{Systems}}; 20th {{Int Conf}} on {{Smart City}}; 8th {{Int Conf}} on {{Dependability}} in {{Sensor}}, {{Cloud}} \& {{Big Data Systems}} \& {{Application}} ({{HPCC}}/{{DSS}}/{{SmartCity}}/{{DependSys}})},
  author = {Zhou, Yuxiao and Yang, Kecheng},
  year = 2022,
  month = dec,
  pages = {2011--2018},
  publisher = {IEEE},
  address = {Hainan, China},
  doi = {10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00299},
  urldate = {2024-10-19},
  abstract = {Deep learning (DL) has dramatically evolved and become one of the most successful machine learning techniques. A variety of DL-enabled applications have been widely integrated into software systems, including embedded ones. Although having achieved very successful results in accuracy, the large size of deep neural networks could require significant runtime and computing resource consumption. To overcome these drawbacks, TensorRT has been developed and may be incorporated into popular DL frameworks such as PyTorch and Open Neural Network Exchange (ONNX). In this paper, focusing on inference, we provide a comprehensive evaluation on the performances of TensorRT. Specifically, we evaluate inference output validation, inference time, inference throughput, and GPU memory usage. Our results demonstrate that TensorRT is able to significantly improve the inference efficiency metrics without compromising inference accuracy. Furthermore, TensorRT may be adopted via several alternative workflows. Our evaluation also shows the pros and cons of these TensorRT workflows. We analyze that for each workflow and discuss the workflow selection for different application scenarios.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-1993-4},
  langid = {english},
  file = {/Users/dan/Zotero/storage/MZBAHF57/Zhou and Yang - 2022 - Exploring TensorRT to Improve Real-Time Inference .pdf}
}

@article{zhouUnderwaterImageEnhancement2023,
  title = {Underwater {{Image Enhancement Method}} via {{Multi-Interval Subhistogram Perspective Equalization}}},
  author = {Zhou, Jingchun and Pang, Lei and Zhang, Dehuan and Zhang, Weishi},
  year = 2023,
  month = apr,
  journal = {IEEE J. Oceanic Eng.},
  volume = {48},
  number = {2},
  pages = {474--488},
  issn = {0364-9059, 1558-1691, 2373-7786},
  doi = {10.1109/JOE.2022.3223733},
  urldate = {2024-10-01},
  abstract = {Due to the selective attenuation of light in water, captured underwater images exhibit poor visibility and cause considerable challenges for vision tasks. The structural and statistical properties of different regions of degraded underwater images are damaged at different levels, resulting in a global nonuniform drift of the feature representation, causing further degradation of visual performance. To handle these issues, we present an underwater image enhancement method via multi-interval subhistogram perspective equalization to address the issues posed by underwater images. We estimate the degree of feature drifts in each area of an image by extracting the statistical characteristics of the image, using this information to guide feature enhancement to achieve adaptive feature enhancement, thereby improving the visual effect of degraded images. We first design a variational model that uses the difference between data items and regular items to improve the color correction performance of the method based on subinterval linear transformation. In addition, a multithreshold selection method, which adaptively selects a threshold array for interval division, is developed. Ultimately, a multi-interval subhistogram equalization method, which performs histogram equalization in each subhistogram to improve the image contrast, is presented. Experiments on underwater images with various scenarios demonstrate that our method significantly outperforms many state-of-theart methods qualitatively and quantitatively.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/Users/dan/Zotero/storage/9CPDAYJE/Zhou et al. - 2023 - Underwater Image Enhancement Method via Multi-Inte.pdf}
}

@misc{zhouUnderwaterImageEnhancement2023a,
  title = {Underwater {{Image Enhancement Method}} via {{Multi-Interval Subhistogram Perspective Equalization}}},
  author = {Zhou, Jingchun and Pang, Lei and Zhang, Dehuan and Zhang, Weishi},
  year = 2023,
  month = feb,
  journal = {Institute of Electrical and Electronics Engineers},
  volume = {48},
  number = {2},
  pages = {474--488},
  doi = {10.1109/joe.2022.3223733}
}

@article{zotero-147,
  type = {Article}
}

@misc{zotero-495,
  type = {Misc}
}

@misc{zotero-506,
  type = {Misc}
}

@misc{zotero-800,
  type = {Misc}
}

@article{zotero-829,
  type = {Article}
}
