{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import open_clip\n",
    "from torchvision import transforms\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioCLIPDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, transform):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data.iloc[idx]['image_path']\n",
    "        text = self.data.iloc[idx]['text']\n",
    "\n",
    "        image = self.transform(Image.open(image_path).convert(\"RGB\"))\n",
    "        token = self.tokenizer([text])[0]\n",
    "\n",
    "        return image, token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioCLIPLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model_name='hf-hub:imageomics/bioclip', lr=5e-5):\n",
    "        super().__init__()\n",
    "        self.model, self.preprocess_train, _ = open_clip.create_model_and_transforms(model_name)\n",
    "        self.tokenizer = open_clip.get_tokenizer(model_name)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        image_features = self.model.encode_image(images)\n",
    "        text_features = self.model.encode_text(texts)\n",
    "        return image_features, text_features\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, stage=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, stage=\"val\")\n",
    "\n",
    "    def _shared_step(self, batch, stage):\n",
    "        images, texts = batch\n",
    "        image_features, text_features = self(images, texts)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logits_per_image = image_features @ text_features.T\n",
    "        logits_per_text = text_features @ image_features.T\n",
    "        labels = torch.arange(len(images), device=self.device)\n",
    "\n",
    "        loss_i = nn.CrossEntropyLoss()(logits_per_image, labels)\n",
    "        loss_t = nn.CrossEntropyLoss()(logits_per_text, labels)\n",
    "        loss = (loss_i + loss_t) / 2\n",
    "\n",
    "        self.log(f\"{stage}_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 3 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=3)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "/home/dzimmerman2021/miniconda3/envs/fathmonet_bioclip/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/dzimmerman2021/miniconda3/envs/fathmonet_biocl ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "2025-05-20 15:45:39.095950: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-20 15:45:39.728432: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name  | Type | Params | Mode \n",
      "---------------------------------------\n",
      "0 | model | CLIP | 149 M  | train\n",
      "---------------------------------------\n",
      "149 M     Trainable params\n",
      "0         Non-trainable params\n",
      "149 M     Total params\n",
      "598.483   Total estimated model params size (MB)\n",
      "277       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 593/593 [03:03<00:00,  3.23it/s, v_num=0]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 593/593 [03:03<00:00,  3.23it/s, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "train_csv = \"/home/dzimmerman2021/Documents/fathomnet/train/bioclip_train.csv\"\n",
    "val_csv = \"/home/dzimmerman2021/Documents/fathomnet/train/bioclip_val.csv\"\n",
    "\n",
    "# Model\n",
    "model_module = BioCLIPLightningModule()\n",
    "\n",
    "# Datasets and Dataloaders\n",
    "train_dataset = BioCLIPDataset(train_csv, model_module.tokenizer, model_module.preprocess_train)\n",
    "val_dataset = BioCLIPDataset(val_csv, model_module.tokenizer, model_module.preprocess_train)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Checkpointing\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    filename=\"bioclip-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.fit(model_module, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 75/75 [00:08<00:00,  8.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Internal Test Accuracy: 0.5937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_csv = \"/home/dzimmerman2021/Documents/fathomnet/train/bioclip_test_internal.csv\"\n",
    "test_dataset = BioCLIPDataset(test_csv, model_module.tokenizer, model_module.preprocess_train)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Simple accuracy evaluation\n",
    "def evaluate_accuracy(model_module, dataloader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model_module.model.to(device).eval()\n",
    "\n",
    "    correct = total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, texts in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            imgs = imgs.to(device)\n",
    "            texts = texts.to(device)\n",
    "\n",
    "            img_feat = model.encode_image(imgs)\n",
    "            txt_feat = model.encode_text(texts)\n",
    "\n",
    "            img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
    "            txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            sims = img_feat @ txt_feat.T\n",
    "            preds = sims.argmax(dim=1)\n",
    "            labels = torch.arange(len(imgs), device=device)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += len(imgs)\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"✅ Internal Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "\n",
    "evaluate_accuracy(model_module, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dzimmerman2021/Documents/fathomnet/BioCLIP/src/open_clip/factory.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n",
      "BioCLIP Test Inference: 100%|██████████| 788/788 [00:05<00:00, 141.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to bioclip_submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import open_clip\n",
    "\n",
    "# Recreate the module and load weights\n",
    "model_module = BioCLIPLightningModule.load_from_checkpoint(\n",
    "    \"/home/dzimmerman2021/Documents/fathomnet/lightning_logs/version_0/checkpoints/bioclip-epoch=08-val_loss=2.66.ckpt\",\n",
    "    model_name='hf-hub:imageomics/bioclip'\n",
    ")\n",
    "model_module.eval()\n",
    "\n",
    "# Load BioCLIP model\n",
    "model = model_module.model\n",
    "_, preprocess_val, _ = open_clip.create_model_and_transforms('hf-hub:imageomics/bioclip')\n",
    "tokenizer = open_clip.get_tokenizer('hf-hub:imageomics/bioclip')\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()\n",
    "\n",
    "# Load species prompts\n",
    "species_names = [\n",
    "        \"Abyssocucumis abyssorum\", \"Acanthascinae\", \"Acanthoptilum\", \"Actinernus\", \"Actiniaria\", \"Actinopterygii\", \"Amphipoda\", \"Apostichopus leukothele\", \"Asbestopluma\", \"Asbestopluma monticola\", \"Asteroidea\", \"Benthocodon pedunculata\", \"Brisingida\", \"Caridea\", \"Ceriantharia\", \"Chionoecetes tanneri\", \"Chorilia longipes\", \"Corallimorphus pilatus\", \"Crinoidea\", \"Delectopecten\", \"Elpidia\", \"Farrea\", \"Florometra serratissima\", \"Funiculina\", \"Gastropoda\", \"Gersemia juliepackardae\", \"Heterocarpus\", \"Heterochone calyx\", \"Heteropolypus ritteri\", \"Hexactinellida\", \"Hippasteria\", \"Holothuroidea\", \"Hormathiidae\", \"Isidella tentaculum\", \"Isididae\", \"Isosicyonis\", \"Keratoisis\", \"Liponema brevicorne\", \"Lithodidae\", \"Mediaster aequalis\", \"Merluccius productus\", \"Metridium farcimen\", \"Microstomus pacificus\", \"Munidopsis\", \"Munnopsidae\", \"Mycale\", \"Octopus rubescens\", \"Ophiacanthidae\", \"Ophiuroidea\", \"Paelopatides confundens\", \"Pandalus amplus\", \"Pandalus platyceros\", \"Pannychia moseleyi\", \"Paragorgia\", \"Paragorgia arborea\", \"Paralomis multispina\", \"Parastenella\", \"Peniagone\", \"Pennatula phosphorea\", \"Porifera\", \"Psathyrometra fragilis\", \"Psolus squamatus\", \"Ptychogastria polaris\", \"Pyrosoma atlanticum\", \"Rathbunaster californicus\", \"Scleractinia\", \"Scotoplanes\", \"Scotoplanes globosa\", \"Sebastes\", \"Sebastes diploproa\", \"Sebastolobus\", \"Serpulidae\", \"Staurocalyptus\", \"Strongylocentrotus fragilis\", \"Terebellidae\", \"Tunicata\", \"Umbellula\", \"Vesicomyidae\", \"Zoantharia\"\n",
    "    ] \n",
    "species_prompts = [f\"a photo of {s}\" for s in species_names]\n",
    "device = next(model.parameters()).device\n",
    "text_tokens = tokenizer(species_prompts).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Load test annotations\n",
    "ann_df = pd.read_csv(\"test/annotations.csv\")  # update path\n",
    "image_dir = \"test/rois\"  # update if needed\n",
    "\n",
    "# Run inference\n",
    "results = []\n",
    "for idx, row in tqdm(ann_df.iterrows(), total=len(ann_df), desc=\"BioCLIP Test Inference\"):\n",
    "    image_path = os.path.join(image_dir, row[\"path\"])\n",
    "    image = preprocess_val(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(next(model.parameters()).device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_feature = model.encode_image(image)\n",
    "        image_feature /= image_feature.norm(dim=-1, keepdim=True)\n",
    "        similarity = image_feature @ text_features.T\n",
    "        pred_idx = similarity.argmax().item()\n",
    "        pred_name = species_names[pred_idx]\n",
    "\n",
    "    results.append((idx + 1, pred_name))  # annotation_id starts at 1\n",
    "\n",
    "# Save results\n",
    "submission = pd.DataFrame(results, columns=[\"annotation_id\", \"concept_name\"])\n",
    "submission.to_csv(\"bioclip_submission.csv\", index=False)\n",
    "print(\"Submission saved to bioclip_submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fathmonet_bioclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
